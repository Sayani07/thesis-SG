# Introduction {#ch:intro}

<!-- Introduction -->
<!-- Motivation & Background -->
<!-- Outline of the thesis -->

To tame the complexity of time, breaking it into years, months, weeks, days and so on in a hierarchical manner is a common way to relate data to time. Such dicrete human made abstractions of time can be thought of as time granularities.(Aigner et al., 2008).


<!-- introducing the overall field-->
With the availability of data at more and more finer time scales, exploration of time series data may be required to be carried out across both finer and coarser scales to draw useful inferences about the underlying process. For example, data collected at an hourly scale could be analyzed using coarser temporal scales such as days, months or quarters. This approach requires deconstructing time in various possible ways. Moreover, often it might be interesting to capture calendar or periodic effects like month-of-year, day-of-week or hour-of-day. They help us in answering questions like if certain levels of those time deconstructions are characterized by unusual/routine values of the observed variable. For example, certain days of the week or months of the year are likely to be characterized by higher values. It is important to be able to navigate through all the temporal deconstructions that accommodate for periodicities to have multiple perspectives of the observed data. This idea aligns with the notion of EDA (Tukey 1977) which emphasizes the use of multiple perspectives on data to help formulate hypotheses before proceeding to hypothesis testing.



<!-- outline the intro chapter structure -->

This chapter will provide an introduction to the study by first discussing the background and context, followed by the research aims, objectives and questions.

<!-- Background -->

The motivation for this work comes from the desire to provide methods to better understand large quantities of measurements on energy usage reported by smart meters
in household across Australia, and indeed many parts of the world. Smart meters currently
provide half-hourly use in kwh for each household, from the time that they were
installed, some as early as 2012. Households are distributed geographically, and have
different demographic properties such as the existence of solar panels, central heating or air conditioning. The behavioral patterns in households vary substantially, for example, some families use a dryer for their clothes while others hang them on a line, and some households might consist of night owls, while others are morning larks. It is common to see aggregates of usage across households, total kwh used each half hour
by state, for example, because energy companies need to understand maximum loads
that they will have to plan ahead to accommodate. But studying overall energy use hides
the distributions of usage at finer scales, and making it more difficult to find solutions to improve energy efficiency.


<!-- introduce the specific problem  -->


<!-- ## change in data structure as a result of transitioning from linear to cyclic deconstructions -->
<!-- + Existing approaches, why we need to drill down, or why we want to -- probability distributions -->
However, restructuring time in this manner leads to restructured data where each level of the time deconstructions correspond to multiple values of the observed variable. It is common to see aggregation or summarization of these multiple observations with a unique value to study calendar effects. For example, using aggregates of usage across each hour/half-hour has been common in the literature because energy companies need to plan for maximum loads on the network. But studying overall energy use hides the distributions of usage at finer scales, and making it more difficult to find solutions to improve energy efficiency. Summarizing the probability distribution of these multiple observations to capture both the shape and uncertainty could be a potential way to understand the underlying distribution of these observations. Studying probability distributions is likely to focus on features of the data which are not transparent through raw data or a unique summary statistic.

<!-- Following research objectives would facilitate the achievement of this aim -->
<!-- Research objectives -->

<!-- ## Research aims -->

Hence, the overarching research goal is to study the periodic behavior of temporal data in a structured way by studying the probability distributions by best exploiting the characteristics of time. Slicing and dicing the data in all possible temporal scales as suggested by EDA can be a daunting task as it leads to a myriad of possibilities. This inspires the research presented in this thesis, which aims to provide a platform to systematically explore periodicities in temporal data and support finding regular patterns or anomalies, explore clusters of behaviors or summarize the behavior. The first part of the work discusses computation of all possible combinations of cyclic time granularities and a graphical mapping such that distributions of a numeric response variable is displayed across combinations of two cyclic granularities. Even analyzing the distribution of the measured variable across two cyclic granularities at once could amount to displaying many plots in search of potential  patterns. Thus, the first part of the research (@Gupta2020-vo) also introduces "harmony" to denote pairs of granularities that could be analyzed together and reduces the search from all possible options. But this approach is still overwhelming for human consumption because there would still be huge number of harmonies. Hence, the second part of the research extends this work and narrows the search further by finding pair of cyclic granularities which are informative enough and rank them according to their importance. However, to explore periodic patterns of many households, we have to resort to clustering which has been addressed in the third part of the research. Although the motivation came through the smart meter example, this is a problem that is relevant to any temporal data observed more than once per year.


Temporal data analysis has assumed that the entry point to data analysis is at a model-ready data format, which provides little organization or conceptual oversight on how one should get the wild data into a tamed state. This mind-set is related to a long-held belief that exploratory data analysis is a highly ad hoc statistical area, impossible to teach or formalize. However, the **tidyverse** framework implemented in the statistical software R [@R-base], as originating in @wickham2014tidy, fundamentally overturns this thinking. Data plots and data wrangling, which the "tidy data" conceptualization supports, can be formally described using an abstract grammar. The grammar of graphics and data manipulation, as implemented in the **ggplot2** [@R-ggplot2] and **dplyr** [@R-dplyr] R packages respectively, form the core of the **tidyverse** suite of tools. My contributions extend the **tidyverse** way of thinking to the temporal domain, by providing tidy tools, built as R packages, for supporting fluent workflow in temporal data analysis.

## Visualizing probability distributions across bivariate cyclic temporal granularities {#sec:gravitas}

Deconstructing a time index into time granularities can assist in exploration and automated analysis of large temporal data sets. This paper describes classes of time deconstructions using linear and cyclic time granularities. Linear granularities respect the linear progression of time such as hours, days, weeks and months. Cyclic granularities can be circular such as hour-of-the-day, quasi-circular such as day-of-the-month, and aperiodic such as public holidays. The hierarchical structure of granularities creates a nested ordering: hour-of-the-day and second-of-the-minute are single-order-up. Hour-of-the-week is multiple-order-up, because it passes over day-of-the-week. Methods are provided for creating all possible granularities for a time index. A recommendation algorithm provides an indication whether a pair of granularities can be meaningfully examined together (a "harmony"), or when they cannot (a "clash").  
 
 Time granularities can be used to create data visualizations to explore for periodicities, associations and anomalies. The granularities form categorical variables (ordered or unordered) which induce groupings of the observations. Assuming a numeric response variable, the resulting graphics are then displays of distributions compared across combinations of categorical variables.  
  
 The methods implemented in the open source R package `gravitas` are consistent with a tidy workflow, with probability distributions examined using the range of graphics available in `ggplot2`.

## Detecting distributional differences between temporal granularities for exploratory time series analysis {#sec:hakear}

Cyclic temporal granularities, which are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, can be useful for measuring repetitive patterns in large univariate time series data. The granularities feed new approaches to exploring time series data. One use is to take pairs of granularities, and make plots of response values across the categories induced by the temporal deconstruction. However, when there are many granularities that can be constructed for a time period, there will also be too many possible displays to decide which might be the more interesting to display. This work proposes a new distance metric to screen and rank the possible granularities, and hence choose the most interesting ones to plot. The distance measure is computed for a single or pairs of cyclic granularities can can be compared across different cyclic granularities and also on a collection of time series. The methods are implemented in the open-source R package `hakear`. 

## Clustering time series based on probability distributions across temporal granularities {#sec:gracsr}


 With more and more time series data being collected at much finer temporal resolution, for a longer length of time, and for a larger number of individuals/entities, time series clustering research is getting a lot of traction. The sort of noisy, patchy, uneven, and asynchronous time series that is typical in many disciplines limits similarity searches among these lengthy time series. In this work, we suggest a method for overcoming these constraints by grouping time series based on probability distributions over cyclic temporal granularities. Cyclic granularities are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, and so on, and can be helpful for detecting repeating patterns. Looking at probability distributions across cyclic granularities results in an approach that is robust to missing or noisy data, aids in dimension reduction, and ensures small pockets of similar repeated behaviours. The proposed method was tested using a collection of residential electricity customers. The simulated and empirical evidence demonstrates that our method is capable of producing meaningful clusters.


<!-- Time series clustering research is gaining a lot of importance with more and more time series data recorded at much finer temporal resolution, for a longer period of time and for a larger number of individuals/entities.Similarity searches amongst these long time series are often limited by the type of noisy, patchy, unequal and asynchronous time series common in many fields. In this paper we propose a strategy to alleviate these limitations by  clustering time series based on probability distributions across cyclic temporal granularities. Cyclic granularities are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, which can be useful for measuring repetitive patterns in large univariate time series data. Thus, looking at the probability distributions across cyclic granularity leads to a method which is robust to missing or noisy data, helps in dimension reduction while ensuring small pockets of similar repetitive behaviors. The method is applied to electricity smart meter dat -->
<!-- The suggested approach was evaluated on a set of benchmark time series on residential electricity customers. The empirical results show that our approach is able to yield meaningful clusters. -->

## Summary

The thesis is structured as follows. Chapter \@ref(ch:calendar-vis) provides details of the calendar plot, algorithm and applications. This is implemented in the R package **sugrrants**. Chapter \@ref(ch:tsibble) explains the new data abstraction--tsibble--and illustrates how it can be used to for the basis of exploratory methods, visualization and modeling of temporal data. This is available as the R package **tsibble**. Chapter \@ref(ch:mists) describes new procedures for exploring temporal missing patterns, and assisting in handling missing values prior to modeling. This is in the developing R package **mists**.

Chapter \@ref(ch:conclusion) summarizes the software tools developed for the work and their impact, and discusses some future plans.


