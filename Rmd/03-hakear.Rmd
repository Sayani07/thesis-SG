# Detecting distributional differences between temporal granularities for exploratory time series analysis  {#ch-hakear}

 Cyclic temporal granularities, which are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, can be useful for measuring repetitive patterns in large univariate time series data. The granularities feed new approaches to exploring time series data. One use is to take pairs of granularities, and make plots of response values across the categories induced by the temporal deconstruction. However, when there are many granularities that can be constructed for a time period, there will also be too many possible displays to decide which might be the more interesting to display. This work proposes a new distance metric to screen and rank the possible granularities, and hence choose the most interesting ones to plot. The distance measure is computed for a single or pairs of cyclic granularities that can be compared across different cyclic granularities and also on a collection of time series. The methods are implemented in the open-source R package [`hakear`](https://github.com/Sayani07/hakear).
 
 
```{r external-hakear, include = FALSE, cache = FALSE}
read_chunk("scripts/hakear.R")
```

```{r load}
```


```{r}
set.seed(3215)
```

## Introduction

<!-- <introducing the problem> -->
<!-- background of the problem -->

<!--Exploratory data analysis, as coined by John W. Tukey [@Tukey1977-jx] involves many iterations of finding structures and patterns that allow the data to be informative.-->

Cyclic temporal granularities [@Bettini1998-ed;@Gupta2021-gravitas] are <!--theoretical--> temporal deconstructions that define cyclic repetitions in time, e.g. hour-of-day, day-of-month, or regularly scheduled public holidays. These granularities form ordered or unordered categorical variables. An example of an ordered granularity is day-of-week, where Tuesday is always followed by Wednesday, and so on. An unordered granularity example is \DC{week type in an academic semester: orientation, break, exam or regular classes}. We can use granularities to explore patterns in univariate time series by examining the distribution of the measured variable across different categories of the cyclic granularities.

```{r calendar-elec, fig.height = 8, fig.cap = "Calendar displays faceted by two households are shown. The calendar displays are rich in information and has several components. For example, it can be observed easily that the level of energy consumption by household 2 is higher than household 4. Also, it gives an overview on when these households are away for holidays and which months consumptions are highest (Jan, Feb). However, when it comes to discerning periodic patterns, analyzing all possible periodic patterns through this display is overwhelming due to combination of linear and cyclic representation of time. For example, there is more going on in terms of peaks and troughs in id 2 compared to id 4, but it is not clear which all periodic patterns are dominant and if it differs between households."}

```

```{r onegran-new, fig.cap="A cyclic granularity can be considered to be a categorical variable, and used to break the data into subsets. Here, side-by-side boxplots overlaid on jittered dotplots explore the distribution of energy use (on a logarithmic scale) by a household for two different cyclic granularities: (a) hour-of-day and (b) month-of-year. Daily peaks occur in morning and evening hours, indicating a working household, where members leave for and return from work. More volatility of usage in summer months in Australia (Jan, Feb) is probably due to air conditioner use on just some days.", fig.pos="!b"}
```

As a motivating example, consider Figure \ref{fig:onegran-new} which shows electricity smart meter data plotted against two granularities (hour-of-day, month-of-year). The data was collected on a single household in Melbourne, Australia, over a six month period, and was previously used in @wang2020calendar. The categorical variable (granularity) is mapped to the x-axis, and the distribution of the response variable is displayed using both side-by-side jittered dotplots and boxplots. From panel (a) it can be seen that energy consumption is higher during the morning hours (5--8), when members in the household wake up, and again in the evening hours (17--20), possibly when members get back from work. In addition, the largest variation in energy use is in the afternoon hours (12--16), as seen in the length of the boxes. From panel (b), it is seen that the variability in energy usage is higher in Jan and Feb, probably due to the usage of air conditioners on some days. The median usage is highest in January, dips in February--April and rises again in May--June, although not to the height of January usage. This suggests that the household does not use as much electricity for heating as it does for air conditioning. A lot of households in Melbourne use gas heating and hence the heater use might not be reflected in the electricity data.

Many different displays could be constructed using different granularities including day-of-week, day-of-month, weekday/weekend, etc. However, only a few might be interesting and reveal important patterns in energy usage. Determining which displays have "significant" distributional differences between categories of the cyclic granularity, and plotting only these, would make for efficient exploration.

```{r intro_all}

```

```{r id2-new2, fig.cap = "Distribution of energy consumption displayed through area quantile plots across two cyclic granularities month-of-year and hour-of-day and two households. The black line is the median, whereas the orange band covers the 25th to 75th percentile and the green band covers the 10th to 90th percentile. Difference between the 90th and 75th quantiles is less for (Jan, Feb) for the first household (a), suggesting that it is a more frequent user of air conditioners than the second household (b). Distribution of energy usage for (a) changes across both granularities, whereas for (b) daily pattern stays same irrespective of the months."}
```

<!-- comparing one and two grans for one households -->
Exploring the distribution of the measured variable across two cyclic granularities provides more detailed information on its structure. For example, Figure \ref{fig:id2-new2}(a) shows the usage distribution across hour-of-day conditional on month-of-year across two households. It shows the hourly usage over a day does not remain the same across months. Unlike other months, the 75th and 90th percentile for all hours of the day in January are high, similar, and are not characterized by a morning and evening peak. The household in Figure \ref{fig:id2-new2}(b) has 90th percentile consumption higher in summer months relative to autumn or winter, but the 75th and 90th percentile are far apart in all months, implying that the second household resorts to air conditioning much less regularly than the first one. The differences seem to be more prominent across month-of-year (facets) than hour-of-day (x-axis) for this household, whereas they are prominent for both cyclic granularities for the first household.

<!-- The problem and its dimension -->
Are all four displays in Figures \ref{fig:onegran-new} and \ref{fig:id2-new2} useful in understanding the distributional difference in energy usage? Which ones are more useful than others? If $N_C$ is the total number of cyclic granularities of interest, the number of displays that could be potentially informative is $N_C$ when considering displays of the form in Figure \ref{fig:onegran-new}. The dimension of the problem, however, increases when considering more than one cyclic granularity. When considering displays of the form in Figure \ref{fig:id2-new2},
there are $N_C(N_C-1)$ possible pairwise plots exhaustively, with one of the two cyclic granularities acting as the conditioning variable.
This can be overwhelming for human consumption even for moderately large $N_C$. It is therefore  useful to identify those displays that are informative across at least one cyclic granularity.

<!-- Moreover, the volatility in consumption are almost equal and high for all hours in Jan, whereas variability is most in the evening hours for other months. The distribution of consumption across different hours of the day look similar for Apr, May and Jun, characterized by a distinct morning, afternoon and evening peaks and higher variability in evening hours -->

<!-- months Jan and Feb not only have a high median consumption, but also more variability, possibly due to the usage of air conditioning. The other months shows more consistent behavior with the energy consumption rising in May and June. -->

<!-- Scagnostics literature, other literature and why we need more/different --> 

This problem is similar to Scagnostics (Scatterplot Diagnostics) by @tukey1988computer, which are used to identify meaningful patterns in large collections of scatterplots. Given a set of $v$ variables, there are $p(p-1)/2$ pairs of variables, and thus the same number of possible pairwise scatterplots. Therefore, even for small $v$, the number of scatterplots can be large, and scatterplot matrices (SPLOMs) can easily run out of pixels when presenting high-dimensional data. @Dang2014-tw; @wilkinson2005graph provided potential solutions to this, where a few characterizations can be used to locate anomalies in density, shape, trend, and other features in the 2D point scatters.

In this paper, we provide a solution to narrowing down the search from $N_C(N_C-1)$ conditional distribution plots by introducing a new distance measure that can be used to detect significant distributional differences across cyclic granularities.<!-- harmonies and why it is not enough --> This work is a natural extension of our previous work [@Gupta2021-gravitas] which narrows down the search from $N_C(N_C-1)$ plots by identifying pairs of granularities that can be meaningfully examined together (a "harmony"), or when they cannot (a "clash"). However, even after excluding clashes, the list of harmonies left may be too large for exhaustive exploration. Hence, there is a need to reduce the search even further by including only those harmonies that contain useful information.

@inference; @Majumder2013-hb presented methods for statistical significance testing of visual findings using human cognition as the statistical tests. In this paper, the visual discovery of distributional differences is facilitated by choosing a threshold for the proposed numerical distance measure, eventually selecting only those cyclic granularities for which the distributional differences are sufficient to make it an interesting display. Two things need to be accomplished here: 1) to see if there are any statistically significant differences between independent groups, and 2) to quantify any differences that do exist. One way to address this problem is by using a one-way or two-way ANOVA [@fisher1992statistical]. Assume we want to examine whether there is a significant difference in electricity demand on various days of the week. In this case, each day of the week may be regarded as an independent group, and a one-way ANOVA can be used to assess how the means of the electricity demand varies across different days of the week. The approach proposed in this paper looks at the distributional differences in the quantitative variable instead of merely the mean or one measure of central tendency. Besides, it also takes into account that the levels in each group have an inherent order in a time series context.

<!-- \ref{fig:id2-new2} shows the distribution of energy consumption across hour-of-day conditional on month-of-year. \ref{fig:id2-new2}(a) shows that energy consumption is higher during the morning hours when members in the household wake up and then again higher in the evening hours possibly when members are back from work with them showing maximum variable behavior in the afternoon hours. \ref{fig:id2-new2}(b) shows the months Jan and Feb not only have a high median consumption, but also more variability, possibly due to the usage of air conditioning. The other months shows more consistent behavior with the energy consumption rising in May and June. \ref{fig:id2-new2} adds to this information, by showing that hours in Jan do not necessarily follow the same pattern across the day (due to Holidays, people visiting). Moreover, the volatility in consumption are almost equal and high for all hours in Jan, whereas variability is most in the evening hours for other months. The distribution of consumption across different hours of the day look similar for Apr, May and Jun, characterized by a distinct morning, afternoon and evening peaks and higher variability in evening hours. -->

<!-- # interaction for two households, importance could be across one --> 

<!-- Even when it is known which all interactions to look at, all of them would not be interesting and that too will vary across different households. -->

<!-- calendar plots -->
<!-- The authors show how hour-of-the-day interact with weekdays and weekends and then move on to use calendar display to show daily schedules. The calendar display has several components in it, which helps us to look at energy consumption across hour-of-day, day-of-week, week-of-month, and month-of-year at once. Some interaction of these cyclic granularities, for example, how day-of-week relates to month-of-year, could also be interpreted from this display. This is a great way for having an overview of energy consumption. However, if one wants to understand the repetitiveness in energy behavior and how they interact in greater detail, it is not easy to comprehend the interactions of all cyclic granularities from this display, due to the combination of linear and cyclic representation of time. For example, this display might not be the best to understand how hour-of-the-day or month-of-year varies across week-of-the-month as well as with each other. Furthermore, it is not clear what all interactions of cyclic granularities should be read from this display as there could be many combinations that one can look at. Moreover, there could be non-conventional cyclic granularities, which could potentially become useful depending on the context. -->

<!-- context when it could be useful is monitoring heart rates. There are devices that can detect each heartbeat and transfer the data to a receiver such as a watch or phone. These data could be available for a temporal scale as fine as a minute and it could be of interest to see regular patterns across any deconstruction of time coarser than a minute. -->

```{r id4, fig.cap = "something2", eval = FALSE}

```

<!-- Again, look at \ref{fig:} c and d, where energy consumption for these two households are plotted against (weekend/weekday, week-of-month). Here, for both households, the pattern of energy consumption vary across different weeks of the month irrespective of the fact it is a weekday or weekend. In that respect, the harmony pair (month-of-year, hour-of-day) seems to be more informative than (weekend/weekday, week-of-month) for the first household. -->

<!-- Take an example of a data set which are observed at fine temporal scales, like that of NYC bike usage available at https://www.citibikenyc.com/system-data. We use the `nyc_bikes` data set from the R package `tsibbledata` which takes a sample of 10 bikes for the year 2018. The `start_time` and the `stop_time` are recorded to a fineness of seconds. We can look at pair of cyclic granularities (hour_day, wknd_wday) or (week_month, day_week) to see how these periodicities interact. But there could be other pairs that are important too. How to understand which pairs are sufficient to explore given the data set without losing much information about the data. -->

<!-- When we need to understand the interplay of different periodicities in a high frequency temporal datasets, we have many choices to consider. In [@wang2020tsibble] and [@wang2020calendar], periodicities are explored across hour of the day and day of the week or months. But calendar effects are not restricted to conventional day-of-week or month-of-year deconstructions. -->

<!-- If we have $n$ periodic linear granularities in the hierarchy table, then $n(n-1)/2$ circular or quasi-circular cyclic granularities could be constructed. -->

<!-- Our contributions in this paper are: -->

 <!-- * We introduce a new distance measure for detecting periodic interactions. This induces data reduction which allows for identification of patterns, if any, in the time series data. -->

 <!-- * We show that the distance metric could be used to rank the periodic patterns based on how well they capture the variation in the measured variable as they have been normalized for different number of comparisons. -->

 <!-- * We device a framework for choosing a threshold, which will result in detection of only significantly interesting periodic patterns in the time series data. -->

 <!-- * -->

 <!-- * ; -->

 <!-- * show that the proposed distance metric could be used to rank the interesting patterns across different datasets and temporal granularities since they have been normalized for relevant parameters. -->

The article is organized as follows. Section \ref{sec:computation-wpd} introduces a distance measure for detecting distributional difference in temporal granularities for a continuous univariate dependent variable. This enables identification of patterns in the time series data;
Section \ref{sec:rank-wpd} devises a selection criterion by choosing a threshold, which results in detection of only significantly interesting patterns. Section \ref{sec:simulations} provides a simulation study on the proposed methodology. Section \ref{sec:application-wpd} presents an application to residential smart meter data in Melbourne to show how the proposed methodology can be used to automatically detect temporal granularities along which distributional differences are significant.

<!-- acts as a way to automatically detect periodic patterns in time series. -->

<!-- Also, ranking the remaining harmony pairs based on how well they capture the variation in the measured variable could be potentially useful.  -->

<!-- Talk about Scagnostics: Tukey -->

## Proposed distance measure {#sec:computation-wpd}

We propose a measure called Weighted Pairwise Distances ($\wpd$) to detect distributional differences in the measured variable across cyclic granularities.

### Principle

(ref:null4by2) An example illustrating the principle of the proposed distance measure, displaying the distribution of a normally distributed variable in four panels each with two x-axis categories and three facet levels, but with different designs. Panel (a) is not interesting as the distribution of the variable does not depend on x or facet categories. The data set in (a) is consistent with the null distribution. Panels (b) and (c) are more interesting than (a) since there is a change in distribution either across facets (b) or x-axis (c). Panel (d) is most interesting in terms of capturing structure in the variable as the distribution of the variable changes across both facet and x-axis variable. The value of our proposed distance measure is presented for each panel, the relative differences between which will be explained later in Section \@ref(sec:ranking).

```{r compute-pairwise-norm-null4by2}

```


```{r null4by2, fig.cap = "(ref:null4by2)", fig.pos="!b"}
```

The principle behind the construction of $\wpd$ is explained through a simple example in Figure&nbsp;\ref{fig:null4by2}. Each of these figures describes a panel with two x-axis categories and three facet levels, but with different designs. Figure \ref{fig:null4by2}a has all categories drawn from a standard normal distribution for each facet. It is not a particularly interesting display, as the distributions do not vary across x-axis or facet categories. Figure \ref{fig:null4by2}b has x categories drawn from the same distribution, but across facets the distribution means are three standard deviations apart. Figure \ref{fig:null4by2}c exhibits the opposite situation where distribution between the x-axis categories are three standard deviations apart, but they do not change across facets. In Figure \ref{fig:null4by2}d, the distribution varies across both facet and x-axis categories by three standard deviations.

If the panels are to be ranked in order of capturing maximum variation in the measured variable from minimum to maximum, then an obvious choice would be (a) followed by (b), (c) and then (d). It might be argued that it is not clear if (b) should precede or succeed (c) in the ranking. Gestalt theory suggests items placed at close proximity can be compared more easily, because people assume that they are in the same group and apart from other groups. With this principle in mind, Panel (b) is considered less informative compared to Panel (c) in emphasizing the distributional differences.

For displays showing a single cyclic granularity rather than pairs of granularities, we have only two design choices corresponding to no difference and significant differences between categories of that cyclic granularity.

The proposed measure $\wpd$ is constructed in such a way that it can be used to rank panels of different designs as well as test if a design is interesting. This measure is aimed to be an estimate of the maximum variation in the measured variable explained by the panel. A higher value of $\wpd$ would indicate that the panel is interesting to look at, whereas a lower value would indicate otherwise.

<!-- To compute the maximum variation of the measured variable within each panel, the distributional differences need to be measured for both within-group and between-group categories. Moreover, a tuning parameter specifying the weightage given to within-facet or between-facet categories can help to choose between designs like \ref{fig:null4by2}b and c. The maximum of all these weighted differences serves as an estimate of the maximum variation in the panel and is taken as the value of $\wpd$. -->

<!-- Larger differences imply stronger patterns, whereas small difference would imply that the underlying structure is not changing within or between group. -->

<!-- A distance measure aimed to capture the structure of the measured variable in designs like \ref{fig:null4by2}, should ideally estimate these within-group and between-group variations. -->

<!-- \noindent Intuitively, while finding a structure or measuring the strength of patterns in Figure \ref{fig:null4by2}, it makes sense to look for within-group and between-group variations. Larger variation would imply stronger patterns, whereas small variation would imply the underlying structure is not changing within or between group. -->
<!-- Thus, a distance measure aimed to capture this structure should ideally estimate these within-group and between-group variations. One of the potential ways to do this is to measure the distances between distributions of the continuous random variable measured within and between groups, weigh them basis if they are within or between groups and then take the maximum of those distances as an estimate of the maximum variation in the structure. This section starts with possible ways of characterizing distributions and computing distances between them and then describe in details how the measure $\wpd$ is defined. -->

<!-- This is similar to @ieee-irish where the authors compute the Jensen Shannon distance between two density estimates by computing percentiles and stresses the advantages to working with percentiles rather than the raw data directly in case of missing observations. Working with quantiles also ensure that unsynchronized time series could be handled. -->

<!-- Hence, with reference to the graphical design in @Gupta2021-gravitas, therefore the idea would be to rate a harmony pair higher if the variation between different levels of the x-axis variable is higher on an average across all levels of the facet variables. -->

<!-- To elaborate further, look at the examples in Figure \ref{}, where Figure \ref{}a represents the panel design with distribution of each x categories drawn from N(5, 10) distribution. It could be observed that the graph is not particularly interesting, as there is no significant change in distribution between x-axis levels or facets. Figure \ref{}b represents the same panel design with no difference in distribution of x-axis categories within a facet, but different distribution of x-axis categories for different facets. For example, if there are 4 facet levels and 2 x-axis levels, data is generated in the way as described in Table \ref{}. Figure \ref{}b exhibits an opposite situation where the x-axis within facets are different but not across facets. -->
<!-- Figure \ref{}d takes it further by varying the distribution across both facet and x-axis categories. -->


<!-- A "significant" cyclic granularity have at least two categories which are significantly different from each other. It does not tell you which categories are statistically significantly different from each other -->

<!-- in the same group would be important to bring out different patterns of the data. -->

### Notation

Let the number of cyclic granularities considered in the display be $m$. The notations and methodology are described in detail for $m=2$. But it can be easily extended to $m>2$. Consider two cyclic granularities $A$ and $B$, such that $A = \{a_j: j = 1, 2, \dots, \nx\}$ and $B = \{b_k: k = 1, 2, \dots, \nf\}$ with $A$ placed across the x-axis and $B$ across facets. Let $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ be a continuous variable observed across $T$ time points. This data structure with $\nx$ x-axis levels and $\nf$ facet levels is referred to as a $(\nx, \nf)$ panel. For example, a $(2, 3)$ panel will have cyclic granularities with two x-axis levels and three facet levels. Let the four elementary designs as described in Figure \ref{fig:null4by2} be $\Dnull$ (referred to as "null distribution") where there is no difference in distribution of $v$ for $A$ or $B$. For example, the data set in Figure \ref{fig:null4by2}(a) is consistent with the null distribution. $\Df$ denotes the set of designs where there is difference in distribution of $v$ for $B$ and not for $A$. Similarly, $\Dx$ denotes the set of designs where difference is observed only across A. Finally, $\Dfx$ denotes those designs for which difference is observed across both $A$ and $B$. We can consider a single granularity ($m = 1$) as a special case of two granularities with $\nf = 1$.

```{r notations}

```

 <!-- Thus, by CLT, ${\chi^2}_{m} \tilde{} N(m, 2m)$, which would depend on the number of discretization used to approximate the continuous distribution. -->
 <!-- Then $b_n = 1-1/n$ quantile of the normal distribution and $a_n = 1/[n*\phi(b_n)]$ where $\phi$ is the normal density function. $n$ is the number of pairwise comparisons being made. -->

<!-- The Jensen-Shanon distance between two probability distribution $p_1$ and $p_2$ is given by $$d = [D(p_1, r) + D(p_2, r)]/2 \quad where \quad r = (p_1 + p_2)/2$$ where, -->
<!-- $$D(p_1,p_2) = \int^{\infty}_{-\infty}p_1(x)log\frac{p_1(x)}{p_2(x)}\,dx$$ is the Kullback-Leibler divergence between $p_1$ and $p_2$.  -->

<!-- We call this measure of variation as Median Maximum Pairwise Distances (MMPD). -->

<!-- #### Distribution of Jensen-Shannon distances -->

<!-- Jensen-Shannon distances (JSD) are distributed as chi-squared with $m$ df where we discretize the continuous distribution with $m$ discrete values. Taking sample percentiles to approximate the integral would mean taking $m = 99$. -->
<!-- With large $m$, chi-squared is asymptotically normal by the CLT. Thus, by CLT, ${\chi^2}_{m} \tilde{} N(m, 2m)$, which would depend on the number of discretization used to approximate the continuous distribution. Then $b_n = 1-1/n$ quantile of the normal distribution and $a_n = 1/[n*\phi(b_n)]$ where $\phi$ is the normal density function. $n$ is the number of pairwise comparisons being made. -->

### Computation

The computation of the distance measure $\wpd$ for a panel involves characterizing distributions, computing distances between distributions, choosing a tuning parameter to specify the weight for different groups of distances and summarizing those weighted distances appropriately to estimate maximum variation. Furthermore, the data needs to be appropriately transformed to ensure that the value of $\wpd$ emphasizes detection of distributional differences across categories and not across different data generating processes.

<!-- but not when underlying distributions are different -->

<!-- irrespective of the distribution from which the data is generated. -->

<!-- is aimed to capture structure and patterns by estimating the maximum variation of the measured variable within a panel. $\wpd$ -->

<!-- the distributional differences need to be measured for both within-group and between-group categories. Moreover, a tuning parameter specifying the weightage given to within-facet or between-facet categories can help to choose between designs like \ref{fig:null4by2}b and c. The maximum of all these weighted differences serves as an estimate of the maximum variation in the panel and is taken as the value of $\wpd$. -->

#### Data transformation {-}

The intended aim of $\wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. Hence, as a pre-processing step, the raw data is normal-quantile transformed (NQT) [@Krzysztofowicz1997-bv], so that the transformed data follows a standard normal distribution. The empirical NQT involves the following steps:

 1. The observations of measured variable $v$ are sorted from the smallest to the largest observation $v_{(1)},\dots, v_{(n)}$.
 2. The cumulative probabilities $p_{(1)},\dots, p_{(n)}$ are estimated using $p_{(i)} = i/(n + 1)$  [@Hyndman1996-ty] so that $p_{(i)} = \text{Pr}(v \leq v_{(i)})$.
 3. Each observation $v_{(i)}$ of $v$ is transformed into $v^*(i) = \Phi^{-1}(p(i))$, with $\Phi$ denoting the standard normal distribution function.

#### Characterizing distributions {-}

Multiple observations of $v$ correspond to the subset $v_{jk} = \{s: A(s) = j, B(s) = k\}$. The number of observations might vary widely across subsets due to the structure of the calendar, missing observations or uneven locations of events in the time domain. In this paper, quantiles of $\{v_{jk}\}$ are chosen as a way to characterize distributions for the category $(a_j, b_k)$, $\forall j\in \{1, 2, \dots, \nx\}, k\in \{1, 2, \dots, \nf\}$. We use percentiles with $p = {0.01, 0.02, \dots, 0.99}$ to reduce the computational burden in summarizing distributions. The assumption is that there is sufficient data for each level or combination of levels, and hence no adjustment is made for the varying number of observations across levels. However, if one or more levels has a small number of data points, consecutive categories should be collapsed prior to data transformation and quantile estimation.

<!-- The benefit of using a non-parametric estimator is that there are less rigid assumptions made about the nature of the underlying distribution of the data. Sample quantiles could be used for estimating population quantiles in a non-parametric setup. describes the many ways of defining sample quantiles and recommends the use of median-unbiased estimator because of _desirable properties of a quantile estimator and can be defined independently of the underlying distribution._. -->

<!-- in this paper. -->

<!-- Each $v_{jk}$'s $\forall j\in \{1, 2, \dots, \nx\}, k\in \{1, 2, \dots, \nf\}\}$ are assumed to be drawn from a continuous probability distribution and have certain characteristics. Shape, central tendency, and variability are the common characteristics used to describe a distribution. -->

<!-- Mean, median or mode are generally used to describe the center of the distribution, while range, standard deviation, quantiles, standard errors and confidence intervals are used to describe variability. -->

<!-- The quantile of a distribution with probability $p$ is defined as $Q(p)=F^{-1}(p) = inf\{x: F(x) >p\}$, $0<p< 1$ where $F(x)$ is the distribution function. There are two broad approaches to quantile estimation, viz, parametric and non-parametric. The benefit of using a non-parametric estimator is that there are less rigid assumptions made about the nature of the underlying distribution of the data. Sample quantiles could be used for estimating population quantiles in a non-parametric setup. @Hyndman1996-ty describes the many ways of defining sample quantiles and recommends the use of median-unbiased estimator because of _desirable properties of a quantile estimator and can be defined independently of the underlying distribution._. The `stats::quantile()` function in @R-language could be used for practical implementation where type = 8 refers to the algorithm corresponding to the median-unbiased estimator. The default quantile chosen in this paper is percentiles computed for $p = {0.01, 0.02, \dots, 0.99}$, where for example, the $99^{th}$ percentile would be the value corresponding to $p=0.99$ and hence 99% of the observations would lie below that. -->

#### Distance between distributions {-}

<!-- One of the most important class of divergence is the f-divergence and includes measures like Kullback-Leibler divergence, Hellinger distance etc. The continuous version of f -divergence is given by -->
<!-- $$\Df(P||Q) := \int q(x)f(\frac{p(x)}{q(x)})$$, where -->
<!-- $f : [0,\infty) \rightarrow R \cup \{\infty\}$ is a continuous convex function, and $f(1) = 0$. -->

A common way to measure divergence between distributions is the Kullback-Leibler (KL) divergence [@Kullback1951-jy]. The KL divergence denoted by $D(q_1||q_2)$ is a non-symmetric measure of the difference between two probability distributions $q_1$ and $q_2$ and is interpreted as the amount of information lost when $q_2$ is used to approximate $q_1$. The KL divergence is not symmetric and hence can not be considered as a "distance" measure. The Jensen-Shannon divergence [@Menendez1997-in] based on the Kullback-Leibler divergence is symmetric and has a finite value. Hence, in this paper, the pairwise distances between the distributions of the measured variable are obtained through the square root of the Jensen-Shannon divergence, called Jensen-Shannon distance (JSD), and defined by
$$
  JSD(q_1||q_2) = \frac{1}{2}D(q_1||M) + \frac{1}{2}D(q_2||M),
$$
where $M = \frac{q_1+q_2}{2}$ and $D(q_1||q_2) := \int^\infty_{-\infty} q_1(x)log(\frac{q_1(x)}{q_2(x)})$ is the KL divergence between distributions $q_1$ and $q_2$. There are other ways to obtain distance between distributions like Hellinger distance, total variation distance and Fisher information metric (all of which are special cases of f-divergence), but Jensen-Shannon distance was chosen for ease of computation.

<!-- Furthermore, these distances are distributed as chi-squared with $m$ degrees of freedom [@Menendez1997-in], if the continuous distribution is being discretized with $m$ discrete values. Taking sample percentiles to approximate the integral would mean taking $m = 99$. As the degrees of freedom $m$ get larger, the chi-square distribution approaches the normal distribution. -->

#### Within-facet and between-facet distances {-}

```{r distance-explain, fig.cap = "Within and between-facet distances shown for two cyclic granularities A and B, where A is mapped to x-axis and B is mapped to facets. The dotted lines represent the distances between different categories. Panels 1) and 2) show the between-facet distances. Within-facet distances are illustrated in Panels 3) (when categories are un-ordered, shown only with respect to a1) and Panel 4) (when categories are ordered). When categories are ordered, distances should only be considered for consecutive x-axis categories. Between-facet distances are distances between different facet levels for the same x-axis category; for example, distances between {($a_1$,$b_1$) and ($a_1$, $b_2$)} or {($a_1$,$b_1$) and ($a_1$, $b_3$)}.", fig.pos="!b"}

```

Pairwise distances could be within-facets or between-facets for $m = 2$. Figure \ref{fig:distance-explain} illustrates how they are defined. Pairwise distances are within-facets when $b_{k} = b_{k'}$, that is, between pairs of the form $(a_{j}b_{k}, a_{j'}b_{k})$ as shown in panel (3) of Figure \ref{fig:distance-explain}. If categories are ordered (like all temporal cyclic granularities), then only distances between pairs where $a_{j'} = (a_{j+1})$ are considered (panel (4)). Pairwise distances are between-facets when they are considered between pairs of the form $(a_{j}b_{k}, a_{j}b_{k'})$. There are a total of ${\nf \choose 2}\nx$ between-facet distances, and ${\nx \choose 2}\nf$ within-facet distances if they are unordered and $\nf(\nx-1)$ within-facet distances if they are ordered.

#### Tuning parameter {-}

For displays with $m>1$ granularities, we can use a tuning parameter to specify the relative weight given to each granularity. In general, the tuning parameters should be chosen such that $\sum_{i=1}^m \lambda_i = 1$.

Following the general principles of Gestalt theory, we wish to weight more heavily granularities that are plotted closer together. For $m=2$ we choose  $\lambda_x = \frac{2}{3}$ for the granularity on the x-axis and $\lambda_f = \frac13$ for the granularity mapped to facets, giving a relative weight of $2:1$ for within-facet to between-facet distances. No human experiment has been conducted to justify this ratio.  Specifying $\lambda_x>0.5$ will weight within-facet distances more heavily, while $\lambda_x <0.5$ would weight the between-facet distances more heavily. (See [Section 2.1 supplements](https://github.com/Sayani07/paper-hakear/blob/master/paper/paper-supplementary.pdf) for more details.)

#### Raw distance measure {-}

The raw distance measure, denoted by $\wpdsub{raw}$, is computed after combining all the weighted distance measures appropriately. First, NQT is performed on the measured variable $v_t$ to obtain $v^*_t$ (_data transformation_). Then, for a fixed harmony pair $(A, B)$, percentiles of $v^*_{jk}$ are computed and stored in $q_{jk}$ (_distribution characterization_). This is repeated for all pairs of categories of the form $(a_{j} b_{k}, a_{j'}b_{k'}): \{a_j: j = 1, 2, \dots, \nx\}, B = \{ b_k: k = 1, 2, \dots, \nf\}$. The pairwise distances between pairs $(a_{j} b_{k}, a_{j'}b_{k'})$ denoted by $d_{(jk, j'k')} = JSD(q_{jk}, q_{j'k'})$ are computed (_distance between distributions_). The pairwise distances (_within-facet and between-facet_) are transformed using a suitable _tuning parameter_ ($0<\lambda<1$) depending on if they are within-facet($d_w$) or between-facets($d_b$) as follows:
\begin{equation}\label{def:raw-wpd}
  d*_{(j,k), (j'k')} =
    \begin{cases}
      \lambda d_{(jk), (j'k')},& \text{if } d = d_w;\\
      (1-\lambda) d_{(jk), (j'k')},       & \text{if } d = d_b.
    \end{cases}
\end{equation}
The $\wpdsub{raw}$ is then computed as
$$
  \wpdsub{raw} = \max_{j, j', k, k'}(d*_{(jk), (j'k')}) \qquad\forall j, j' \in \{1, 2, \dots, \nx\},\quad k, k' \in \{1, 2, \dots, \nf\}
$$
The statistic "maximum" is chosen to combine the weighted pairwise distances since the distance measure is aimed at capturing the maximum variation of the measured variable within a panel. The statistic "maximum" is, however, affected by the number of comparisons (resulting pairwise distances). For example, for a $(2, 3)$ panel, there are $6$ possible subsets of observations corresponding to the combinations $(a_1, b_1), (a_1, b_2), (a_1, b_3), (a_2 ,b_1), (a_2 ,b_2), (a_2, b_3)$, whereas for a $(2, 2)$ panel, there are only $4$ possible subsets $(a_1, b_1), (a_1, b_2), (a_2 ,b_1), (a_2 ,b_2)$. Consequently, the measure would have higher values for the panel $(2, 3)$ as compared to $(2, 2)$, since maximum is taken over higher number of pairwise distances.

### Adjusting for the number of comparisons

Ideally, it is desired that the proposed distance measure takes a higher value only if there is a significant difference between distributions across categories, and not because the number of categories $\nx$ or $\nf$ is high. That is, under designs like $\Dnull$, their distribution should not differ for a different number of categories. Only then could the distance measure be compared across panels with different levels. This calls for an adjusted measure, which normalizes for the different number of comparisons.

Two approaches for adjusting the number of comparisons are discussed, both of which are substantiated using simulations. The first one defines an adjusted measure $\wpdsub{perm}$ based on the permutation method to remove the effect of different comparisons. The second approach fits a model to represent the relationship between $\wpdsub{raw}$ and the number of comparisons and defines the adjusted measure ($\wpdsub{glm}$) as the residual from the model.

#### Permutation approach {-}

This method is somewhat similar in spirit to bootstrap or permutation tests, where the goal is to test the hypothesis that the groups under study have identical distributions. This method accomplishes a different goal of finding the null distribution for different groups (panels in our case) and standardizing the raw values using that distribution. The values of $\wpdsub{raw}$ are computed on many ($\nsub{perm}$) permuted data sets and stored in $\wpdsub{perm-data}$. Then $\wpdsub{perm}$ is computed as follows:
\begin{align*}
  \wpdsub{perm} = \frac{\wpdsub{raw} - \text{mean}(\wpdsub{perm-data})}{\text{sd}(\wpdsub{perm-data})}
\end{align*}
where $\text{mean}(\wpdsub{perm-data})$ and $\text{sd}(\wpdsub{perm-data})$ are the mean and standard deviation of $\wpdsub{perm-data}$ respectively. Standardizing $\wpd$ in the permutation approach ensures that the distribution of $\wpdsub{perm}$ under $\Dnull$ has zero mean and unit variance across all comparisons. While this works successfully to make the location and scale similar across different $\nx$ and $\nf$, it is computationally heavy and time consuming, and hence less user-friendly. Hence, another approach to adjustment, with potentially less computational time, is proposed.

<!-- for the $l^{th}$ panel and -->

#### Modeling approach {-}

In this approach, a Gamma generalized linear model (GLM) for $\wpdsub{raw}$ is fitted with the number of comparisons as the explanatory variable. Since, $\wpdsub{raw}$ is a Jensen-Shannon distance, it follows a Chi-square distribution [@Menendez1997-in], which is a special case of a Gamma distribution. Furthermore, the mean response is bounded, since any JSD is bounded by $1$ if a base $2$ logarithm is used [@Lin1991-pt]. Hence, by @Faraway2016-uk, an inverse link is used for the model, which is of the form $y = a+b\times\log(z) + e$, where $y = \wpdsub{raw}$, $z = (\nx \times \nf)$ is the number of groups and $e$ are idiosyncratic errors. Let $\text{E}(y) = \mu$ and $a + b\times\log(z) = g(\mu)$ where $g(\mu)= 1/\mu$ and $\hat \mu = 1/(\hat a + \hat b \log(z))$. The residuals from this model $(y-\hat y) = (y-1/(\hat a + \hat b \log(z)))$ would be expected to have no dependency on $z$. Thus, $\wpdsub{glm}$ is defined as the residuals from this model given by
$$\wpdsub{glm} = \wpdsub{raw} - 1/(\hat a + \hat b\times\log(\nx\times\nf))$$
The distribution of $\wpdsub{glm}$ under $\Dnull$ will have approximately zero mean and a constant variance (not necessarily 1).

#### Combination approach {-}

The simulation results (in Section \ref{sec:simulations}) show that the distribution of $\wpdsub{glm}$ under the null design is similar for high $\nx$ and $\nf$ (levels higher than $5$) but less so for lower values of $\nx$ and $\nf$. An empirical explanation for this is that with smaller $n_x$ and $n_f$, the variance in the values of $wpd_{raw}$ is greater [Figure 13 of supplements](https://github.com/Sayani07/paper-hakear/blob/master/paper/paper-supplementary.pdf). As a result, our modeling approach that fits $wpd_{raw}$ to the number of comparisons ($n_x\times n_f$) performs poorly. In order to address this, a combination approach is proposed where we use a permutation approach for categories with small numbers of levels, and a modeling approach for categories with higher numbers of levels. This ensures that the computational load of the permutation approach is alleviated while maintaining a similar null distribution across different categories. This approach, however, requires that the adjusted variables from the two approaches are brought to the same scale. We define $\wpdsub{glm-scaled} = \wpdsub{glm}\times \sigma^2_{\text{perm}}/\sigma^2_{\text{glm}}$ as the transformed $\wpdsub{glm}$ with a similar scale as $\wpdsub{perm}$. The adjusted measure from the combination approach, denoted by $\wpd$ is then defined as follows:
\begin{equation}
\wpd = \begin{cases}
         \wpdsub{perm},       & \text{if $\nx, \nf \le 5$};\\
         \wpdsub{glm-scaled}  & \text{otherwise}.\\
       \end{cases}
\end{equation}

<!-- ### Algorithm -->

<!-- The distance measure $\wpd$ between two cyclic granularities $A$ and $B$ is aimed to capture the strength of the structure by estimating the maximum within-group and between-group variations. Furthermore, -->

<!-- The steps employed for computing $\wpd$ is summarized as follows: -->

<!-- The values of $\wpd$ indeed depend upon the underlying distribution. This is also not desirable as the intended aim of $\wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. The steps employed for computing the distance measure is summarized as follows: -->

<!-- Hence, differences in distributions of the measured variable are computed between all pairs of categories $(a_{j} b_{k}, a_{j'}b_{k'}): j = 1, 2, \dots, \nx\}, B = \{ b_k: k = 1, 2, \dots, \nf\}$. Percentiles of $v_{jk}$ and $v_{j'k'}$ are computed and stored in $q_{jk}$ and $q_{j'k'}$ respectively. Then the pairwise distances between pairs $(a_{j} b_{k}, a_{j'}b_{k'})$ be denoted as $d_{(jk, j'k')} = JSD(q_{jk}, q_{j'k'})$ is computed. Pairwise distances could be within-facets or between-facets. Figure \ref{fig:distance-explain} illustrates how the within-facet or between-facet distances are defined. Pairwise distances are within-facets ($d_{w}$) when $b_{k} = b_{k'}$, that is, between pairs of the form $(a_{j}b_{k}, a_{j'}b_{k})$ as shown in panel (3) of Figure \ref{fig:distance-explain}. If categories are ordered (like all temporal cyclic granularities), then only distances between pairs where $a_{j'} = (a_{j+1})$ are considered (panel (4)). Pairwise distances are between-facets ($d_{b}$) when they are considered between pairs of the form $(a_{j}b_{k}, a_{j}b_{k'})$. Number of between-facet distances would be $^{\nf}C_2*\nx$ and number of within-facet distances are $\nf*(\nx-1)$ (ordered) and $^{\nx}C_2*\nf$ (un-ordered). The pairwise distances $d_{(jk, j'k')}$ are transformed using a suitable tuning parameter ($0<\lambda<1$) depending on if they are $d_{b}$ or $d_{w}$ as follows: -->



<!-- 5. Compute distribution of maximum distance ($M$) by shuffling the data $200$ times and finishing Steps1-4 in each case. Scale $M$ using the mean and sd of the distribution.

5. Use Steps 1-4 to compute maximum distance for $\forall k \in \{1, 2, \ldots, \nf\}$.

6. Compute the distance measure MMPD_raw = median $(M_1, M_2, \dots, M_K)$.

<!-- All of these distances are then aggregated by taking the maximum from these distances to obtain $\wpd$. -->

<!-- _j$ maps index set to a set $\{B_\ell \mid \ell =1,\dots,L\}$. Here, $A_k$ and $B_\ell$ are the levels/categories corresponding to $C_i$ and $C_j$ respectively. Let $S_{k\ell}$ be a subset of the index set such that for all $s \in S_{k\ell}$, $C_i(s) = A_k$ and $C_j(s) = B_\ell$. There are $KL$ such data subsets, one for each combination of levels ($A_k$, $B_\ell$). Moreover, consider that in the graphical space, $C_i$ is mapped to facets and $C_j$ is mapped to x-axis. -->

 <!-- old -->
<!-- Consider two cyclic granularities $C_i$ and $C_j$, such that $C_i$ maps index set to a set $\{A_k \mid k=1,\dots,\nf\}$ and $C_j$ maps index set to a set $\{B_\ell \mid \ell =1,\dots,L\}$. Here, $A_k$ and $B_\ell$ are the levels/categories corresponding to $C_i$ and $C_j$ respectively. Let $S_{k\ell}$ be a subset of the index set such that for all $s \in S_{k\ell}$, $C_i(s) = A_k$ and $C_j(s) = B_\ell$. There are $KL$ such data subsets, one for each combination of levels ($A_k$, $B_\ell$). Moreover, consider that in the graphical space, $C_i$ is mapped to facets and $C_j$ is mapped to x-axis. -->

<!-- The algorithm employed for computing the distance measure is summarized as follows: -->

<!-- 1. Fix harmony pair $(C_i, C_j)$. -->

<!-- 2. Fix $k$. Then there are $L$ groups corresponding to level $A_k$ of $C_i$. -->

<!-- 3. Compute $m = \binom{L}{2}$ pairwise distances between distributions of $L$ unordered levels and $m = L-1$ pairwise distances for $L$ ordered categories. -->

<!-- 4. Identify maximum within the $m$ computed distances. -->

<!-- <!-- 5. Compute distribution of maximum distance ($M$) by shuffling the data $200$ times and finishing Steps1-4 in each case. Scale $M$ using the mean and sd of the distribution. -->

<!-- 5. Use Steps 1-4 to compute maximum distance for $\forall k \in \{1, 2, \ldots, \nf\}$. -->

<!-- 6. Compute the distance measure MMPD_raw = median $(M_1, M_2, \dots, M_K)$. -->

<!-- \begin{algorithm}[!thb] -->
<!-- 	\caption{Calculation for a raw distance measure between two cyclic granularities $A = \{ a_j: j = 1, 2, \dots, \nx\}$, $B = \{ b_k: k = 1, 2, \dots, \nf\}$ with $A$ placed across x-axis and $B$ across facets.}\label{alg:scoreopt}. -->
<!-- 	\begin{algorithmic}[1] -->
<!-- 		\Procedure{RawMMPD}{$A = \{ a_j: j = 1, 2, \dots, \nx\}$, $B = \{ b_k: k = 1, 2, \dots, \nf\}$, $v = \{ v_t: t = 1, 2, \dots, T\}$}. -->
<!-- 		\For{$k=1:\nf$, $j=1:\nx$} -->
<!-- 		\State Find distances between pairs of all possible combinations of categories $(a_jb_k,a_j'b_k')$ by computing JSD between quantiles of the measured variable $q(v)$ across these combinations. -->
<!-- 		\State $d \gets JSD(\tilde{q(v)_{a_{j}b_{k}}},\tilde{q(v)_{a_{j}'b_{k}'}})$ -->
<!-- 		\If {$b_k = b_k'$} -->
<!-- 		\State $d* \gets \lambda d$ \Comment{upweight within-facet distances} -->
<!-- 		\Else -->
<!-- 		\State $d* \gets 1/\lambda d $ \Comment{downweight across-facet distances} -->
<!-- 		\EndFor -->
<!-- 		\State Set the raw distance measure as $max(d*)$ where max is taken over all $j, j', k, k'$. -->

<!-- 		\EndProcedure -->
<!-- 	\end{algorithmic} -->
<!-- \end{algorithm} -->

<!-- A stronger measure "max" is chosen for aggregating x-axis categories compared to "median" for aggregating facet categories as the measure is intended to put more importance in pointing towards distributional differences between x-axis categories than for facet categories. -->

<!-- ## Properties of $\wpd$ -->

<!-- Simulations were carried out to explore the behavior of $\wpd$ under the following factors that could potentially impact the values of $\wpd$: $\nx$, $\nf$, $\lambda$, $\omega$, $dist$ (normal/non-normal distributions with different location and scale), $ntimes$, and $designs$ and results are presented in two parts. The dependence of $\wpd$ on $\nx$ and $\nf$ under $\Dnull$ is presented here, which lays the foundation for the next section. The rest of the results that discuss the relationship of the $\wpd$ with other factors is presented in details in the Supplementary section of the paper. They show that the designs $\Df$ and $D_x$ intersect at $\lambda = 0.5$ and hence for up-weighing designs of the form $D_x$, $\lambda = 0.67$ has been considered for computation of $\wpd$ in the rest of the paper. -->

<!-- - $\nx$ (number of levels of x-axis) -->
<!-- - $\nf$ (number of levels of facets) -->
<!-- - $\lambda$ (tuning parameter) -->
<!-- - $\omega$ (increment in each panel design) -->
<!-- - $dist$ (normal/non-normal distributions with different location and scale) -->
<!-- - $n$ (sample size for each combination of categories) -->
<!-- - $designs$ ($\Dnull$, $\Df$, $D_x$ and $D_{fx}$) -->

<!-- - $\nsub{sim}$ (number of simulations) -->
<!-- - $\nsub{perm}$ (number of permutations of data)  -->

<!-- ### Simulation design{#sec:sim-study} -->

<!-- Observations are generated from a Gamma(2,1) distribution for each combination of $\nx$ and $\nf$ from the following sets: $nx = \nf = \{2, 3, 5, 7, 14, 20, 31, 50\}$ to cover a wide range of levels from very low to moderately high. Each combination is being referred to as a _panel_. That is, data is being generated for each of the panels $\{nx = 2, \nf = 2\}, \{nx = 2, \nf = 3\}, \{nx = 2, \nf = 5\}, \dots, \{nx = 50, \nf = 31\}, \{nx = 50, \nf = 50\}$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. That is, if we consider the panel $\{nx = 2, \nf = 2\}$, $500$ observations are generated for each of the combination of categories from the panel, namely, $\{(1, 1), (1, 2), (2, 1), (2, 2)\}$. The values -->
<!-- of $\wpd$ is obtained for each of the panels. This design corresponds to $\Dnull$ as each combination of categories in a panel are drawn from the same distribution. Furthermore, the data is simulated for each of the panels $\nsub{sim}=200$ times, so that the distribution oaf $\wpd$ under $\Dnull$ could be observed. $wpd_{l, s}$ denotes the value of $\wpd$ obtained for the $l^{th}$ panel and $s^{th}$ simulation. -->

<!-- ### Results -->

<!-- Figure \ref{fig:raw} shows the distribution of $\wpd$ plotted across different $\nx$ and $\nf$ categories. Since under $\Dnull$, there is no difference in distributions across different categories, we expect the distance measure $\wpd$ to reflect that as well and have the same distribution across categories. But Figure \ref{fig:raw} shows that both the location and scale of the distributions change across panels. This is not desirable under $\Dnull$ as it would mean comparisons of $\wpd$ values is not appropriate across different $\nx$ and $\nf$. Figure \ref{fig:quadratic} shows how the median of $\wpd$ varies with the total number of distances $nx*\nf$ for each panel. The median increases abruptly for lower values of $nx*\nf$ and slowly for higher $nx*\nf$. -->

<!-- Furthermore, simulation results also show that the values of $\wpd$ indeed depend upon the underlying distribution. This is also not desirable as the intended aim of $\wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. -->

<!-- last writeup -->
<!-- In the linear model approach, $wpd\in R$ was assumed, whereas, $\wpd$ is a Jensen-Shannon Distance (JSD) and lies between 0 and 1 [@JSD]. Furthermore, JSD follows a Chi-square distribution, which is a special case of Gamma distribution. Therefore, a generalized linear model could be fitted instead of a linear model to allow for the response variable to follow a Gamma distribution. The inverse link is used when we know that the mean response is bounded, which is applicable in our case since $0 \leq wpd\leq 1$. -->

<!-- We fit a Gamma generalized linear model with the inverse link which is of the form:  -->
<!-- $$y_l = a+b*log(z_l) + e_l$$, where $y_l = median_m(wpd_{l, m})$, $z_l$ is the $l^{th}$ panel and $e_l$ are idiosyncratic errors. Let $E(y) = \mu$ and $a + b*log(z) = g(\mu)$ where $g$ is the link function. Then $g(\mu)= 1/\mu$ and $\hat \mu = 1/(\hat a + \hat b log(z))$. The residuals from this model $(y-\hat y) = (y-1/(\hat a + \hat b log(z)))$ would be expected to have no dependency on $z$. Thus, $\wpdsub{glm}$ is chosen as the residuals from this model and is defined as: -->
<!-- $\wpdsub{glm} = wpd - 1/(\hat a + \hat b*log(nx*\nf))$. -->

<!-- which is more approximate than exact but still has the similar accuracy when compared to the permutation approach. -->

<!-- use later -->
<!-- The measure $\wpd$ could potentially lead to comparison of the measure across different panels and also help distinguishing the interesting panels from a data set. We discuss two approaches for normalization, both of which are substantiated using simulations. -->

<!-- ## Methodology -->

<!-- The transformed ${wpd}$ which is normalized for the values of $\nx$ and $\nf$ is denoted by $\wpd$. Two approaches have been employed - the first one involves a permutation method to make the distribution of the transformed $\wpd$ similar for different comparisons and the second one fits a model to represent the relationship between the two variables and defines $\wpd$ as the residual of the model. -->

<!-- ### Notations -->

<!-- Let $\{nx_{i}, i = 1, 2, \dots, nx\}$, $\{\nf_{j}, j = 1, 2, \dots, \nf\}$ be the set of x-axis and facet categories respectively. Each combination of $nx_{i}$ and $\nf_{j}$is being referred to as a _panel_. Then the total number of panel is $nx*\nf$. Let the total number of pairwise distances that could result in each panel be $\{z_k, k = 1, 2 , \dots, nx*\nf\}$. Here, $\{z_1 = nx_1*\nf_1\}$, $\{z_2 = nx_2*\nf_2\}$ and $\{z_k = nx*\nf\}$. Now, let $\{x_{k,l}, k = 1, 2, \dots, nx*\nf, l = 1, 2, \dots, \nsub{sim}\}$ denote the values of $\wpdsub{raw}$ obtained from the simulation study for $k^{th}$ panel in the $i^{th}$ simulation. Hence, for each of those $k$ panel, we have $\nsub{sim}$ values of $\wpdsub{raw}$. -->

<!-- use soon -->
<!-- ## Properties -->

<!-- This section reports the results of a simulation study that was carried out to evaluate the behavior of $\wpd$ under different designs and other potential factors. The behavior of $\wpd$ is explored in designs where there is in fact difference in distribution between facet categories ($\Df$) or across x-categories ($D_x$) or both ($D_{fx}$). Using $\omega = \{1, 2, \dots, 10\}$ and $\lambda = seq(from = 0.1, to = 0.9, by = 0.05)$, observations are drawn from a N(0,1) distribution for each combination of $\nx$ and $\nf$ from the following sets: $nx = \nf = \{2, 3, 5, 7, 14, 20, 31, 50\}$. $ntimes = 500$ is assumed for this setup as well. Furthermore, to generate different distributions across different combination of facet and x levels, the following method is deployed - suppose the distribution of the combination of first levels of $x$ and $facet$ category is $N(\mu,\sigma)$ and $\mu_{jk}$ denotes the mean of the combination $(a_jb_k)$, then $\mu_{j.} = \mu + j\omega$ (for design $D_x$) and $\mu_{.k} = \mu + k\omega$ (for design $\Df$). -->

<!-- The tabulated values and graphical representations of the simulation results are provided in the Supplementary paper. The learning from the simulations are as follows: The values of $\wpd$ is least for $\Dnull$, followed by -->
<!-- $\Df$, $D_x$ and $D_{fx}$. This is a desirable result since the measure $\wpd$ was designed such that this relationship holds. Furthermore, the distribution of the measure $\wpd$ does not change for different facet and x categories. The distribution of $\wpd$ looks similar with at least the mean and standard of the distributions being uniform across panels. This means $\wpd$ could be used to measure differences in distribution across panels. Also, note that since the data is processed using normal-quantile-transform, this measure is independent of the initial distribution of the underlying data and hence is also comparable across different data sets. This is valid for the case when sample size $ntimes$ for each combination of categories is at least 30 and $\nsub{perm}$ used for computing $\wpd$ is at least 100. More detailed results about the properties of $\wpd$ could be found in the Supplementary paper. -->

## Ranking and selection of cyclic granularities {#sec:rank-wpd}

A cyclic granularity is referred to as "significant" if there is a significant distributional difference of the measured variable between different categories of the harmony. In this section, a selection criterion to choose significant harmonies is provided, thereby eliminating all harmonies that exhibit non-significant differences in the measured variable. The distance measure $\wpd$ is used as a test statistic to test the null hypothesis that no harmony/cyclic granularity is significant. We select only those harmonies/cyclic granularities for which the test fails. They are then ranked based on how well they capture variation in the measured variable.

<!-- . Randomization tests (permutation tests) generates a random distribution by re-ordering the observed data and allows us to test if the observed data is significantly different from any random distribution. -->
<!-- the harmonies are not significant under null hypothesis and then test if our hypotheses are true: -->

<!--  $H_0$: harmonies are not significant -->
<!--  $H_1$: at least one harmony is significant -->

<!-- we provide a method to select important harmonies by eliminating all harmonies for which patterns are not significant by employing randomization test. Randomization tests (permutation tests) generates a random distribution by re-ordering our observed data and allow to test if the observed data is significantly different from any random distribution. Complete randomness in the measured variable indicates that the process follows a homogeneous underlying distribution over the whole time series, which essentially implies there is no interesting distinction across any different categories of the cyclic granularities. -->

### Selection

<!-- Under the null hypothesis, the harmonies are not significant. -->

A threshold (and consequently a selection criterion) is chosen using the notion of randomization tests [@edgington2007randomization]. The data is permuted several times and $\wpd$ is computed for each of the permuted data sets to obtain the sampling distribution of $\wpd$ under the null hypothesis. If the null hypothesis is true, then $\wpd$ obtained from the original data set would be a likely value in the sampling distribution. But in case the null hypothesis is not true, then it is less probable that $\wpd$ obtained for the original data will be from the same distribution. This idea is utilized to come up with a threshold for selection, denoted by $\wpdsub{threshold}$, defined as the $99^{th}$ percentile of the sampling distribution. A harmony is selected if the value of $\wpd$ for that harmony is greater than the chosen threshold. The detailed algorithm for choosing a threshold and selection procedure (for two cyclic granularities) is listed as follows:

<!-- The selection criterion is stated as follows: for $(i \in {1, 2, \dots, H_{N_C}})$, select $H_i$ if $wpd_i > \wpdsub{threshold}$, else reject. -->

- **Input:** All harmonies of the form $\{(A, B),~ A = \{ a_j: j = 1, 2, \dots, \nx\},~ B = \{ b_k: k = 1, 2, \dots, \nf\}\}$, $\forall (A, B) \in H_{N_C}$.

- **Output:** Harmony pairs $(A, B)$ for which $\wpd$ is significant.

1. For each harmony pair $(A, B) \in H_{N_C}$, the following steps are taken.

    a. Given the measured variable; $\{v_t: t=0, 1, 2, \dots, T-1\}$, $\wpd$ is computed and is represented by $\wpd^{A, B}_{obs}$.
    b. For $i=1,\dots,M$, randomly permute the original time series: $\{v_t^{i}: t=0, 1, 2, \dots, T-1\}$ and compute $\wpd^{A, B}_{i}$ from $\{v_t^{i}\}$.
    c. Define $\wpdsub{sample} = \{\wpd^{A, B}_{1}, \dots, \wpd^{A, B}_{M}\}$.

2. Stack the $\wpdsub{sample}$ vectors as $\wpdsub{sample}^{\text{all}}$ and compute its $p = 100(1-\alpha)$ percentiles as $\wpdsub{threshold\emph{p}}$.
3. If $\wpd^{A, B}_{obs} > \wpdsub{threshold\emph{p}}$, harmony pair $(A, B)$ is selected at the $1-p/100$ level, otherwise rejected.
4. Harmonies selected using the $99^{th}$, $95^{th}$ and $90^{th}$ thresholds are tagged as \*\*\*, \*\*, \* respectively.

### Ranking {#sec:ranking}

The distribution of $\wpd$ is expected to be similar for all harmonies under the null hypothesis, since they have been adjusted for different number of categories for the harmonies or underlying distribution of the measured variable. Hence, the values of $\wpd$ for different harmonies are comparable and can be used to rank the significant harmonies. A higher value of $\wpd$ for a harmony indicates that higher maximum variation in the measured variable is captured through that harmony.


```{r varall-new}

```

```{r varx-new}

```

```{r varf-new}
```

```{r null-new}

```

Figure \ref{fig:null4by2} also presents the results of $\wpd$ from the illustrative designs in Section \ref{sec:computation-wpd}. The value of $\wpd$ under null design (a) is the least, followed by (b), (c) and (d). This aligns with the principle of $\wpd$, which is expected to have lowest value for null designs and highest for designs of the form $\Dfx$ (d). Moreover, note the relative differences in $\wpd$ values between (b) and (c). The value of the tuning parameter $\lambda$ is set to 2/3, which gives greater emphasis to differences in x-axis categories than facets.

Again consider Figures \ref{fig:onegran-new}(a) and \ref{fig:onegran-new}(b) with a $\wpd$ value of 20.5 and 145 respectively. This is because there is a more gradual increase across hours of the day than across months of the year. If the order of categories is ignored, the resulting $\wpd$ values are $97.8$ and $161$ respectively, because differences between any hours of the day tend to be larger than differences only between consecutive hours. Similarly, Figures \ref{fig:id2-new2}(a) and (b) have $\wpd$ values of $110.79$ and $125.82$ respectively. The ranking implies that the distributional differences are more prominent for the second household, as is also seen from the bigger fluctuations in the $90^{th}$ percentile than for the first household.

<!-- ## Choosing a threshold -->

<!-- A randomization test involves calculating a test statistic, randomly shuffling the data and calculating the test statistic several times to obtain a distribution of the test statistic. But we will use this procedure to obtain a threshold such that harmony pairs with a $\wpd$ value higher than this threshold will only be considered significant. The process of choosing a threshold is described as follows: -->

<!-- to test if there is any interesting pattern captured by the harmonies, which essentially implies if $\wpd$ is significantly different from zero. The percentages of times the $\wpd$ obtained from the permuted data is greater than or equal to the observed $\wpd$ is the p-value. The randomization test is described as follows: -->

<!-- <!-- We can remove the harmonies for which no interesting patterns are observed through a randomization permutation method. --> 
<!-- Essentially, the assumption is that under the null hypothesis, there is no difference in categories between the pair of cyclic granularities in the chosen harmony. This method is based on the generation of randomly chosen reassignments (permutations) of the data across different cyclic granularities and the computation of -->
<!-- $\wpd$ for each of these reassignments. -->

<!-- **Assumption:** random permutation of the data keeping the categories of the cyclic granularities constant. -->

<!-- Complete randomness in the measured variable indicates that the process follows a homogeneous underlying distribution over the whole time series, which essentially implies there is no interesting distinction across any different categories of the cyclic granularities. We can remove the harmonies for which no interesting patterns are observed through a randomization permutation method. Essentially, the assumption is that under the null hypothesis, there is no difference in categories between the pair of cyclic granularities in the chosen harmony. This method is based on the generation of randomly chosen reassignments (permutations) of the data across different cyclic granularities and the computation of -->
<!-- $\wpd$ for each of these reassignments. The percentages of times the theoretical distribution greater than or equal to the respective observed $\wpd$ values are calculated and are used to obtain the P value. The procedure for the permutation test is: -->

<!-- **Assumption:** random permutation of the data keeping the categories of the cyclic granularities constant. -->

<!-- 14, 20, 31, 50\}$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. The values of $\wpd$ is obtained for each of the panels for the designs $\Dnull$, $D_x$, $\Df$ and $D_{fx}$. $\wpdsub{threshold}$ is computed from all of these panels together and the number of times a harmony pair $(A, B) \in H_{N_C}$ is selected when in fact it was of the design $\Dnull$ is noted. This entire process is repeated for several null data sets to see the number of times any harmony pair $(A, B) \in H_{N_C}$ is selected under null. -->

<!-- ### Results -->

### Simulations {#sec:simulations}

Simulations were carried out to explore how the behavior of $\wpd$ as $\nx$ and $\nf$ were varied, in order to compare and evaluate different normalization approaches for both $m=1$ and $m=2$. Here the simulation design and results corresponding to $m=2$ are presented. Similar design and results for $m=1$, although important, are not included in the paper but in the [supplements](https://github.com/Sayani07/paper-hakear/blob/master/paper/paper-supplementary.pdf) along with other more detailed simulation results.

#### Simulation design {-}

<!-- \noindent $m=1$ -->

<!-- \noindent Observations were generated from a N(0,1) distribution for $\nx \in \{2, 3, 5, 7, 9, 14, 17, 20, 24, 31, 42, 50\}$, with $\nsub{times} = 500$ observations drawn for each combination of categories.  Let $\wpd_{\ell, s}$ denote the value of $\wpd$ obtained for the $\ell^{th}$ panel and $s^{th}$ simulation. -->

<!-- \noindent $m=2$ -->

\noindent Observations were generated from a N(0,1) distribution for each combination of $\nx$ and $\nf$ from $\{2, 3, 5, 7, 14, 20, 31, 50\}$, with $\nsub{times} = 500$ observations drawn for each of the $64$ combinations. This design corresponds to $\Dnull$. For each of the categories, there were $\nsub{sim}=200$ replications, so that the distribution of $\wpd$ under $\Dnull$ could be observed.

#### Results {-}

Figure \ref{fig:raw} shows that both the location and scale of the distributions change across panels. This is not desirable under $\Dnull$ as it would mean comparisons of $\wpd$ values are not appropriate across different $\nx$ and $\nf$ values. Table \ref{tab:glm-tab} gives the summary of a Gamma generalized linear model to capture the relationship between $\wpdsub{raw}$ and the number of comparisons. The intercepts and slopes are similar, independent of the underlying distributions (see [Table 3 supplementary paper](https://github.com/Sayani07/paper-hakear/blob/master/paper/paper-supplementary.pdf) for details) and hence the coefficients are shown for the case when observations are drawn from a N(0,1) distribution. Figure \ref{fig:dist-new-same-scale-link} shows the distribution of $\wpdsub{perm}$ and $\wpdsub{glm-scaled}$ on the same scale to show that a combination approach could be used for higher values of $\nx$ and $\nf$ to alleviate the computational time of the permutation approach.

(ref:raw) Distribution of $\wpdsub{raw}$ is plotted across different $\nx$ and $\nf$ categories under $\Dnull$ through density and rug plots for $m=2$. Both location (blue line) and scale of the distribution shifts for different panels. This is not desirable since under the null design, the distribution is not expected to capture any differences.

```{r raw, fig.cap = "(ref:raw)", fig.pos="!htb"}
```

```{r glm-tab}

```

```{r wpd-glm-dist}

```

```{r hist-qq-new, eval = FALSE}

```

```{r quadratic, eval=FALSE}
```

```{r norm, eval}
```

(ref:dist-new-same-scale-link) The distributions of $\wpdsub{perm}$ and $\wpdsub{glm-scaled}$ are overlaid to compare the location and scale across different $\nx$ and $\nf$ for $m=2$. $\wpdsub{norm}$ takes the value of $\wpdsub{perm}$ for lower levels, and $\wpdsub{glm-scaled}$ for higher levels to to alleviate the problem of computational time in permutation approaches. This is possible as the distribution of the adjusted measure looks similar for both approaches for higher levels.

```{r dist-new-same-scale-link, fig.cap = "(ref:dist-new-same-scale-link)"}
```

These results justify our use of the permutation approach when $\nx \leq 5$ and $\nf \leq 5$, and the use of the GLM otherwise.
<!--
## Choosing the threshold

### Simulation design {-}
 
Observations were generated from a N(0,1) distribution for each combination of $\nx$ and $\nf$ from the following sets: $nx \in \{3, 7, 14\}$ and $\nf \in \{2, 9, 10\}$, giving nine possible panels/combinations. In the first scenario, data for all panels were simulated using the null design $\Dnull$. In other scenarios, data simulated from the panel $(14, 2)$ and $(3, 10)$ used $\Dfx$ where the means of the distributions increase by $\omega$ across both x-axis and facets, where $\omega \in \{0.5, 2, 5\}$. This allows us to examine if the proposed test is able to capture subtle differences and obvious differences when we shift from the null design.

In the last scenario, we consider the panel $(3, 2), (7,9), (14, 10)$ to be under $\Dnull$, the panels $(7, 2), (14, 9)$ to be under $\Df$. $(14, 2), (3, 10)$ under $D_x$ and the rest under $\Dvar{{null}}$. This is done to check if the consequent ranking procedure leads to designs like $D_{vary_{all}}$ to be chosen first followed by $D_{vary_{all}}$. We generate only one data set each for which these scenarios were simulated and consider this as the original data set. We generate $1000$ repetitions of this experiment with different seeds.

### Results {-}

For the first scenario, we set $\wpdsub{threshold99}$ so that the probability of rejecting each panel under $\Dnull$ is $0.01$. We have 9 tests, so the size of the tests overall is $1-0.99^9 \approx 0.09$. We also compute the proportion of times a panel is rejected when it belongs to a non-null design to obtain the power of the test. The detailed results and graphics are included in the Supplementary paper.

```{r, eval = FALSE}
all_data <- read_rds("simulations/supplementary/test-hpc/data-agg/all_data.rds")

# computes size for 99 percentile threshold
data_summary <- all_data %>%
 mutate(sig = if_else(significance!=99, 0, 99)) %>%
 group_by(seed_id) %>%
 summarize(sumsig = sum(sig)) %>%
 count(sumsig==0) %>%
 mutate(p_value = n/sum(n)) %>%
 slice(1)

# plots
ggplot() + 
 geom_histogram(data = all_data, aes(x = wpd)) +
 geom_vline(data = data_summary, aes(xintercept = p_value),color = "red") +
 facet_grid(x_levels~facet_levels)

```
 -->

## Application to residential smart meter dataset {#sec:application-wpd}

The smart meter data set for eight households in Melbourne was procured by downloading individual level data from energy suppliers/retailers. To reflect various energy behaviours, data was obtained from friends and colleagues with different professions, lifestyles, and household sizes. This level of data is private, and we thank our friends and colleagues for agreeing to have their data used for this analysis. The data has been cleaned to form a `tsibble` [@wang2020tsibble] containing half-hourly electricity consumption from July to December 2019 for each of the households. No behavioral pattern is likely to be discerned from the time plot of energy usage over the entire period, since the plot will have too many observations squeezed in a linear representation. When we zoom into the September 2019 data in Figure \ref{fig:dotplot-8}(b), some patterns are visible in terms of peaks and troughs, but we do not know if they are regular or what is their period.

(ref:dotplot-8) An ensemble plot with a heatmap (a), line plot (b), parallel coordinate plot (c) to demonstrate energy behavior of the households in different ways. (a) and (b) display the household ids in the same order as indicated by the same colour of the line plot in (b) and box border in (a). (b) shows the raw demand series for September to highlight the repetitive patterns of energy demand. (a) shows $\wpd$ values across harmonies where a darker shade of red indicates a higher ranking harmony and gray represents a clash. A significant harmony is shown with an asterisk. For example, ids 7 and 8 have significant patterns across the pair (*hod*, *dow*) since the tile with *hod* as facet and *dow* as x variable has an asterisk. (c) is useful for comparing households across harmonies. For example, for the harmony (*dow*-*hod*), ids 1 and 7 have the least and highest $\wpd$ respectively. 

```{r dotplot-8, fig.cap = "(ref:dotplot-8)", fig.height = 7}
```

Electricity demand, in general, has daily, weekly and annual seasonal patterns. However, it is not apparent from this view if all households have those patterns, or how strong they are in each case. It is also not clear from this view if any other periodic patterns are present in any household. We start the analysis by choosing a few harmonies, and ranking them for each of these households. The ranking and selection of significant harmonies is validated by analyzing the distribution of energy usage across significant harmonies.

<!-- if we are benefiting by moving from a linear to cyclic representation of time to . -->

```{r linear-scale-8, fig.cap = "Electricity demand for eight households is shown in different facets from Jul-19 to Dec-19 in Fig a and it has been zoomed in for Sep-19 in Fig b, where a week in Sep-19 has been highlighted. From the scales of Fig a, it is apparent that these households vary in consumption levels, but all their periodic patterns have been squeezed with this representation. In fig b, we can see some weekly patterns for the entire period and daily patterns for the highlighted week and there could be some other patterns visible in terms of peaks and troughs, but it is not clear if they are regular or significant.", fig.height=8, eval = FALSE}

```

#### Choosing cyclic granularities of interest and removing clashes {-}

Let $v_{i, t}$ denote the electricity demand for the $i^{th}$ household in time period $t$. The series $\{v_{i, 1},\dots,v_{i,T}\}$ is the linear granularity corresponding to half-hour since the interval of the `tsibble` is 30 minutes. We consider coarser linear granularities like hour, day, week and month from the commonly used Gregorian calendar. From the four linear granularities of hour, day, week, and month, we obtain $N_C = 4\times 3/2 = 6$ cyclic granularities:  "hour_day", "hour_week", "hour_month", "day_ week", "day_month" and "week_month" abbreviated as *hod*, *how*, *hom*, *dow*, *dom* and *wnwd* respectively. Further, we add cyclic granularity day-type ("wknd wday") to capture weekend and weekday behavior. Thus, seven cyclic granularities are considered to be of interest. The pairs of cyclic granularities ($C_{N_C}$) will have $7\times 6=42$ elements. The set of possible harmonies $H_{N_C}$ from $C_{N_C}$ are chosen by removing clashes using procedures described in @Gupta2021-gravitas. Table \ref{tab:tab-rank-8} shows $14$ harmony pairs that belong to $H_{N_C}$.

```{r rank-household-mclapply-8}
```

```{r elec_select_harmony-8}
```

```{r tab-rank-8, dependson="dotplot-8"}
```

 <!-- The paper also shows how the number of harmonies could be identified among by removing clashes. This section shows how we could refine the search further by only looking at significant harmonies. -->

#### Selecting and ranking harmonies for all households {-}

$\wpd_{i}$ is computed on $v_{i, t}$ for all harmony pairs $\in H_{N_C}$ and for each household $i \in \{1, 2, \dots, 8\}$. The harmony pairs are then arranged in descending order and highlighted with `***`, `**` and `*` corresponding to the $99^{th}$, $95^{th}$ and $90^{th}$ percentile threshold. Table \ref{tab:tab-rank-8} shows the rank of the harmonies for different households. The rankings are different for different households, which is a reflection of their varied behaviors. Most importantly, there are at most three harmonies that are significant for any household. This is a huge reduction in the number of potential harmonies to explore.

#### Detecting patterns not apparent from linear display {-}

Figure \ref{fig:dotplot-8} helps to compare households through the heatmap (a) across harmony pairs. Each household is represented by 25 tiles, each tile representing a pair of cyclic granularities. The colors (in shades of red) represent the value of $\wpd$ for each of the harmony pairs (in Table \ref{tab:tab-rank-8}) and the gray tiles correspond to clashes. A darker shade of red corresponds to higher values of $\wpd$. Those with `*` correspond to $\wpd$ values above $\wpdsub{threshold95}$. 

We can now see some patterns that were not discernible in Panel (b), including:

  1. id 7 and 8 have the same significant harmonies despite having very different total energy usage.
  2. id 6 and 7 differ in the sense that for id 6, the difference in patterns is only during weekday/weekends, whereas for id 7 all or few other days of the week are also important. This might be due to their flexible work routines or different day-off.
  3. There are no significant periodic patterns for id 5 when we fix the threshold to $\wpdsub{threshold95}$.

Note that the $\wpd$ values are computed over the entire range, but the linear display in (b) is only for September, with the major and minor x-axis corresponding to weeks and days respectively.

<!-- Households id 2 and 3 are similar, in terms of linear display and heatmap, which implies that similar periodic behavior can stem from households with different demographics. -->


#### Comparing households and validating rank of harmonies {-}

Figure \ref{fig:dotplot-8}(c) shows a parallel coordinate plot across different harmonies with harmonies arranged from highest to lowest $wpd$ values averaged over all households. This display is useful for comparing households across harmonies. For example, for the harmony pair (*dow*-*hod*), household id 7 has the greatest value of $\wpd$, while id 1 has the least. From Table \ref{tab:tab-rank-8} it can be seen that the harmony pair (*dow*, *hod*) is important for id 7; however, it has been labeled as an insignificant pair for id 1. The distribution of energy demand for both of these households, with *dow* as the facet and *hod* on the x-axis, may help explain the choice. Figure \ref{fig:gravitas-plot-8} demonstrates that for id 7, the median (black) and quartile deviation (orange) of energy consumption fluctuates for most hours of the day and days of the week, while for id 1, daily patterns are more consistent within weekdays and weekends. As a result, for id 1, it is more appropriate to examine the distributional difference solely across (*dow*, *wnwd*), which has been rated higher in Table \ref{tab:tab-rank-8}.

<!-- Although the differences might seem significant at first, with closer inspection it could be seen that the scale of the demand is lower in this case and hence the differences are not large enough to cross the threshold for significance. -->

<!-- Figure \ref{fig:gravitas-plot-8} is used to show -->
<!-- if this selection and ranking of harmony pairs makes sense for this household. Panel a) of Figure \ref{fig:gravitas-plot-8} shows the distribution of energy demand with weekday/weekend as the x-axis and hour-of-day as the facets and helps to compare the weekend/weekday patterns for different hours of the day. It could be observed that the difference between weekend and weekday is the highest from 15 to 19 hours of the day. Panel b) shows the distribution of energy demand with the variables swapped and helps to compare the daily patterns within weekday and weekend. It could be observed that the daily pattern is similar for weekdays and weekends with a morning and evening peak. However, the difference between morning and evening peaks are higher for weekends. Since $\wpd$ is designed to put more weightage on within-facet differences for $\lambda>0.5$, it makes sense that the pair (hod, wnwd) has been ranked higher than (wnwd, hod). Panel c) shows the distribution of energy demand with weekday/weekend as the x-axis and week-of-month as the facets. Although the differences might seem significant at first, with closer inspection it could be seen that the scale of the demand is lower in this case and hence the differences are not large enough to cross the threshold for significance. -->

(ref:gravitas-plot-8) Comparing distribution of energy demand shown for household id 1 (a) and 7 (b) on logarithmic scale across *hod* in x-axis and *dow* in facets through quantile area plots. The value of $\wpd$ in Table 3 suggests that the harmony pair (*dow*, *hod*) is significant for household id 7, but not for id 1. This implies that distributional differences are captured more by this harmony for id 7, which is apparent from this display with more fluctuations across median and 75th percentile for different hours of the day and day of week. For id 1, patterns look similar within different days of weekdays and weekends. Here, the median is represented by the black line, the orange area corresponds to quartile deviation and the green area corresponds to area between $10^{th}$ and $90^{th}$ quantile. The display with *hod* vs *wnwd* would have shown higher values of wpd for id:1 (as can be verified from Table 3).


```{r gravitas-plot-8, message = FALSE, warning=FALSE,fig.cap="(ref:gravitas-plot-8)", fig.pos="!htb"}

```

<!-- From Figure \ref{fig:gravitas-plot-8-id5}, it could further be observed that id5 has only one significant harmony (*hod*, *dow*). Apparently, $(hod, wnd/wday)$ which is an important harmony for most households is not important for this one. -->

## Discussion

Exploratory data analysis involves many iterations of finding and summarizing patterns. With temporal data available at finer scales, exploring time series has become overwhelming with so many possible granularities to explore. A common solution is to aggregate and look at the patterns across the usual granularities such as hour-of-day or day-of-week, but there is no way to know the “interesting” granularities a priori. A huge number of displays need to be analyzed or we might end up missing informative granularities. This work refines the search for informative granularities by identifying those for which the differences between the displayed distributions are greatest and rating them in order of importance of capturing maximum variation.

The significant granularities across different datasets (individuals/subjects) do not imply similar patterns across different datasets. They simply mean that maximum distributional differences are being captured across those granularities. A future direction of work is to be able to explore and compare many individuals/subjects together for similar patterns across significant granularities.

_Computational time _

<!-- A rough estimate of working with $\wpd$ working with two granularities with $(l,m)$ levels: ~(30s - 300s for 6<=l,m<=50) Computing $\wpd$ is computationally more expensive for cyclic granularities with lower levels $\nx,\nf<=5$. -->
<!-- patterns are interesting and others are not. The subjects for which behaviors across a group of cyclic granularities are similar, could be grouped together as their periodic behavior is similar. to group subjects -->
The computation time of $\wpd$ for a cyclic granularity or harmony is determined by its levels. For example, in a harmony with $6 \leq\nx,\nf \leq 50$, the computation time grows with the number of levels and ranges from $30-300$s. $\wpd$ becomes computationally more expensive for cyclic granularities with lower levels $\nx,\nf \leq 5$. It is around $40-50$ minutes for $\nx,\nf = \{2,3\}$. Even with parallel computation, computing $\wpd$ for many individuals and cyclic granularity/harmonies takes a long time. One solution could be to obtain the significant granularities for a smaller number of individuals in order to get a sense of the significant granularities for the context and then computing $\wpd$ only for those granularities for other individuals. <!--For example, in the case of electricity data, the important granularities for most people were *wnwd*, *hod*, *dow*, and *dom*. After this fact has been established for a few people, $\wpd$ can be computed exclusively for these granularities to save time for others.--> However, information may be lost for individuals with different sets of significant granularities. In any case, additional studies may be conducted to speed up the processing of cyclic granularities with low levels.



## Acknowledgments {-}

The authors thank the ARC Centre of Excellence for Mathematical and Statistical Frontiers [(ACEMS)](https://acems.org.au/home) for supporting this research. Sayani Gupta was partially funded by [Data61 CSIRO](https://data61.csiro.au/) during her PhD. The Github repository, [github.com/Sayani07/paper-hakear](https://github.com/Sayani07/paper-hakear), contains all materials required to reproduce this article and the code is also available online in the supplemental materials. This article was created with R [@R-language], `knitr` [@knitr; @R-knitr] and `rmarkdown` [@rmarkdown; @R-rmarkdown]. Graphics are produced with `ggplot2` [@Wickham2009pk].

## Supplementary materials {- #supplement-hakear}

**Data and scripts:** Data sets and R code to reproduce all figures in this article (main.R).

\noindent
**Supplementary paper:** All simulation tables, graphics and and R codes to reproduce the supplementary paper (paper-supplementary.Rmd, paper-supplementary.pdf).

\noindent
**R-package:** The open-source R package `hakear` is available on Github (https://github.com/Sayani07/hakear) to implement ideas presented in this paper.

