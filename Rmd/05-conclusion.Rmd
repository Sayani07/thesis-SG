# Conclusion and future plans {#ch:conclusion}

In this thesis, I present a systematic approach for visualizing and analyzing distributions of large temporal data by leveraging time characteristics. The building blocks of this framework are presented in each chapter. Chapter \@ref(ch:gravitas) contains tools for computing all cyclic granularities, as well as a recommendation system for selecting pairs of granularities that can be effectively analyzed together. These temporal granularities can be used to generate data visualizations to look for patterns, associations, and anomalies. But it is difficult to decide which of the various granularities to display when there are many options. Chapter \@ref(ch:hakear) presents a methodology for selecting displays that are interesting, such that differences between displayed distributions are greatest. The methodology also ranks the displays in order of priority for capturing the most variation. Both \@ref(ch:gravitas) and \@ref(ch:hakear) are used for studying patterns in individual time series or comparing a few time series together. This is extended in Chapter @ref(ch:gracsr) to allow for the exploration of distributions for multiple time series at the same time using unsupervised clustering. The clustering methodology produces small groups of time series with similar distributions over multiple granularities. This technique is more robust to noisy, patchy, and uneven length time series because it makes use of probability distributions.

<!-- is more comprehensive in recognizing clusters with recurring patterns over many important granularities and -->

```{r software-initial, echo = FALSE, cache = FALSE, include = FALSE}
read_chunk('scripts/software.R')
```

## Software development

```{r software-impact}
```

This thesis focuses on integrating research approaches into open source R packages for reproducibility and ease of use in other applications. So a significant amount of work has been devoted to the development of R packages  **gravitas**, **hakear**, and **gracsr**, each of which correspond to a chapter presented in this thesis.

<!-- Figure \@ref(fig:software-ghcommits) gives an overview of my Git commits to these repositories, and Figure \@ref(fig:software-downloads) shows the daily downloads of the packages from the RStudio mirror (one of 90 CRAN mirrors) since they were available on CRAN. -->

<!-- ```{r software-ghcommits, fig.cap = "(ref:software-ghcommits-cap)"} -->
<!-- include_graphics("img/pkg-commits.png") -->
<!-- ``` -->

<!-- (ref:software-ghcommits-cap) Patterns of my package development effort during my PhD years based on Git commits to three repositories, sugrrants, tsibble, and mists. Scatter plots of weekly totals are overlaid with a loess smoother. The **sugrrants** package was the first project with much initial energy, followed by small constant maintenance. The **tsibble** package has been a major project with ongoing constant development and bursts of effort in response to users' feedback. The **mists** package has been a recent intense project. -->

<!-- # ```{r software-downloads, fig.height = 3, fig.cap = "(ref:software-downloads-cap)"} -->
<!-- # ``` -->
<!-- #  -->
<!-- # (ref:software-downloads-cap) Impact of these works (sugrrants and tsibble) as measured by daily downloads (on square root scale) from the RStudio mirror since they landed on CRAN. The **tsibble** package has an increasing trend, suggesting the steady adoption of the new data structure. -->

### gravitas

The **gravitas** package provides very general tools to compute and manipulate cyclic granularities and generate plots displaying distributions conditional on those granularities. The functions `search_gran()`, `create_gran()`, `harmony()`, `gran_advice()` and `prob_plot()` provides a workflow for an analyst to systematically explore large quantities of temporal data across different harmonies (pairs of granularities that can be analyzed together.) This package was developed as part of my internship at Google Summer of Code, 2019. It and has been on CRAN since January 2020. The website (<https://sayani07.github.io/gravitas>) includes full documentation and two vignettes about the package usage. There has been a total of 12K downloads from the RStudio mirror dating from 2020-11-01 to 2021-11-01. This package supplements the paper corresponding to \@ref(ch:gravitas), which has won the ACEMS Business Analytics Award 2021. The package can be generalized to non-temporal applications for which a hierarchical structure can be construed similar to time. 

### hakear 

The R package hakear (https://github.com/Sayani07/hakear) implements ideas presented in Chapter @ref (ch:hakear). The function wpd() computes the weighted pairwise distances ($wpd$) for each cyclic granularity or pair of granularities, and select harmonies() selects the ones with significant patterns and ranks them from highest to lowest $wpd$. These selected harmonies can be plotted using package `gravitas` for potentially interesting displays.
Because $wpd$ is computationally expensive, this package uses the R package `parallel` to handle parallel computations.

### gracsr

The open-source R package `gracsr` is available on Github (https://github.com/Sayani07/gracsr) to implement ideas presented in Chapter \@ref(ch:gracsr). The package contains functions for implementing the complete clustering methodology with choices of scaling and distance metrics discussed in the paper. It has received an AUD 3000 grant as part of the ACEMS Business Analytics Prize towards polishing the functions and preparing it for CRAN.


## Future work

### Putting all functionalities on CRAN

I plan to integrate `hakear` with `gravitas` as one R package to systematically exploring few time series, whereas `gracsr` would provide the clustering framework for exploring many time series together. I plan to run it through https://ropensci.org/software-review/ to develop them further for more visibility and efficient usage.


### Scaling up the clustering method to incorporate large uncertainty and improved computational efficiency that comes with large data

With the volume of data projected to grow in the future, potentially leading to increased variability in patterns, research is needed to understand the issues that may arise with the existing methodology and how to adapt our current algorithm. Computational efficiency is also critical when scaling up for analysis of huge data sets, let alone when adding features to existing highly dimensional data structures.

### Comparing our clustering method with other benchmark methods

Testing needs to be carried out with non-hierarchy based clustering methods to see whether these distances perform better with other algorithms. Also, to evaluate how much information is lost when aggregating individual level demand versus distributions.

### Check generalizability of our methods

We provide solutions that are realistically applicable to any temporal data observed more than once per year. We could just verify its value inÂ the electricity smart meter context. With numerous open-source benchmark data sets accessible, it is necessary to test how well the approaches operate in various disciplines.


## Final words

