# Conclusion {#ch-conclusion}

This thesis presents methods for visualizing and analyzing distributions of large temporal data by deconstructing time into temporal granularities. This chapter summarizes the thesis content, outlines the original contributions, software development, and possible directions for future research.

## Original contributions

<!-- In this thesis, I present a systematic approach for visualizing and analyzing distributions of large temporal data by leveraging time characteristics. The building blocks of this framework are presented in each chapter. Chapter \@ref(ch-gravitas) contains tools for computing all cyclic granularities, as well as a recommendation system for selecting pairs of granularities that can be effectively analyzed together. These temporal granularities can be used to generate data visualizations to look for patterns, associations, and anomalies. But it is difficult to decide which of the various granularities to display when there are many options. Chapter \@ref(ch-hakear) presents a methodology for selecting displays that are interesting, such that differences between displayed distributions are greatest. The methodology also ranks the displays in order of priority for capturing the most variation. The ideas in Chapters \@ref(ch-gravitas) and \@ref(ch-hakear) can be used for studying patterns in individual time series or comparing a few time series together. This is extended in Chapter \@ref(ch-gracsr) to allow for the exploration of distributions for multiple time series at the same time using unsupervised clustering. The clustering methodology produces small groups of time series with similar distributions over multiple granularities. This technique is more robust to noisy, patchy, and uneven length time series because it makes use of probability distributions. -->

Exploratory time series analysis entails numerous iterations of identifying and summarizing temporal dependencies. It is common practice to divide time into years, months, weeks, days, and so on in order to make inferences at both finer and coarser scales. In the literature, the formalization of these temporal deconstructions (granularities) is limited to linear time granularities such as hours, days, weeks, and months that respect the linear progression of time and are non-repeating. Cyclic granularities that are repeating in nature are useful for finding patterns in temporal data. They can be circular, quasi-circular, or aperiodic in nature. Hour-of-the-day and day-of-the-week are examples of circular granularities; the day-of-the-month is an example of a quasi-circular granularity; and public holidays and school holidays are examples of aperiodic granularities. Additionally, time deconstructions can be based on a time hierarchy. Thus, single-order-up granularities such as second of minute or multiple-order-up granularities such as second of hour can be envisioned. The definitions and rules defined in the literature for linear granularities are insufficient for describing various types of cyclic granularities. 


Chapter \@ref(ch-gravitas) provides a formal characterization of cyclic granularities as well as tools for classifying and computing potential cyclic granularities from an ordered temporal index. It also allows for the manipulation of single- and multiple-order-up time granularities via cyclic calendar algebra. The approach is generalizable to non-temporal hierarchical granularities with an ordered index. Visualizing probability distributions conditional on one or more cyclic granularities is a powerful exploration tool. However, there may be too many cyclic granularities to look at manually for comprehensive exploration, and not all pairs of granularities can be effectively explored together. Chapter \@ref(ch-gravitas) also provides a recommendation on whether a pair of granularities can be meaningfully plotted or analyzed together (a "harmony") or when they cannot (a "clash" or "near-clash").

Cyclic granularities could be used to create a wide range of displays. And, when there are numerous granularities to choose from, deciding which one to display can be difficult. Moreover, only a few of them may be useful in revealing major patterns. In Chapter \@ref(ch-hakear), the search for informative granularities is facilitated by selecting "significant" granularities. A cyclic granularity is referred to as "significant" if there is a significant distributional difference of the measured variable between different categories. Chapter \@ref(ch-hakear) defines a distance measure to quantify these distributional differences. A higher value of the distance measure for a cyclic granularity or harmony implies that they could be interesting for further investigation, whereas a low value indicates that nothing noteworthy is unfolding. A threshold and, consequently, a selection criterion are chosen using a permutation test such that cyclic granularities with significant values of the distance measure are selected. In addition, the distance metric has been appropriately adjusted, allowing it to be compared not only across cyclic granularities with different numbers of categories but also across a set of time series. As a result, it can also be used to rank the displays according to their ability to capture the greatest amount of variation across one or multiple time series.


The ideas in Chapters \@ref(ch-gravitas) and \@ref(ch-hakear) can be used for studying patterns in individual time series or comparing a few time series together. This is extended in Chapter \@ref(ch-gracsr) to allow for the exploration of distributions for multiple time series at the same time using unsupervised clustering. In the time series clustering literature, probability distributions across cyclic granularities have not been considered in determining similarity. However, such a similarity measure can be useful for characterizing the inherent temporal data structure of long, unequal-length time series in a way that is resistant to missing or noisy data while allowing for the detection of similar repeated patterns. Chapter \@ref(ch-gracsr) proposes two approaches for calculating distances between time series based on probability distributions across cyclic granularities. The first approach considers two time series to be similar if the distributions of each category of one or more cyclic granularities are similar. The second approach considers two time series to be similar if they have a similar significance of patterns across different granularities. A similar significance does not imply a similar pattern, which is where this technique varies from the former. When the distances from these approaches are fed into a hierarchical clustering algorithm, they yield small groups of time series with similar distributions or significance over multiple granularities. Our method is capable of producing useful clusters for both approaches, as demonstrated by testing on a range of validation data designs and a sample of residential smart meter consumers.



<!-- is more comprehensive in recognizing clusters with recurring patterns over many important granularities and -->

```{r software-initial, echo = FALSE, cache = FALSE, include = FALSE}
read_chunk('scripts/software.R')
```

## Software development

```{r software-impact}
```

This thesis focuses on translating research approaches into open source R packages for reproducibility and ease of use in other applications. So a significant amount of work has been devoted to the development of R packages  `gravitas`, `hakear`, and `gracsr`, each of which corresponds to a chapter presented in this thesis.

<!-- Figure \@ref(fig:software-ghcommits) gives an overview of my Git commits to these repositories, and Figure \@ref(fig:software-downloads) shows the daily downloads of the packages from the RStudio mirror (one of 90 CRAN mirrors) since they were available on CRAN. -->

<!-- ```{r software-ghcommits, fig.cap = "(ref:software-ghcommits-cap)"} -->
<!-- include_graphics("img/pkg-commits.png") -->
<!-- ``` -->

<!-- (ref:software-ghcommits-cap) Patterns of my package development effort during my PhD years based on Git commits to three repositories, sugrrants, tsibble, and mists. Scatter plots of weekly totals are overlaid with a loess smoother. The **sugrrants** package was the first project with much initial energy, followed by small constant maintenance. The **tsibble** package has been a major project with ongoing constant development and bursts of effort in response to users' feedback. The **mists** package has been a recent intense project. -->

<!-- # ```{r software-downloads, fig.height = 3, fig.cap = "(ref:software-downloads-cap)"} -->
<!-- # ``` -->
<!-- #  -->
<!-- # (ref:software-downloads-cap) Impact of these works (sugrrants and tsibble) as measured by daily downloads (on square root scale) from the RStudio mirror since they landed on CRAN. The **tsibble** package has an increasing trend, suggesting the steady adoption of the new data structure. -->

### gravitas

The `gravitas` package provides very general tools to compute and manipulate cyclic granularities and generate plots displaying distributions conditional on those granularities. The functions `search_gran()`, `create_gran()`, `harmony()`, `gran_advice()` and `prob_plot()` provides the entire workflow for an analyst to systematically explore large quantities of temporal data across different harmonies (pairs of granularities that can be analyzed together). This package was developed as part of my internship at Google Summer of Code, 2019. It has been on CRAN since January 2020. The website (<https://sayani07.github.io/gravitas>) includes full documentation and two vignettes about the package usage. There has been a total of 12K downloads from the RStudio mirror dating from 2020-11-01 to 2021-11-01. This package supplements the paper corresponding to Chapter \@ref(ch-gravitas), which has won the ACEMS Business Analytics Award 2021. The package can be generalized to non-temporal applications for which a hierarchical structure can be construed similar to time. 

### hakear 

The R package `hakear` (https://github.com/Sayani07/hakear) provides tools for selecting and sorting significant cyclic granularities. The function `wpd()` computes the weighted pairwise distances ($wpd$) for each cyclic granularity or pair of granularities, and the function `select_harmonies()` chooses those with significant patterns and ranks them from highest to lowest $wpd$. This package is reliant on parallel processing using multiple multi-core computers for faster computation of $wpd$. The selected harmonies can be plotted using package `gravitas` for potentially interesting displays. Currently, `hakear` implements ideas presented in Chapter \@ref(ch-hakear), but it will be integrated with `gravitas` in the future to explore distributions of a smaller number of time series.

### gracsr

The R package `gracsr` (https://github.com/Sayani07/gracsr) has functions for exploring a large number of time series using the clustering methodology described in Chapter \@ref(ch-gracsr). The workflow begins with the function `scale gran()`, which may be used to scale individual series using NQT/RS. The distances for the JS and WPD approaches are computed in the second phase of the workflow using `dist gran()`/`dist wpd()`. The distances can then be used to do clustering with `clust gran()`. The package has received a grant (AUD 3000) as part of the ACEMS Business Analytics Prize towards polishing the functions and preparing it for CRAN.


### Computational resources
Simulation studies were carried out to study the behavior of $wpd$, build the normalization method as well as compare and evaluate different normalization approaches in \@ref(ch-hakear). In \@ref(ch-gracsr), our methods were tested on several data designs with different parameters to evaluate their performance. $wpd$ is computationally heavy for cyclic granularities with smaller levels. Moreover, JS and WPD approaches are also computationally intensive when run on large number of customers. Hence, most of the scripts used to run these studies use parallel processing for better computational speed. R version 4.0.1 (2020-06-06) is utilized on the platform x86 64-apple-darwin17.0 (64-bit) operating on macOS Mojave 10.14.6, as well as the High Performance Computing (HPC) resources provided by [MonARCH](https://docs.monarch.erc.monash.edu/MonARCH/aboutMonArch.html).

<!-- The package contains functions for implementing the complete clustering methodology with choices of scaling and distance metrics discussed in the chapter. It -->


## Future ideas

<!-- ### Putting all functionalities on CRAN -->

<!-- I want to combine `hakear` and `gravitas` into a single R package for systematically exploring distributions of a smaller number of time series, whilst `gracsr` would offer the clustering framework for systematically exploring a large number of time series. I intend to run it via https://ropensci.org/software-review/ to further refine them for more exposure and efficiency. -->

<!-- ### Computing aperiodic cyclic granularities -->

<!-- Aperiodic cyclic granularities capture repetitions of aperiodic linear granularities. All of these are recurring events, but with non-periodic patterns which can still be visualized to understand repetitive behavior. The computation of cyclic aperiodic granularities requires first computing aperiodic linear granularities. Aperiodic events may be created using R packages such as `almanac`(Vaughan, 2020) and `gs`(Laird-Smith, 2020). The functions in `gravitas` package need to integrate with them to support computation of aperiodic cyclic granularities. -->

**Non-stationary time series**

Because the time series are observed over a short period of time (1-3 years), they are assumed to be stationary. But it is possible that the distributions change over time, even over a short period. For the smart meter example, the distribution for a customer moving to a different house or changing electrical equipment can change drastically. To detect these dynamic changes, non-stationarity in time series has to be incorporated while visualizing distributions and also computing distances for two non-stationary time series or one stationary and another non-stationary time series.

**Structured missingness**

It is possible that data for a whole category of cyclic granularity is unavailable or that there are insufficient observations to compute distributions. For example, a customer may not have data for a particular day of the week or month throughout their observation period. While visualizing probability distributions across categories in Chapters \@ref(ch-gravitas) and \@ref(ch-hakear), this can be indicated by displaying dot plots instead of summarizing distributions. But the distances in Chapter \@ref(ch-gracsr) can not handle missing observations if they are structured like this. More research is needed to design a distance that can incorporate customers with structural missingness and also comprehend its implications while visually characterizing them.

**Anomalies**

Characterizing clusters with varied or outlying customers can result in patterns that do not represent the group. Moreover, integrating heterogeneous consumers may result in visually identical end clusters, which are potentially not useful. An appropriate anomaly detection approach could be applied to filter the anomalies, and then a decision has to be made whether the anomalies should form a group or be assigned to the closest group through a classification method.

**Develop inferential methods for the clusters**

It is important to determine whether or not any detected clusters are statistically significant. In Chapter \@ref(ch-gracsr), we have used a permutation approach to detect if patterns are authentic or not across one or a pair of granularities. A similar approach could be used for finding the significance of clusters, but permutations used for comparisons must adjust for the fact that different temporal granularities have distinct conditional distributions.

**Scaling up for larger uncertainty and computational efficiency**

Larger data sets with more uncertainty may complicate matters, with potentially more problems than already stated above. More research is needed to understand what other sorts of problems may arise with the current methodology and possible ways to address them. All of this must be done while keeping computational efficiency in mind, as this is critical when scaling up for the analysis of large data sets.

**Scaling up the methods for multivariate time series data**

Another possible extension would be to create a similar framework for visualizing and analyzing multivariate time series data. With multiple time series available for each observation, the complexity of efficient exploration and visualization grows exponentially. In this case, conditional distributions include not only temporal dependency but also variables and their dependencies. This adds to the already high-dimensional data structures that result from studying distributions. This big problem can be tackled by first incorporating time’s inherent characteristics while visualizing one or a few multivariate time series data. Unsupervised clustering can then be used to group multiple time series across multiple time granularities and variables. This is a method similar to the one used in this thesis for dealing with univariate time series.


<!-- ### Comparing our clustering method with other benchmark methods -->

<!-- Testing needs to be carried out with non-hierarchy based clustering methods to see whether the proposed distances perform better with other algorithms. It would also be helpful to know how much more value we obtain from considering distributions rather than aggregating time series data. In the case of smart meters, this could be accomplished by comparing the results while validating them against external data such as weather conditions, socioeconomic or other demographic factors of those households. -->


<!-- ### Extend methods to other fields -->

<!-- We propose solutions that are realistically applicable to any temporal data collected more than once per year and may be implemented in a variety of settings. We could simply verify its utility in the context of smart meter electricity data. With the availability of several open-source benchmark data sets, it is vital to evaluate how well the methodologies perform across a variety of fields. -->



