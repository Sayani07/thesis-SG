# Appendix {-#ch:appendix}


\appendix


```{r external-hakear-supplementary, include = FALSE, cache = FALSE}
read_chunk("scripts/hakear.R")
read_chunk("scripts/hakear-supplementary.R")
knitr::read_chunk('scripts/gracsr.R')
knitr::read_chunk('scripts/gracsr-supplementary.R')
```

```{r load}
```

# Supplementary: Chapter 3


This is the supplementary paper corresponding to the paper "Detecting distributional differences between temporal granularities for exploratory time series analysis" 

## Recalling notations

Let $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ be a continuous measured variable observed across $T$ time points. The number of cyclic granularities considered in the display (at once) is $m$. Consider cyclic granularities $A$ and $B$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. For $m=1$, only one of $A$ or $B$ is plotted at once and a _panel_ refers to a display of distributions of $v$ across $J$ or $K$ levels on the x-axis. For $m=2$, the distribution display of $v$ with $A$ placed across x-axis and $B$ across facets is referred to as a $(J, K)$ _panel_ ($J$ x-axis levels and $K$ facet levels). The pairwise distances between pairs ($a_jb_k,a_j'b_k'$) could be within-facets or between-facets as seen in Figure 4 of the main paper. The tuning parameter, used to put relative weight-age to the pairwise distances within and between facets is denoted by $\lambda$. Let the four elementary designs be $D_{null}$ where there is no pairwise difference in distribution of $v$ across $A$ or $B$, $D_{var_f}$ denotes the set of designs where there is difference in distribution of $v$ for $B$ and not for $A$. Similarly, $D_{var_x}$ denotes the set of designs where difference is observed only across $A$. Finally, $D_{var_{all}}$ denotes those designs for which difference is observed across both $A$ and $B$. The following method is deployed for generating different distributions across different combinations for non-null designs - suppose the distribution of the combination of first levels of x and facet category is $N(\mu,\sigma)$ and $\mu_{jk}$ denotes the mean of the combination $(a_jb_k)$, then $\mu_{j.} = \mu + j\omega$ (for design $D_{var_x}$) and $\mu_{.k} = \mu + k\omega$ (for design $D_{var_f}$), where $\omega$ denotes the increment in mean. Table \ref{tab:explain-design} shows an example of how designs are defined for a $(2, 3)$ panel using $\omega = 3$. $nx$ and $nfacet$ denotes the number of categories placed on x-axis and facets respectively. $wpd_{raw}$ and $wpd$ denote the raw and normalized weighted pairwise distances.


```{r explain-design, fig.pos="ht", eval=FALSE}
sim_varx_normal = function(nx, nfacet, mean, sd, w)
{
  rep(dist_normal((mean + seq(0, nx-1, by  = 1)*w), sd), nfacet)
}

sim_varf_normal = function(nx, nfacet, mean, sd, w)
{
  rep(dist_normal((mean + seq(0, nfacet-1, by  = 1)*w), sd), each = nx)
}
  sim_varall_normal = function(nx, nfacet, mean, sd, w)
  {
    dist_normal((mean + seq(0, (nx*nfacet - 1), by  = 1)*w), sd)
  }
  
mean = 0
sd = 1
w = 3
nx = 2
nfacet = 3


raw_levels = expand.grid(facet = c("$b_1$", "$b_2$", "$b_3$"),
                         x = c("$a_1$", "$a_2$"))

facet = rep(c("$b_1$", "$b_2$", "$b_3$"), each = nx)
x =  rep(c("$a_1$", "$a_2$"), nfacet)

null <-  sim_varall_normal(nx, nfacet, mean, sd, w=0) %>% tibble()
vary_f <-  sim_varf_normal(nx, nfacet, mean, sd, w) %>% tibble()
vary_x <-  sim_varx_normal(nx, nfacet, mean, sd, w) %>% tibble()
vary_all <-  sim_varall_normal(nx, nfacet, mean, sd, w) %>% tibble()


raw_table <- bind_cols(x = x, 
                       facet = facet, 
                       null = null,
                       vary_f = vary_f, 
                       vary_x = vary_x, 
                       vary_all = vary_all) %>% 
  tibble() 
names(raw_table) = c("x levels", "facet levels", "$D_{null}$",  "$D_{var_f}$", "$D_{var_x}$", "$D_{var_{all}}$" )
raw_table %>% 
  kable(format = "latex",
    booktabs = TRUE, 
    escape = FALSE,
    caption = "Simulation setup for a panel with 3 facet levels and 2 x-axis levels for different designs starting from an initial distribution N(0, 1) for the combination $(a_1, b_1)$ and $\\omega=3$.") %>% kable_styling()
```

## Raw weighted pairwise distance

### Tuning parameter

For $m=1$, pairwise distances could be defined only between different categories of the cyclic granularity considered. So no tuning parameter is applicable for this case. For $m=2$, $\lambda$ might impact $wpd_{raw}$ differently depending on the value of $\omega$ and different values of $nx$, $nfacet$ and designs. The following simulation study sees the impact of $\lambda$ for different $\omega$, $nx$, $nfacet$ and designs.    

_Simulation design_

Observations are generated from N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7\}$ and $wpd_{raw}$ is computed for $\lambda = {0.1, 0.2, \dots, 0.9}$ under two designs $D_{var_x}$ and $D_{var_f}$ for $\omega = \{1, 8\}$  to observe how the value of $wpd_{raw}$ changes for different designs. Moreover, to observe for which value of $\lambda$ the two designs intersect, we generate observations from N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7, 14, 20\}$ and $\omega = \{1, 2, \dots, 10\}$ under the same two designs.

<!-- and the values of $wpd$ is computed for all these different values of the variables $nx$, $nfacet$, $\lambda$ and $\omega$ under two different designs $D_{var_x}$ and $D_{var_f}$ .  -->

_Results_

Figure \ref{fig:tuning-oneomega} shows how the value of $wpd$ changes for $\lambda = {0.1, 0.2, \dots, 0.9}$ for the two different designs $D_{var_x}$ and $D_{var_f}$ for two values of increment in mean $\omega = {1, 8}$. For a lower value of $\omega$, the two designs intersect at $\lambda > 0.7$ and for a higher $\omega$, the two designs intersect at $\lambda =  0.5$. The value of $wpd$ increases with $\lambda$ for $D_{var_x}$ and decreases with increasing $\lambda$ for $D_{var_f}$. \noindent Figure \ref{fig:tuning-parameter-inter} shows the value of $\lambda$ for which the two designs intersect for different values of $\omega$. It can be observed that as the value of $\omega$ ($\omega > 4$) increase, the value of $\lambda$ at which the two designs intersect converge is $\lambda = 0.5$ and for $\omega$ ($\omega <= 4$), designs interact between $\lambda = [0.6,0.75]$. Hence, $\lambda = 0.67$ is used for the rest of the paper, which is an average of the range of values $\lambda$ can take in order to have equal or more weightage for the within-facet distances.


```{r tuning-oneomega, fig.cap = " $wpd_{raw}$ from two designs are plotted for different values of $\\lambda$ $nx$ and $nfacet$. For $\\omega = 1$, the designs intersect for $0.6<\\lambda<=0.75$, whereas for higher omega, design intersects at $\\lambda = 0.5$."}
knitr::include_graphics(here::here("data/hakear/simulations/tuning_param/figs/fixed_omega.png"))
```



```{r tuning-parameter-inter, fig.cap ="The point of intersection of $wpd_{raw}$ values under the designs $D_{var_f}$ and $D_{var_x}$ are plotted across different $\\omega$ and $\\lambda$. For most panels it is observed that a common value of $\\lambda$ for which the designs interact is $0.5$ for $\\omega >= 4$, which implies any value greater than $0.5$ could be chosen to up-weigh the within-facet distances and down-weigh the between-facet distances. The value of $\\lambda$ is higher for  $\\omega<4$ could be anywhere between 0.6 and 0.75."}
knitr::include_graphics(here::here("data/hakear/simulations/tuning_param/figs/intersection_plot.png"))
```

### Underlying distributions   

The following simulation study sees the impact of different underlying distributions on $wpd_{raw}$ for different $nx$ and $nfacet$ before and after performing Normal Quantile Transformation.   

_Simulation design_

Observations are generated from N(0,1), N(5, 1), N(0, 5), Gamma(0.5, 1) and Gamma(2, 1) distributions for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}$ and $wpd_{raw}$ is computed with $\lambda = 0.67$ with (scenario 1) and without (scenario 2) NQT.

_Results_

Figure \ref{fig:gamma-diff-wotrans} shows ridge plots of $wpd_{raw}$ for a Gamma(0.5,1), Gamma(2,1) before NQT. It is observed that for the underlying distribution Gamma(2,1), location and scale of the distribution of wpd changes from top-left panel to bottom-right panel. Figure \ref{fig:gamma-diff-quantrans} shows the the distributions of $wpd$ under same underlying distributions after performing NQT. It is observed that within each panel, the distributions of the $wpd$ looks same, however, the distributions change from extreme top-left panel to bottom-right panels. Similar observations could be made in Figure \ref{fig:normal-diff-quantrans} for different underlying normal distributions  N(0,1), N(5,1) and N(0,5). This implies, NQT has atleast been able to make the location and scale of the distribution of $wpd_{raw}$ same for different underlying distributions.


<!-- Since the measure $wpd$ is essentially set up to detect "differences" in distributions irrespective of underlying distribution, it would be ideal if it has minimal dependency on the type, location and scale of the initial distribution. To that end, some data pre-processing through the Normal Score Transform (NQT) has been applied in order to make most asymmetrical distributed measured variables more normal-like. -->

```{r gamma-diff-wotrans, fig.cap="Ridge plots of raw wpd is shown for Gamma(0.5,1), Gamma(2,1) distribution without NQT. The densities change across different facet and x levels and also looks different for the two distributions, which implies wpd value is affected by the change in the shape paramter of the gamma distribution."}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_scale/figs/gamma_ridge_nxbynfacet.png"))
```

```{r gamma-diff-quantrans, fig.cap="Ridge plots of raw wpd is shown for Gamma(0.5,1), Gamma(2,1) distribution. The densities change across different facet and x levels but look same for the two distributions, which implies wpd value is unaffected by the change in the shape paramter of the gamma distribution"}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_quantrans/figs/diff_mean3_gamma.png"))
```

```{r normal-diff-quantrans, fig.cap = "Ridge plots of raw wpd is shown for N(0,1), N(5,1) and N(0,5) distribution. The densities change across different facet and x levels but look same for each panel, which implies wpd value is unaffected by the change in mean and standard deviation of the normal distribution."}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_quantrans/figs/diff_mean3_normal.png"))
```

### Number of comparisons 

#### Case: m = 1

_Simulation design_

\noindent Observations are generated from a N(0,1) distribution for each $nx = \{2, 3, 5, 7, 9, 14, 17, 20, 24, 31, 42, 50\}$ to cover a wide range of levels from very low to moderately high. $ntimes = 500$ observations are drawn for each combination of the categories, that is, for a panel with $nx=3$, $500$ observations are simulated for each of the categories. This design corresponds to $D_{null}$ as each combination of categories in a panel are drawn from the same distribution. Furthermore, the data is simulated for each of the categories $nsim=200$ times, so that the distribution of $wpd$ under $D_{null}$ could be observed. The values of $wpd$ is obtained for each of the panels. $wpd_{l, s}$ denotes the value of $wpd$ obtained for the $l^{th}$ panel and $s^{th}$ simulation.


_Results_

Figure \ref{fig:nxbyfacet-ridge-N01-onegran} shows ridge plots of $wpd_{raw}$ for an underlying N(0,1) distribution. For each panel, it could be seen that the location shifts to the right for increasing x levels. Across each panel, the scale of the distribution seems to change for low/moderately from lower values  to higher values of $nx$ and left tails are longer for lower facet levels.

```{r nxbyfacet-ridge-N01-onegran, fig.cap="Ridge plots of raw wpd is shown for N(0,1) distribution. For each panel, it could be seen that the location shifts to the right for increasing x levels. Across each panel, the scale of the distribution seems to change for low/moderately lower values and higher values of nfacets and left tails are longer for lower facet levels."}
knitr::include_graphics(here::here("data/hakear/simulations/supplementary/one-gran/raw/null_design_quantrans/figs/nxbyfacet_ridge_wpd_N01.png"))
```

```{r nfacetbynx-onegran, fig.cap="Ridge plots of raw wpd is shown for N(0,1) distribution. For each panel, it could be seen that the peakedness shifts for increasing x levels."}
knitr::include_graphics(here::here("data/hakear/simulations/supplementary/one-gran/raw/null_design_quantrans/figs/nfacetbynx_ridge_wpd_N01.png"))
```

```{r density-raw-onegran, eval = FALSE}
knitr::include_graphics(here::here("data/hakear/simulations/supplementary/one-gran/raw/null_design_quantrans/figs/nxbyfacet_density_wpd_N01.png"))
```

#### Case: m = 2   

_Simulation design_

\noindent Similarly, observations are generated from a N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}$. That is, data is being generated for each of the panels $(2, 2), (2, 3), (2, 5) \dots, (50, 31), (50, 50)$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. That is, if we consider a $(2, 2)$ panel, $500$ observations are generated for each of the possible subsets, namely, $\{(1, 1), (1, 2), (2, 1), (2, 2)\}$. 

<!-- Observations are generated from Gamma(2,1), G(0.5, 1), N(0,1), N(0, 5) and N(5, 1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}$ to cover a wide range of levels from very low to moderately high. Each combination is being referred to as a _panel_. That is, data is being generated for each of the panels $\{nx = 2, nfacet = 2\}, \{nx = 2, nfacet = 3\}, \{nx = 2, nfacet = 5\},  \dots, \{nx = 50, nfacet = 31\}, \{nx = 50, nfacet = 50\}$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. That is, if we consider the panel $\{nx = 2, nfacet = 2\}$, $500$ observations are generated for each of the combination of categories from the panel, namely, $\{(1, 1), (1, 2), (2, 1), (2, 2)\}$. The values -->
<!-- of $wpd$ is obtained for each of the panels. The measurement variable for each combination of categories in a panel are drawn from the same distribution and hence the design corresponds to $D_{null}$ . Furthermore, this entire method is repeated for each panels $nsim=200$ times, so that the distribution of $wpd$ under $D_{null}$ could be observed.    -->

_Results_

Figures \ref{fig:nxbyfacet-ridge-N01}, \ref{fig:nfacetbynx} and  \ref{fig:density-raw} shows the ridge plot of $wpd_{raw}$ with $nx$ as facets, $nfacet$ as facets and the density plot of $wpd_{raw}$ with $nx$ on the x-axis and $nfacet$ on the facets. The right-ward shift in location can be clearly observed from all figures.

```{r nxbyfacet-ridge-N01, fig.cap="Ridge plots of raw wpd is shown for N(0,1) distribution with nfacets as facets. For each panel, it could be seen that the location shifts to the right for increasing nx levels."}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_quantrans/figs/nxbyfacet_ridge_wpd_N01.png"))
```

```{r nfacetbynx, fig.cap="Ridge plots of raw wpd is shown for N(0,1) distribution with nx as facets. For each panel, it could be seen that the location shifts to the right for increasing nfacet levels."}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_quantrans/figs/nfacetbynx_ridge_wpd_N01.png"))
```

```{r density-raw, fig.cap="Density plot of raw wpd is shown for N(0,1) distribution with nx on x-axis and nfacet on facets. For each panel, it could be seen that the location as well as scale shifts as we move from top-left to bottom-right corner."}
knitr::include_graphics(here::here("data/hakear/simulations/raw/null_design_quantrans/figs/nxbyfacet_density_wpd_N01.png"))
```

## Adjusted weighted pairwise distances

_Simulation design_  

This section shows results from the different approaches of adjusting for the number of comparisons. Simulation design is the same as for raw weighted pairwise distances for $m=1$ and $m=2$. In place of $wpd_{raw}$, $wpd_{perm}$, $wpd_{glm}$ and $wpd$ is computed for all panels. 

<!-- \noindent Observations are generated from a N(0,1) distribution for each $nx = \{2, 3, 5, 7, 9, 14, 17, 20, 24, 31, 42, 50\}$ to cover a wide range of levels from very low to moderately high. $ntimes = 500$ observations are drawn for each combination of the categories, that is, for a panel with $nx=3$, $500$ observations are simulated for each of the categories. This design corresponds to $D_{null}$ as each combination of categories in a panel are drawn from the same distribution. Furthermore, the data is simulated for each of the categories $nsim=200$ times, so that the distribution of $wpd$ under $D_{null}$ could be observed. The values of $wpd$ is obtained for each of the panels. $wpd_{l, s}$ denotes the value of $wpd$ obtained for the $l^{th}$ panel and $s^{th}$ simulation. -->

### Permutation approach

Figure \ref{fig:one-gran-nxnyfacet} and \ref{fig:norm} show the normalized ${wpd}$ for $m=1$ and $m=2$ respectively. The location and scale of the distribution of ${wpd}$ is similar for increasing values of $nx$ or $nfacet$. Due to heavy computational load, $wpd_{perm}$ for few panels with very high value of $nx$ or $nfacet$ are not computed.


```{r one-gran-nxnyfacet, fig.cap="Distribution of $wpd_{perm}$ are shown for different levels for $m=1$ under the null design. The distribution for different panels have similar location and scale."}
include_graphics(here("data/hakear/simulations/supplementary/one-gran/norm/null_design_quantrans_nperm/figs/nxbyfacet_ridge_wpd_N01.png"))
```


```{r norm, fig.cap = "Distribution of $wpd_{perm}$ are shown for different levels for $m=2$ under the null design. The distribution for different panels have similar location and scale."}
G21_norm <- read_rds(here("data/hakear/simulations/norm/null_design_quantrans_nperm/data-agg/all_data_wpd_N01.rds"))

G21_norm %>% 
  ggplot(aes(x = value)) + 
  geom_density(fill = "blue") +
  facet_grid(nx~nfacet,
             labeller = "label_both") + 
  scale_x_continuous(breaks = scales::breaks_extended(3)) + 
  xlab("wpd normalised using permutation approach") +    xlab(TeX("wpd_{perm}")) + theme_bw()

```

### Modeling approach

For for $m=1$, Figure \ref{fig:glm-horizontal-onegran1} shows the scatterplot of $wpd_{raw}$ against different values of $nx$ and also the display of residuals from the model and Figure \ref{fig:glm-density-onegran} shows the distribution of $wpd_{glm}$ for different $nx$. For $m=2$, Figure \ref{fig:glm-horizontal-twogran1} shows the scatter plot of $wpd_{raw}$ against different values of $nx$ and $nfacet$ and also the display of residuals from the model. Figure \ref{fig:wpd-glm-dist-appendix} shows the distribution of $wpd_{glm}$ for different $nx$ and $nfacet$. Tables \ref{tab:tab-modelsm1} and \ref{tab:tab-models} show the GLM results for $m=1$ and $m=2$ respectively. All of these jointly show that the residuals from the model are independent of the number of comparisons. The location and scale of the distribution of $wpd_{glm}$ is similar for higher levels and distinctly different for lower levels.

```{r tab-modelsm1}
library(here)
G21_onegran <- read_rds(here("data/hakear/simulations/supplementary/one-gran/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))

G21_median_onegran  <- G21_onegran  %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))


glm_fit_onegran  <- glm(actual ~ log(`nx * nfacet`),
               family = Gamma(link = "inverse"),
               data = G21_median_onegran)

bind_cols(distribution = "N(0, 1)", broom::tidy(glm_fit_onegran)) %>% 
  mutate(estimate = round(estimate, 2),
         std.error = round(std.error, 3),
         statistic = round(statistic, 2),
         p.value = round(p.value, 2)) %>% kable(caption = "Results of generalised linear model to capture the relationship between $wpd_{raw}$ and number of comparisons for $m=1$.")

```



```{r glm-horizontal-onegran1-raw}

G21 <- read_rds(here("data/hakear/simulations/supplementary/one-gran/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))

G21_median <- G21 %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))


glm_fit <- glm(actual ~ log(`nx * nfacet`),
               family = Gamma(link = "inverse"),
               data = G21_median)

intercept <- glm_fit$coefficients[1]
slope <- glm_fit$coefficients[2]
G21_sd  = G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
  )
  )
  ))
#scale_fac <- 1/G21_sd$wpd_glm %>% sd()


fitted_glm <- fitted(glm_fit, type = "response")
residuals <- residuals.glm(glm_fit, type = "response")

residual <- G21_median$actual  - fitted_glm

G21_horizontal <- G21 %>% 
  ggplot(aes(x=log(nx*nfacet),
             y = (value - (1/(intercept + slope*log(nx*nfacet)
             )
             )
             )
  )
  ) +
  geom_point() + stat_summary(fun=mean, geom="line", aes(group=1), color = "blue", size = 1) + 
  ylab(TeX("wpd_{glm} = wpd_{raw} - 1/(a  + b*log(nx*nfacet))")) + xlab("log(nx)")

#ggsave("figs/G21_horizontal.png")
```

```{r glm-horizontal-onegran1, fig.cap = "$wpd_{raw}$ (points) and residuals (blue line) from the model is plotted across different number of comparisons for $m=1$. Residuals seem to be independent of $nx$ and have been defined as $wpd_{glm}$."}
knitr::include_graphics(here("img/G21_horizontal.png"))
```


```{r glm-density-onegran, fig.cap = "Distribution of $wpd_{glm}$ is plotted across different $nx$ for $m=1$. Both shape and location of the distributions are different for lower $nx$ than for higher $nx$."}
G21_glm_m1 <- G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
  )
  )
  ),
  wpd_glm_scaled = ((wpd_glm*261)))

mylabels <- G21_glm_m1 %>% 
  group_by(nx) %>% 
  summarise(mean = round(mean(wpd_glm), 3), sd = round(sd(wpd_glm), 3)) %>% 
  mutate(label_glm = paste0("(", mean, ", " ,sd, ")"))

nxbyfacet_ridge <- G21_glm_m1 %>% 
  ggplot(aes(x = wpd_glm, y = as.factor(nx))) +
  ggridges::geom_density_ridges() +
  xlab(TeX("wpd_{glm}")) +
  ylab("nx") 
  # geom_label(data = mylabels,
  #            aes(y = as.factor(nx),
  #                x = label_glm, 
  #                label = label_glm))


nxbyfacet_ridge
```



```{r tab-models}
G21 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_Gamma21.rds"))
G01 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_Gamma01.rds"))
N01 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))
N05 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N05.rds"))
N51 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N51.rds"))
N55 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N55.rds"))


all_data <- bind_rows(G21 = G21, G01 = G01, N01= N01, N05 = N05, N51= N51, N55 = N55, .id = "group")


all_median <- all_data %>% 
  group_by(group, nx*nfacet) %>% 
    summarise(actual = median(value))


glm_model <- function(df) {
  glm(actual ~ log(`nx * nfacet`), 
               family = Gamma(link = "inverse"), data = df)
}
  
glm_fit <- all_median %>% 
  group_by(group) %>% 
  nest()

models <- map(glm_fit$data, glm_model)
  
  
by_dist <- glm_fit %>% 
  mutate(model = map(data, glm_model))
 
tidy_obs <- by_dist %>%
  mutate(tidy = map(model, tidy)) %>%
  unnest(cols = c(tidy)) %>% 
  mutate(estimate = round(estimate, 2),
         std.error = round(std.error, 3),
         statistic = round(statistic, 2))%>% 
  mutate(distribution = case_when(
    group== "G01" ~ "Gamma(0.5, 1)",
    group== "G21" ~ "Gamma(2, 1)",
    group== "N01" ~ "N(0, 1)",
    group== "N05" ~ "N(0, 5)",
    group== "N51" ~ "N(5, 1)",
    group== "N55" ~ "N(5, 5)",
    TRUE~ as.character(group)
    )) %>% ungroup() %>% 
  select(-data, -model, - group) %>% 
     select(distribution, everything())  

tidy_obs %>% kable(caption = "Results of generalised linear model to capture the relationship between $wpd_{raw}$ and number of comparisons for different underlying distribution.")
```

<!-- # ```{r} -->
<!-- #  -->
<!-- # G21_median <- N01 %>% -->
<!-- #   group_by(nx*nfacet) %>% -->
<!-- #   summarise(actual = median(value)) -->
<!-- #  -->
<!-- #  -->
<!-- # glm_fit <- glm(actual ~ log(`nx * nfacet`), -->
<!-- #                family = Gamma(link = "inverse"), -->
<!-- #                data = G21_median) -->
<!-- # # tidy(glm_fit) -->
<!-- #  -->
<!-- # ``` -->

```{r glm-horizontal-twogran, fig.cap = "Residuals from the model is plotted across different $nx$ and $nfacet$. Residuals seem to be independent of $log(nx*nfacet)$ and have been defined as $wpd_{glm}$ in the paper."}

G21 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))

G21_median <- G21 %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))

glm_fit <- glm(actual ~ log(`nx * nfacet`),
               family = Gamma(link = "inverse"),
               data = G21_median)

intercept <- glm_fit$coefficients[1]
slope <- glm_fit$coefficients[2]
G21_sd  = G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
  )
  )
  ))
#scale_fac <- 1/G21_sd$wpd_glm %>% sd()
# checking the fit of the residuals from glm fit
# fitted_glm <- fitted(glm_fit, type = "response")
# residuals <- residuals.glm(glm_fit, type = "response")
# hist(residuals)
#h = augment(glm_fit)
# ggplot(h) +
#   geom_histogram(aes(x = .resid))
# residual <- G21_median$actual  - fitted_glm
# 
# hist(residual)
G21_horizontal <- G21 %>%
  ggplot(aes(x=log(nx*nfacet),
             y = (value - (1/(intercept + slope*log(nx*nfacet)
             )
             )
             )
  )
  ) +
  geom_point() + 
  stat_summary(fun=mean,
               geom="line",
               aes(group=1),
               color = "blue",
               size = 1) +
  ylab(TeX("wpd_{glm} = wpd_{raw} - 1/(a  + b*log(nx*nfacet))"))

# broom::tidy(glm_fit) %>% kable(caption = "Results of generalised linear model to capture the relationship between $wpd_{raw}$ and number of comparisons.")

#ggsave("figs/G21_horizontal_m2.png")
```

```{r glm-horizontal-twogran1, fig.cap = "$wpd_{raw}$ (points) and residuals (blue line) from the model is plotted across different number of comparisons for $m=2$. Residuals seem to be independent of $nx$ and have been defined as $wpd_{glm}$."}
knitr::include_graphics(here("img/G21_horizontal_m2.png"))
```

```{r wpd-glm-dist-appendix, fig.cap = "Distribution of $wpd_{glm}$ is plotted across different $nx$ for $m=2$. Both shape and scale of the distributions are different for lower $nx$ than for higher $nx$."}
G21_glm <- G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
  )
  )
  ),
  wpd_glm_scaled = ((wpd_glm*320)))

#G21_glm$wpd_glm_scaled %>% sd()

G21_glm %>%
  ggplot() +
  geom_density(aes(x = wpd_glm),
               fill = "blue") +
  facet_grid(nx~nfacet,
             labeller = "label_both") +
  theme(legend.position = "bottom")

```

### Combination approach

Figure \ref{fig:dist-same-scale-onegran} and \ref{fig:dist-new-same-scale-link} show the distribution of $wpd_{perm}$ and $wpd_{glm-scaled}$ overlaid on each other for $m=1$ and $m=2$ respectively. $wpd$ takes the value of $wpd_{perm}$ for lower levels 
$(<=5)$, and $wpd_{glm-scaled}$ for higher levels so that we have similar location and scale across panels with different levels.


```{r dist-same-scale-onegran, fig.cap = "The distribution of $wpd_{perm}$ and $wpd_{glm-scaled}$ are overlaid to compare the location and scale across different $nx$ for $m=1$. The distribution of the adjusted measure looks similar for both approaches for higher levels, but for $nx=2$ both location and scale differs."}
G21_permutation <- read_rds(here("data/hakear/simulations/supplementary/one-gran/norm/null_design_quantrans_nperm/data-agg/all_data_wpd_N01.rds")) %>%
  rename("wpd_permutation" = "value")



G21_all_data <- G21_permutation %>% 
  # left_join(G21_lm, by = c("nx", "nfacet", "perm_id")) %>% 
  left_join(G21_glm_m1, by = c("nx", "nfacet", "perm_id")) %>% 
  pivot_longer(cols = c(3, 7),
               names_to = "type_estimate",
               values_to = "value_estimate")
G21_all_data$type_estimate = factor(G21_all_data$type_estimate , levels = c( "wpd_permutation", "wpd_glm_scaled"))


summary_data <- G21_all_data %>% 
  group_by(nx, nfacet, type_estimate) %>% 
  summarise(mean = mean(value_estimate)) %>% ungroup()


G21_all_data %>% 
  filter(type_estimate %in% c("wpd_glm_scaled", "wpd_permutation")) %>% 
  ggplot(aes(x = value_estimate)) +
  geom_density(aes(fill = type_estimate), alpha = 0.5, size = .5) +
  geom_vline(data = summary_data, aes(xintercept = mean, color = type_estimate)) + 
  geom_rug(aes(color = type_estimate), length = unit(0.09,"cm"), alpha = 0.5) +
  #coord_cartesian(clip = "off") + 
  facet_wrap(~nx,
             labeller = "label_both") +
  theme_bw() + 
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c( "#D55E00", "#0072B2")) +
  scale_color_manual(values = c( "#D55E00", "#0072B2")) +
  xlab("adjusted values of wpd") 
```

```{r dist-new-same-scale-link, fig.cap = "The distribution of $wpd_{perm}$ and $wpd_{glm-scaled}$ are overlaid to compare the location and scale across different $nx$ and $nfacet$ for $m=2$. $wpd$ takes the value of $wpd_{perm}$ for lower levels, and $wpd_{glm-scaled}$ for higher levels. "}

G21 <- read_rds(here("data/hakear/simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))
G21_median <- G21 %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))
glm_fit <- glm(actual ~ log(`nx * nfacet`),
               family = Gamma(link = "inverse"),
               data = G21_median)
intercept <- glm_fit$coefficients[1]
slope <- glm_fit$coefficients[2]

G21_glm <- G21_glm <- G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
  )
  )
  ),
  wpd_glm_scaled = ((wpd_glm*320)))


G21_permutation <- read_rds(here("data/hakear/simulations/norm/null_design_quantrans_nperm/data-agg/all_data_wpd_N01.rds")) %>%
  rename("wpd_permutation" = "value")


# G21_model_data <- G21 %>%
#   mutate(model =
#            ((1/value)
#                   - intercept -
#                     slope*log(nx*nfacet))/slope) %>%
#   mutate(model_trans =
#            (model - mean(model))/sd(model))

# G21_model_data$model %>% summary()

G21_all_data <- G21_permutation %>% 
  # left_join(G21_lm, by = c("nx", "nfacet", "perm_id")) %>% 
  left_join(G21_glm, by = c("nx", "nfacet", "perm_id")) %>% 
  pivot_longer(cols = c(3, 7),
               names_to = "type_estimate",
               values_to = "value_estimate")
G21_all_data$type_estimate = factor(G21_all_data$type_estimate , levels = c( "wpd_permutation", "wpd_glm_scaled"))


summary_data <- G21_all_data %>% 
  group_by(nx, nfacet, type_estimate) %>% 
  summarise(mean = mean(value_estimate)) %>% ungroup()


G21_all_data %>% 
  filter(type_estimate %in% c("wpd_glm_scaled", "wpd_permutation")) %>% 
  ggplot(aes(x = value_estimate)) +
  geom_density(aes(fill = type_estimate), alpha = 0.5, size = .5) +
  geom_vline(data = summary_data, aes(xintercept = mean, color = type_estimate)) + 
  geom_rug(aes(color = type_estimate), length = unit(0.09,"cm"), alpha = 0.5) +
  #coord_cartesian(clip = "off") + 
  facet_grid(nx~nfacet,
             labeller = "label_both") +
  theme_bw() + 
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c( "#D55E00", "#0072B2")) +
  scale_color_manual(values = c( "#D55E00", "#0072B2")) +
  xlab("adjusted values of wpd") +
  scale_x_continuous(breaks = c(-5, -3, 0, 3, 5)) 

```



## Ranking and selecting harmonies

\noindent _Simulation design_  
 
Observations are generated from a N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = \{3, 7, 14\}$ and $nfacet = \{2, 9, 10\}$. This would result in 9 panels, viz, $(3, 2), (3, 9), (3, 10), \dots, (14,9), (14, 10)$. Few experiments were conducted. In the first scenario, data for all panels are simulated using the null design $D_{null}$. In other scenarios, data simulated from the panel $(14, 2)$ and $(3, 10)$ are under $D_{vary_{all}}$. Moreover, $\omega = \{0.5, 2, 5\}$ are considered to examine if the proposed test is able to capture subtle differences and non-subtle differences when we shift from the null design. In the last scenario, we consider the panel $(3, 2), (7,9), (14, 10)$ to be under $D_{null}$, the panels $(7, 2), (14, 9)$ to be under $D_{var_f}$. $(14, 2), (3, 10)$ under $D_{var_x}$ and the rest under $D_{var_{null}}$. This is done to check if the consequent ranking procedure leads to designs like $D_{vary_{f}}$ rank below $D_{vary_{all}}$. We generate only one data set each for which these scenarios were simulated and consider this as the original data set. We generate $1000$ repetitions of this experiment with different seeds.

\noindent _Results_  

Figure \ref{fig:foo} shows the distribution of $wpd$ for the simulated $9$ null designs of different levels with each harmony being rejected if the corresponding $wpd$ value is higher than $wpd_{threshold99}$. Although the level of significance for each test is 1%, the level of significance for multiple tests together is 10% as displayed through the red line. This means that 10% of the times the null designs will be tagged as interesting even when there is actually nothing interesting going on.

```{r}
all_data <- read_rds("data/hakear/simulations/supplementary/test-hpc/data-agg/all_data.rds")


# computes size for 99 percentile threshold
all_data <- all_data %>%
  filter(!is.na(x_levels), !is.na(facet_levels))



data_summary <- all_data%>% 
  mutate(sig = if_else(significance!=99, 0, 99)) %>% 
  group_by(seed_id) %>% 
  summarize(sumsig = sum(sig)) %>% 
  count(sumsig==0) %>% 
  mutate(p_value = n/sum(n)) %>% 
  slice(1)

data_summary_lowlev <- all_data%>% 
  filter(x_levels==3, facet_levels ==2) %>% 
  mutate(sig = if_else(significance!=99, 0, 99)) %>% 
  group_by(seed_id) %>% 
  summarize(sumsig = sum(sig)) %>% 
  count(sumsig==0) %>% 
  mutate(p_value = n/sum(n)) %>% 
  slice(1)

# plots

global_threshold <-  tibble(threshold = "89.4", value = quantile(all_data$wpd, 0.894, na.rm = TRUE))
```

<!-- $wpd_{threshold99}$ is obtained as `r round(global_threshold$value, 2)` for this experiment, which leads to a p-value of `r round(data_summary$p_value, 3)`. Figure \ref{fig:global-size} shows the distribution of $wpd$ obtained from this experiment with the red line denoting $wpd_{threshold99}$. If we split the display of distribution between $nx$ and $nfacet$, then Figure \ref{fig:local-size} shows that the probability of rejecting the null when it is actually true is higher for smaller levels, however, it is still within limits (around `r round(data_summary_lowlev$p_value, 3)`). -->

(ref:foo) The distribution of $wpd$ for 9 null designs of different levels with each harmony being rejected if the corresponding $wpd$ value is higher than $wpd_{threshold99}$. Although the level of significance for each test is 1%, the level of significance for multiple tests together is 10% as displayed through the red line. This means 10% of the times the null designs will be tagged as interesting even when there is actually nothing interesting going on.

```{r foo, fig.cap='(ref:foo)'}

ggplot() + 
  geom_histogram(data = all_data, aes(x = wpd))  + 
  geom_vline(data = global_threshold, aes(xintercept =  value),colour = "red", size = 1.5) 
```


```{r local-size, fig.cap = "something", eval = FALSE}
ggplot() + 
  geom_histogram(data = all_data, aes(x = wpd))  + 
  geom_vline(data = global_threshold, aes(xintercept =  value),colour = "red", size = 1.5) +
  facet_grid(x_levels~facet_levels)
```

```{r hist-qq-new-appendix, fig.cap = "In panel a, the histogram of $wpd_{glm-scaled}$ is plotted. In part b, the QQ plot is shown with the theoretical quantiles on the x-axis and $wpd_{glm-scaled}$ quantiles on the y-axis. The distribution looks symmetric and looks like normal except in the tails.", eval = FALSE}

```  

```{r quadratic, fig.cap = "$wpd$ is plotted against $nx*nfacet$ (the maximum number of pairwise comparisons) and the blue line represents the median of the multiple values for each $nx*nfacet$. The median increases abruptly for lower values of $nx*nfacet$ and slowly for higher $nx*nfacet$. Thus, the measure will have higher values for higher levels in $nx$ or $nfacet$.", eval = FALSE}
```


# Supplementary: Chapter 4


## Raw data plot

```{r raw-data-50, out.width = "100%", fig.cap=" The raw half-hourly energy usage for $50$ sampled households is plotted along the y-axis versus time in a linear scale.  Each of these series is associated with a single customer. It looks like there is a lot of missing values and unequal length of time series along with asynchronous periods for which data is observed. No insightful behavioral pattern could be discerned from this view other than when the customer is not at home.", fig.pos="h"}

knitr::include_graphics("img/raw_plot_cust.png") # look at smart-meter.R for the code
```

## Missing data plot

(ref:missing-data) Investigating the temporal location of missing values for customers who have implicit missing values. There are $13,735$ customers in the data set, with 8,685 having no missing values and the remaining $5,050$ having at least one missing value. Each cross represents a missed observation in time, while the line connecting two dots represents continuous missingness over time. Missing values occur at random times and do not appear to follow a pattern, although there is a higher concentration of missing values in September and October 2012 for the majority of customers. This plot is inspired by @wang2020tsibble.

```{r missing-data, fig.cap="(ref:missing-data)", fig.pos="h"}
knitr::include_graphics("img/missing-data-5050.png")
```


## Prototype selection

S1. Robust scaling is applied to each customer.

S2. $50^{th}$ percentile for each category for each granularity is obtained for each customers. So we have a data structure with  $356$ rows and $(24 + 12 + 2)$ variables corresponding to $50^{th}$ percentile for each hour-of-day, month-of-year and weekend-weekday.

S3. Apply principal components and restrict the results down to the first six principal components (which makes up approximately 85% of the variance explained in the data) to use with the grand tour.

S4. Run t-SNE using the default arguments on the complete data (sets the perplexity to equal 30 and performs random initialization). We then create a linked tour with t-SNE layout with R package liminal.

S5. We inspect of the subspace generated by the set of low-dimensional projections in tour by looking for a simplex shape while the visualization moves from one basis to another. When we brush the corners of the simplex, we find they fall on the edge of the t-SNE point cloud.

<!-- Hall, Marron, and Neeman (2005) have shown that in the extreme case of high-dimension, low-sample size data, observations are on the vertices of a simplex. -->

<!-- This is because in high-dimensional data analysis the curse of dimensionality reasons that points tend to be far away from the center of the distribution and on the edge of high-dimensional space. Contrary to this, is that projected data tends to clump at the center. -->

S6. These points should ideally correspond to different behavior with respect to all the variables considered while running PCA. 


```{r mds, out.width="100%", fig.cap="One instance of brushing in tours (right) and projecting the points in a lower dimensional tsne cloud (left).", out.height="200px"}
knitr::include_graphics("img/tour-tsne.png")
```

```{r prototype-data-pick}
```


```{r assemble}
```


```{r data-pick}
```


```{r all-data}

```


```{r clustering}
```


```{r groups-24}

```


```{r tsne-fit}
```


```{r tsne-plot-supplementary, fig.cap="t-SNE summary of selected $24$ prototype customers. Each ellipse corresponds to a group after clustering using our methodology. Few customers are not enclosed inside an ellipse as they are from mixed groups and there are too few points to calculate an ellipse around them. It is important to note that the selection of prototypes is based on only $50$ percentiles, whereas, the clustering is based on all the deciles."}

```

<!-- From this set we select $4$ "anchor" customers which are far apart from each other and $5$ neighboring customers for each of these anchors. These selections were done using the granularity $hod$ space. It is important to note that when we use our proposed methodologies, it is based on all dimensions `hod`, `moy` and `wkndwday`.Fig \ref{fig:mds} shows the MDS of these 356 customers in a 2D space basis their distance on individual granularities and when all of them are combined. Our methodolgies are run on these $24$ customers, which act as a way to evaluate the proposed methodologies.  -->


<!-- . Instances may be distributed in the representation space in a reasonable manner, revealing their similarities, using this instance wise discriminative learning. A dual-level progressive similar instance selection (DPSIS) approach could also be used -->

## Interaction of granularities

Consider a case in which there are only two interacting granularities of interest, $g1$ and $g2$. In contrast to the previous situation, when we could study distributions across $n_{g_1} + n_{g_2} = 5$ separate categories, with interaction, we must evaluate the distribution of the $n_{g_1}*n_{g_2}=6$ combination of categories. Consider the $4$ designs in Figure \ref{fig:interaction-gran-supplement}, where various distributions are assumed for different combinations of categories, resulting in different designs. Design $D1$ exhibits no change in distributions across $g1$ or $g2$, whereas Designs $D2$ and $D3$ alter across only $g1$ and $g2$, respectively. D4 varies across both $g1$ and $g2$ categories. D3 and D4 appear similar based on their relative differences across consecutive categories, but $D4$ also changes across facets, unlike $D3$, which has all facets look the same.

```{r interaction-gran-supplement, fig.cap = "The distribution of simulated variable across $g1$ conditional on $g2$ is shown through boxplots for 4 designs to extend the proposed validation designs when two granularities of interest interact. D1 has no change in distributions across different categories of $g1$ or $g2$, while D2 and D3 change across only $g1$ and $g2$ respectively. D4 changes across categories of both $g1$ and $g2$."}

```



```{r sim-val}
set.seed(9999)
nx_val = 2 # number of x-axis levels
nfacet_val = 3 # number of facet levels
w1_val = 3 # increment in mean
w2_val = 0 # increment in sd
mean_val = 0 # mean of normal distribution of starting combination
sd_val = 1 # sd of normal distribution of starting combination
quantile_prob_val = seq(0.1, 0.9, 0.1)
```


```{r nobs50}
ntimes_val = 300 # nobs per combination 
```


<!-- # Clustering on simulated datasets -->

```{r sample-seed-few}
sample_seed <-  seq(10, 100, 10)
```

<!-- _DGP: Generate 10 time series from each designs_  -->
<!-- Time series are simulated from each of these designs with $50$ observations in each group. So we have $50$ observations each for the six combination of categories $(1,1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2)$. Time series are simulated for ten different seeds (from `sample_seed`) for each design. `data_varall-1` represents a data set from a design $D_{var_{all}}$ (distributions change across both facet and x) with a seed $10$. `data_varall-2` represents a data set from a design $D_{var_{all}}$ with a seed $20$ and so on. -->
<!-- `data_null`, `data_varx` and `data_varf `corresponds to designs $D_{null}$, $D_{var_{x}}$ and $D_{var_{f}}$ designs respectively.    -->


```{r change-index-data-few}
change_index_data <- function(ntimes_val = NULL,
                         nx_val = NULL,
                         nfacet_val = NULL,
                         sim_function = sim_varx_normal){

  data <- sim_panel(
    nx = nx_val, nfacet =  nfacet_val,
    ntimes = ntimes_val,
    # sim_dist = sim_varx_normal(2, 3, 5, 10, 5, -1.5)
    sim_dist = sim_function(nx_val, nfacet_val, mean_val, sd_val, w1_val, w2_val)
  ) %>% unnest(data)


  index_new <- map(seq_len(ntimes_val), function(i){
    map((seq_len(nx_val*nfacet_val)), function(j)
    {
      value = i + (j-1)*ntimes_val
    })
  }) %>% unlist()

  data_new = data %>%
    ungroup() %>%
    mutate(index_old = row_number(),
           index_new = index_new)

  y = data_new[match(index_new, data_new$index_old),]

  y <- y %>%
    mutate(time = row_number()) %>%
    select(-c(index_old, index_new))

  return(y)
}
```


```{r data-make-few}
data_null <- map(sample_seed, function(seed){
  set.seed(seed)
  change_index_data(5, 2, 3, sim_null_normal) %>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_null")


data_varf <- map(sample_seed, function(seed){
   set.seed(seed)
  change_index_data(5, 2, 3, sim_varf_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varf")

data_varx <- map(sample_seed, function(seed){
   set.seed(seed)
  change_index_data(5, 2, 3, sim_varx_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varx")

data_varall <- map(sample_seed, function(seed){
   set.seed(seed)
  change_index_data(5, 2, 3, sim_varall_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varall")

data_q <- bind_rows(data_null,
                      data_varf,
                      data_varx,
                      data_varall) %>%
  mutate(seed_id = sprintf("%02d", 
                              as.numeric(seed_id))) %>% 
  mutate(unique_data = paste(data_type, seed_id, sep = "-"))

data_q$data_type <- factor(data_q$data_type,
                                    levels = c("data_null" , "data_varf",
                                               "data_varx","data_varall"))

```



```{r JS}

```

```{r JS-mydata-few}
# data_q_wide <- data_q %>%
#   select(id_facet, id_x, unique_data, sim_data_quantile) %>%
#   pivot_wider(names_from = unique_data,
#               values_from = sim_data_quantile) %>%
#   select(-c(1, 2))
# 
# ndata <- data_q %>% distinct(unique_data) %>% mutate(index = row_number())
# ldata <- nrow(ndata)
# lcomb <-  nx_val*nfacet_val
# 
# dist_data <- map(1:ldata, function(x){ # first data
#   map(1:ldata, function(y){ # 2nd data
#     map(1:lcomb, function(z){ # number of combinations nx*nfacet
#       JS(
#         prob = quantile_prob_val,
#         unlist(data_q_wide[z,x]),
#         unlist(data_q_wide[z,y])
#       ) %>% as_tibble()
#     })%>% bind_rows(.id = "combinations")
#   })%>% bind_rows(.id = "data_type1")
# }) %>% bind_rows(.id = "data_type2") 
# 
# 
# 
# dist_mat <- dist_data %>%
#   group_by(data_type1, data_type2) %>%
#   summarise(dist = sum(value)) %>%
#   pivot_wider(names_from = data_type2,
#               values_from = dist) %>%
#   mutate(data_type1 = as.numeric(data_type1)) %>%
#   left_join(ndata, by = c("data_type1" = "index"))
# 
# 
# dist_mat_format <- dist_mat %>%
#   ungroup() %>%
#   select(-data_type1, -unique_data)
# 
# 
# rownames(dist_mat_format) <- dist_mat$unique_data
# write_rds(dist_mat_format, "data/gracsr/validation/interact-4D/distmat_40series.rds")
# write_rds(dist_mat, "data/gracsr/validation/interact-4D/distmat_40series_withnames.rds")
# 
dist_mat_format <- read_rds("data/gracsr/validation/interact-4D/distmat_40series.rds")
dist_mat <- read_rds("data/gracsr/validation/interact-4D/distmat_40series_withnames.rds")
```



```{r hier-clust-few, fig.cap= "A dendogram showing the branching of $40$ series with each of the $4$ designs repeated $10$ times."}
d = stats::as.dist(dist_mat_format)
par(cex=0.5)
hc = stats::hclust(d,method="complete")
plot(hc)
rect.hclust(hc, k = 4, border = "red")

```


```{r mds-plot-interact-few, fig.cap= "MDS summary plot of $40$ series with each of the $4$ designs repeated $10$ times. It can be observed that all the series belonging to same design lie closer in this plot and all the ones in different designs are placed further away."}
mds <- d %>%
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
#rownames(mds) <- dist_mat$unique_data

groups<-cutree(hc, k=4)

all_data_cluster <- cbind(dist_mat_format, groups) %>%
  cbind(mds)%>%
  mutate(groups = as.factor(groups)) %>% as_tibble()


mds_plot_10 <- ggplot(all_data_cluster,
       aes(x = Dim.1,
           y = Dim.2,
       color = groups)) +
  geom_text(label = rownames(dist_mat_format),check_overlap = TRUE)+
  geom_point(size = 1) +
  theme_classic()+
  theme(legend.position = "bottom") +
  scale_color_brewer(palette = "Dark2") +
  coord_fixed(ratio = 1)+
  theme(text = element_text(size = 6)) 

mds_plot_10
```



<!-- # Clustering on simulated datasets -->

```{r sample-seed}
sample_seed <-  seq(1, 100, 1)
```

<!-- _DGP: Generate 10 time series from each designs_  -->
<!-- Time series are simulated from each of these designs with $50$ observations in each group. So we have $50$ observations each for the six combination of categories $(1,1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2)$. Time series are simulated for ten different seeds (from `sample_seed`) for each design. `data_varall-1` represents a data set from a design $D_{var_{all}}$ (distributions change across both facet and x) with a seed $10$. `data_varall-2` represents a data set from a design $D_{var_{all}}$ with a seed $20$ and so on. -->
<!-- `data_null`, `data_varx` and `data_varf `corresponds to designs $D_{null}$, $D_{var_{x}}$ and $D_{var_{f}}$ designs respectively.    -->


```{r change-index-data}
change_index_data <- function(ntimes_val = NULL,
                         nx_val = NULL,
                         nfacet_val = NULL,
                         sim_function = sim_varx_normal){

  data <- sim_panel(
    nx = nx_val, nfacet =  nfacet_val,
    ntimes = ntimes_val,
    # sim_dist = sim_varx_normal(2, 3, 5, 10, 5, -1.5)
    sim_dist = sim_function(nx_val, nfacet_val, mean_val, sd_val, w1_val, w2_val)
  ) %>% unnest(data)


  index_new <- map(seq_len(ntimes_val), function(i){
    map((seq_len(nx_val*nfacet_val)), function(j)
    {
      value = i + (j-1)*ntimes_val
    })
  }) %>% unlist()

  data_new = data %>%
    ungroup() %>%
    mutate(index_old = row_number(),
           index_new = index_new)

  y = data_new[match(index_new, data_new$index_old),]

  y <- y %>%
    mutate(time = row_number()) %>%
    select(-c(index_old, index_new))

  return(y)
}
```


```{r data-make}
data_null <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_null_normal) %>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_null")


data_varf <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varf_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varf")

data_varx <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varx_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varx")

data_varall <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varall_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varall")

data_q <- bind_rows(data_null,
                      data_varf,
                      data_varx,
                      data_varall) %>%
  mutate(seed_id = sprintf("%02d", 
                              as.numeric(seed_id))) %>% 
  mutate(unique_data = paste(data_type, seed_id, sep = "-"))

data_q$data_type <- factor(data_q$data_type,
                                    levels = c("data_null" , "data_varf",
                                               "data_varx","data_varall"))

```




```{r JS-mydata}
# data_q_wide <- data_q %>%
#   select(id_facet, id_x, unique_data, sim_data_quantile) %>%
#   pivot_wider(names_from = unique_data,
#               values_from = sim_data_quantile) %>%
#   select(-c(1, 2))
# 
# ndata <- data_q %>% distinct(unique_data) %>% mutate(index = row_number())
# ldata <- nrow(ndata)
# lcomb <-  nx_val*nfacet_val
# 
# dist_data <- map(1:ldata, function(x){ # first data
#   map(1:ldata, function(y){ # 2nd data
#     map(1:lcomb, function(z){ # number of combinations nx*nfacet
#       JS(
#         prob = quantile_prob_val,
#         unlist(data_q_wide[z,x]),
#         unlist(data_q_wide[z,y])
#       ) %>% as_tibble()
#     })%>% bind_rows(.id = "combinations")
#   })%>% bind_rows(.id = "data_type1")
# }) %>% bind_rows(.id = "data_type2")
# 
# 
# 
# dist_mat <- dist_data %>%
#   group_by(data_type1, data_type2) %>%
#   summarise(dist = sum(value)) %>%
#   pivot_wider(names_from = data_type2,
#               values_from = dist) %>%
#   mutate(data_type1 = as.numeric(data_type1)) %>%
#   left_join(ndata, by = c("data_type1" = "index"))
# 
# 
# dist_mat_format <- dist_mat %>%
#   ungroup() %>%
#   select(-data_type1, -unique_data)
# 
# 
# rownames(dist_mat_format) <- dist_mat$unique_data
# write_rds(dist_mat_format, "data/gracsr/validation/interact-4D/distmat_400series.rds")
# write_rds(dist_mat, "data/gracsr/validation/interact-4D/distmat_400series_withnames.rds")
dist_mat_format <- read_rds("data/gracsr/validation/interact-4D/distmat_400series.rds")
dist_mat <- read_rds("data/gracsr/validation/interact-4D/distmat_400series_withnames.rds")

```



```{r hier-clust, fig.cap = "A dendogram showing the branching of $400$ series with each of the $4$ designs repeated $100$ times."}
d = stats::as.dist(dist_mat_format)
par(cex=0.5)
hc = stats::hclust(d,method="complete")
plot(hc)
rect.hclust(hc, k = 4, border = "red")
```

```{r mds-plot-interact, fig.cap= "MDS summary plot of $400$ series with each of the $4$ designs repeated $100$ times. It can be observed that all the series belonging to same design lie closer in this plot and all the ones in different designs are placed further away."}
mds <- d %>%
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- dist_mat$unique_data

groups<-cutree(hc, k=4)

all_data_cluster <- cbind(dist_mat_format, groups) %>%
  cbind(mds)%>%
  mutate(groups = as.factor(groups)) %>% as_tibble()


mds_plot_10 <- ggplot(all_data_cluster,
       aes(x = Dim.1,
           y = Dim.2,
       color = groups)) +
  #geom_text(label = rownames(mds),check_overlap = TRUE)+
  geom_point(size = 0.5) +
  theme_classic()+
  theme(legend.position = "bottom") +
  scale_color_brewer(palette = "Dark2") +
  coord_fixed(ratio = 1)

mds_plot_10
```


```{r heatmap-interact, fig.height=8, fig.cap="A heatmap showing the cluster summary of $400$ series. Deciles of the clusters are plotted along the y-axis with each group on the x-axis faceted by each combination of the interacting granularity. The four distinct behavior across clusters are evident in all the combinations, with the first block corresponding to $data_null$. The deciles of the second block changes across both facet and x-axis. For similar reasons, the second, third and forth blocks correspond $data_varall$, $data_varf$ and $data_varx$ respectively." }
heatmap_raw <- data_q %>% 
  #select(unique_data, sim_data_quantile, id_facet, id_x) %>% 
  mutate(category = paste0("facet= ", id_facet, ",x= ", id_x)) %>% 
  unnest(sim_data_quantile) %>% 
  group_by(unique_data, category) %>% 
  mutate(D = row_number()) %>% 
  arrange(D, category) %>% 
  left_join(dist_mat, by = c("unique_data")) %>% 
  select(c(2:9))


data2 <- cbind(dist_mat_format, groups) 

heatmap_final <- bind_cols(serial_data = rownames(data2) %>%
                             as_tibble(),
          group = data2$groups) %>%
  left_join(heatmap_raw, by = c("value" = "unique_data")) %>% select(-id_facet, - id_x, - data_type1) %>% 
  arrange(D, category, group)


heatmap_final$data_type <- factor(heatmap_final$data_type,
                                   levels = c("data_null" , "data_varf",
                                              "data_varx","data_varall"))

heatmap_final$category <- reorder(heatmap_final$category, heatmap_final$data_type)
  
ggplot(heatmap_final,
       aes(x= reorder(value, data_type), y = as.factor(D))) +
  geom_tile(aes(fill = sim_data_quantile)) +
  scale_fill_distiller(palette = "YlGnBu", direction = 1)+ 
  facet_wrap(~category, ncol = 1) +
  theme(axis.text.x = element_text(angle = 90)) +
  ylab("Deciles") +
  xlab("data sets") +
  theme(legend.position = "bottom")+
  scale_x_discrete(breaks = c("data_null" , "data_varf",
                                              "data_varx","data_varall"))
  
```


## Clustering 353 customers

<!-- All the customers after data filtering and variable selection in the main chapter, before proceeding to instance selection. -->


```{r data-load-entire}
# Read the nqt distances

wkndwday <- read_rds(here("data/gracsr/dist_gran_wkndwday_356cust_nqt.rds")) %>% broom::tidy()

moy <- read_rds(here("data/gracsr/dist_gran_moy_356cust_nqt.rds")) %>% broom::tidy()

hod <- read_rds(here("data/gracsr/dist_gran_hod_356cust_nqt.rds")) %>% broom::tidy()


# Make the distance metrics

distance <- wkndwday %>% 
  left_join(moy, by = c("item1", "item2")) %>% 
  left_join(hod, by = c("item1", "item2")) %>% 
  rename("wkndwday" ="distance.x",
         "moy" = "distance.y",
         "hod" = "distance") %>%
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2))) 

filtered_distance <- distance %>%
  filter(!(item1 %in% c(8196183, 8508008, 8680538))) %>% 
  filter(!(item2 %in% c(8196183, 8508008, 8680538)))

total_distance <- filtered_distance %>% 
  mutate(total = wkndwday/2 + moy/12 + hod/24) 


total_distance_wide <- total_distance %>% pivot_wider(-c(2:5), 
                                                      names_from = item2,
                                                      values_from = total)


rownames(total_distance_wide) <- total_distance_wide$item1

mds_data <- total_distance_wide %>% 
  mutate_all(~replace(., is.na(.), 0)) %>%
  tibble::rownames_to_column() %>%  
  dplyr::select(-item1) %>% 
  pivot_longer(-rowname) %>% 
  pivot_wider(names_from=rowname, values_from=value) 

rownames(mds_data) <- total_distance_wide$item1

df <- mds_data[-1] %>% as.matrix()
DM <- matrix(0, ncol(mds_data), ncol(mds_data))
DM[lower.tri(DM)] = df[lower.tri(df, diag=TRUE)] # distance metric
f = as.dist(DM)


first_lot <- mds_data %>% names()

id <- c(first_lot[-1], mds_data$name[nrow(mds_data)])
```


```{r opt-clust-350, fig.cap = "Cluster separation for the 353 customers across the number of clusters is shown. When the cluster size changes from 17 to 18, the separation index drops sharply and then flattens out, resulting in the appearance of the elbow. Hence, when grouping the 353 customers, the number of clusters is taken to be 17."}

# Find optimal number of clusters

k = array()
for(i in 5:50)
{
  group <- f %>% hclust (method = "ward.D") %>% cutree(k=i)
  p <- cluster.stats(f, clustering = group, silhouette = TRUE)
  k[i]=p$sindex
}

ggplot(k %>% as_tibble %>% mutate(k = row_number()), aes(x=k, y = value)) + geom_line() + scale_x_continuous(breaks = seq(2, 50, 2), minor_breaks = 1) + xlab("number of clusters") + ylab ("separation index")
```


```{r group-353}

# plot(k, type = "l")
# 6 coming as the number of clusters with maximum silwidth

group <- f %>% hclust (method = "ward.D") %>% cutree(k=17)

cluster_result <- bind_cols(customer_id = id, group = group) %>%
  arrange(group)
# %>% 
#   mutate(divide_cust = c(rep(1, 177), rep(2, 176)))

# cluster_result %>% group_by(group) %>% count()

```


```{r data-heatmap-hod-group-entire}
legend_title <- "group"

data_pick <- read_rds(here::here("data/gracsr/elec_nogap_2013_clean_356cust.rds")) %>%
  mutate(customer_id = as.character(customer_id)) %>%
  gracsr::scale_gran( method = "robust",
                      response = "general_supply_kwh")

data_group <- data_pick  %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
   gracsr::scale_gran( method = "robust",
                       response = "general_supply_kwh") %>% 
  left_join(cluster_result, by = c("customer_id"))

data_heatmap_hod_group <- quantile_gran(data_group,
                                  gran1="hour_day",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

  
data_heatmap_hod_group$category <- factor(data_heatmap_hod_group$category, levels = 0:23)

# data_heatmap_hod_group$group <- paste("group", data_heatmap_hod_group$group, sep = "-")

hod_group_entire1 <- data_heatmap_hod_group %>% 
  filter(group<9) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=group,
                  fill = as.factor(group), alpha = 0.5),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=group, 
                color = as.factor(group)), size = 1)+
  facet_wrap(~group, 
             scales = "free_y",  
             nrow = 8,labeller = "label_both") + 
              #labeller = labeller(xfacet = c(`1` = "Group 2", `2` = "Group 4",`3` = "Group 1",`4` = "Group 3"))
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") + 
  ylab("demand (in Kwh)") + 
  theme_bw()  +
  scale_x_discrete(breaks = seq(1, 24, 3))+ 
  #theme(strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme_application() +
  scale_fill_manual(values=as.vector(okabe(n = 8)))+
  scale_color_manual(values=as.vector(okabe(n = 8)))+
  theme(legend.position = "bottom")

hod_group_entire2 <- data_heatmap_hod_group %>% 
  filter(group>=9) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=group,
                  fill = as.factor(group), alpha = 0.5),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=group, 
                color = as.factor(group)), size = 1)+
  facet_wrap(~group, 
             scales = "free_y",  
             nrow = 9,labeller = "label_both") + 
              #labeller = labeller(xfacet = c(`1` = "Group 2", `2` = "Group 4",`3` = "Group 1",`4` = "Group 3"))
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") + 
  ylab("demand (in Kwh)") + 
  theme_bw()  +
  scale_x_discrete(breaks = seq(1, 24, 3))+ 
  #theme(strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme_application() +
  scale_fill_manual(values=as.vector(tableau20(20)))+
  scale_color_manual(values=as.vector(tableau20(20)))+
  theme(legend.position = "bottom")

```

```{r data-heatmap-moy-group-entire}
data_heatmap_moy_group <- quantile_gran(data_group,
                                  gran1="month_year",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

data_heatmap_moy_group$category <- factor(data_heatmap_moy_group$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))


# data_heatmap_moy_group$group <- paste("group", data_heatmap_moy_group$group, sep = "-")


moy_group_entire1 <- data_heatmap_moy_group %>% 
  filter(group<9) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=group, color = as.factor(group)), size = 1 ) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_both",
             nrow = 8) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + 
  ylab("demand (in Kwh)") +
  theme_bw() + theme_application() +
  scale_fill_manual(values=as.vector(okabe(n = 8)))+
  scale_color_manual(values=as.vector(okabe(n = 8)))+
  theme(legend.position = "bottom")


moy_group_entire2 <- data_heatmap_moy_group %>% 
  filter(group>=9) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=group, color = as.factor(group)), size = 1 ) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_both",
             nrow =  9) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + 
  ylab("demand (in Kwh)") +
  theme_bw() + theme_application() +
  scale_fill_manual(values=as.vector(tableau20(20)))+
  scale_color_manual(values=as.vector(tableau20(20)))+
  theme(legend.position = "bottom")
```


```{r data-wnwd-group-entire}
wkndwday_data <- data_group %>%
  create_gran("wknd_wday") 
# wkndwday_data$group <- as.factor(wkndwday_data$group)
# %>% 
#   create_gran("hour_day")

ylim1 = boxplot.stats(wkndwday_data$general_supply_kwh)$stats[c(1, 5)]

wkndwday_group_entire1 <- wkndwday_data%>% 
  filter(group<9) %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x=wknd_wday, y = general_supply_kwh)) +
  #lvplot::geom_lv(aes(fill = as.factor(group)), k=5) +
  geom_boxplot(aes(fill = group, color = group),alpha = 0.5, outlier.alpha = 0.05)+
  #geom_boxplot(outlier.size = 1) + 
  coord_cartesian(ylim = ylim1*1.05)+
  #ggridges::geom_density_ridges2(aes(x = general_supply_kwh, y = wknd_wday,fill = as.factor(group))) + coord_flip() +
#geom_boxplot(aes(fill = as.factor(group))) +
  #scale_fill_lv() +
 xlab("wknd-wday") + 
  ylab("demand (in Kwh)") +
   facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_both",
             nrow = 8) + 
  theme_bw() + theme_application() +
  scale_fill_manual(values=as.vector(okabe(n = 8)))+
  scale_color_manual(values=as.vector(okabe(n = 8)))+
  theme(legend.position = "none")

wkndwday_group_entire2 <- wkndwday_data%>% 
  filter(group>=9) %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x=wknd_wday, y = general_supply_kwh)) +
  #lvplot::geom_lv(aes(fill = as.factor(group)), k=5) +
  geom_boxplot(aes(fill = group, color = group),alpha = 0.5, outlier.alpha = 0.05)+
  #geom_boxplot(outlier.size = 1) + 
  coord_cartesian(ylim = ylim1*1.05)+
  #ggridges::geom_density_ridges2(aes(x = general_supply_kwh, y = wknd_wday,fill = as.factor(group))) + coord_flip() +
#geom_boxplot(aes(fill = as.factor(group))) +
  #scale_fill_lv() +
 xlab("wknd-wday") + 
  ylab("demand (in Kwh)") +
   facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_both",
             nrow = 9) + 
  theme_bw() + theme_application() +
  scale_fill_manual(values=as.vector(tableau20(20)))+
  scale_color_manual(values=as.vector(tableau20(20)))+
  theme(legend.position = "none")
```


```{r combined-groups-js-entire-all, fig.cap = "The distribution of electricity demand for the clusters across hod, moy and wkndwday for the $17$ groups from $353$ customers. Wknd-wday variations across groups are not distinguishable, but ideally each group should have an unique combination of hod and moy.", fig.height=6}

group_entire1 <- (hod_group_entire1 + moy_group_entire1 + wkndwday_group_entire1)&theme_characterisation()& theme(legend.position = 'none')

group_entire2 <- (hod_group_entire2 + moy_group_entire2 + wkndwday_group_entire2)&theme_characterisation()& theme(legend.position = 'none')

ggpubr::ggarrange(group_entire1, group_entire2)
 #  
 #  
 #  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')+
 #  plot_layout(guides = "collect")& theme(legend.position = 'none')
 # 
 # 
 # (hod_group_entire/moy_group_entire/ wkndwday_group_entire) +
 #  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')+
 #  plot_layout(guides = "collect")& theme(legend.position = 'none')
```




