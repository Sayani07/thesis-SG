# Clustering time series based on probability distributions across temporal granularities {#ch-gracsr}

Clustering is a potential approach for organizing large collections of time series into small homogeneous groups, but a difficult step is determining an appropriate metric to measure similarity between time series. The similarity metric needs to be capable of accommodating long, noisy, and asynchronous time series and also capture cyclical patterns. In this paper, two approaches for measuring distances between time series are presented, based on probability distributions over cyclic temporal granularities. Both are compatible with a variety of clustering algorithms. Cyclic granularities like hour-of-the-day, work-day/weekend, and month-of-the-year, are useful for finding repeated patterns in the data. Measuring similarity based on probability distributions across cyclic granularities serves two purposes: (a) characterizing the inherent temporal data structure of long, unequal-length time series in a manner robust to missing or noisy data; (b) small pockets of similar repeated behaviors can be captured. This approach is capable of producing useful clusters, as demonstrated on validation data designs and a sample of residential smart meter records.

```{r external-gracsr, include = FALSE, cache = FALSE}
#knitr::read_chunk('script/main.R')
knitr::read_chunk('scripts/gracsr/library.R')
knitr::read_chunk('scripts/gracsr/theme.R')
knitr::read_chunk('scripts/gracsr/application.R')
knitr::read_chunk('scripts/gracsr/validation.R')
```


```{r load-lib}

```


```{r mytheme-application}
```

```{r theme_characterisation}
```


```{r mytheme-application2}

```


```{r mytheme-application3}

```


```{r theme-validation}

```


## Introduction

<!-- time series clustering and its challenges -->
Time series clustering is the process of unsupervised partitioning of $n$ time series data into $k$ ($k<n$) meaningful groups such that homogeneous time series are grouped together based on a certain similarity measure. The time series features, length of time series, representation technique, and, of course, the purpose of clustering time series all influence the suitable similarity measure or distance metric to a meaningful level. The three primary methods to time series clustering [@liao2005clustering] are algorithms that operate directly with distances or raw data points in the time or frequency domain (distance-based), with features derived from raw data (feature-based), or indirectly with models constructed from raw data (model-based). The efficacy of distance-based techniques is highly dependent on the distance measure utilized. Defining an appropriate distance measure for the raw time series may be a difficult task since it must take into account noise, variable lengths of time series, asynchronous time series, different scales, and missing data. Commonly used distance-based similarity measures as suggested by a decade review of time series clustering approaches [@Aghabozorgi2015-ct] are Euclidean, Pearson's correlation coefficient and related distances, Dynamic Time Warping (DTW), Autocorrelation, Short time series distance, Piecewise regularization, cross-correlation between time series, or a symmetric version of the Kullback–Liebler distances [@liao2007clustering] but on vector time series data. Among these alternatives, Euclidean distances have high performance but need the same length of data over the same period, resulting in information loss regardless of whether it is on raw data or a smaller collection of features. DTW works well with time series of different lengths [@corradini2001dynamic], but it is incapable of handling missing observations. Surprisingly, probability distributions, which may reflect the inherent temporal structure of a time series, have not been considered in determining time series similarity.

\noindent This work is motivated by a need to cluster a large collection of residential smart meter data, so that customers can be grouped into similar energy usage patterns. These can be considered to be univariate time series of continuous values which are available at fine temporal scales. These time series data are long (with more and more data collected at finer resolutions), are asynchronous, with varying time lengths for different houses and sporadic missing values. Using probability distributions is a natural way to analyze these types of data because they are robust to uneven length, missing data, or noise. This paper proposes two approaches for obtaining pairwise similarities based on Jensen-Shannon distances between probability distributions across a selection of cyclic granularities. Cyclic temporal granularities [@Gupta2021-hakear], which are temporal deconstructions of a time period into units such as hour-of-the-day or work-day/weekend, can measure repetitive patterns in large univariate time series data. The resulting clusters are expected to group customers that have similar repetitive behaviors across cyclic granularities. The benefits of this approach are as follows.

- When using probability distributions, data does not have to be the same length or observed during the exact same time period (unless there is a structural pattern).

<!-- (for example, some customers observed only for holidays, others observed over non holidays or year of observation is different which can lead to technological advancement.) -->

- Jensen-Shannon distances evaluate the distance between two distributions rather than raw data, which is less sensitive to missing observations and outliers than other conventional distance methods.

- While most clustering algorithms produce clusters similar across just one temporal granularity, this technique takes a broader approach to the problem, attempting to group observations with similar distributions across all interesting cyclic granularities.

- It is reasonable to define a time series based on its degree of trend and seasonality, and to take these characteristics into account while clustering it. The modification of the data structure by taking into account probability distributions across cyclic granularities assures that there is no trend and that seasonal variations are handled independently. As a result, there is no need to de-trend or de-seasonalize the data before applying the clustering method. For similar reasons, there is no need to exclude holiday or weekend routines.


<!--_Background and motivation_-->


The primary application of this work is data from the Smart Grid, Smart City (SGSC) project (2010–2014) available through @smart-meter. Half-hourly measurements of usage for more than 13,000 electricity smart meter customers are provided from October 2011 to March 2014. <!--Larger data sets include greater uncertainty about customer behavior due to growing variety of customers.--> Customers vary in size, location, and amenities such as solar panels, central heating, and air conditioning. The behavioral patterns differ amongst customers due to many temporal dependencies. Some customers use a dryer, while others dry their clothes on a line. Their weekly usage profile may reflect this. They may vary monthly, with some customers using more air conditioners or heaters than others, while having equivalent electrical equipment and weather circumstances. Some customers are night owls, while others are morning larks. Daily energy usage varies depending on whether customers stay home or work away from home. Age, lifestyle, family composition, building attributes, weather, availability of diverse electrical equipment, among other factors, make the task of properly segmenting customers into comparable energy behavior complex. The challenge is to be able to cluster consumers into these types of expected patterns, and other unexpected patterns, using only their energy usage history [@Ushakova2020-rl]. <!--To safeguard the customers' privacy, it is probable that such information is not accessible. Also, energy suppliers may not always update client information, such as property features, in a timely manner.--> There is a growing need to have methods that can examine the energy usage heterogeneity observed in smart meter data and what are some of the most common power consumption patterns.


<!-- _Related work_-->

There is an extensive body of literature focused on time series clustering related to smart meter data. @Tureczek2017-pb conducted a systematic study of over $2100$ peer-reviewed papers on smart meter data analytics. <!--None of the $34$ articles chosen for their emphasis use Australian smart meter data.--> The most often used algorithm is $k$-means [@Rhodes2014-it]. $k$-means can be made to perform better by explicitly incorporating time series features such as correlation or cyclic patterns rather than performing it on raw data. To reduce dimensionality, several studies use principal component analysis (PCA) or factor analysis to pre-process smart-meter data before clustering [@Ndiaye2011-pf]. PCA eliminates correlation patterns and decreases feature space, but loses interpretability. Other algorithms utilized in the literature include $k$-means variants, hierarchical clustering, and greedy $k$-medoids. Time series data, such as smart meter data, are not well-suited to any of the techniques mentioned in @Tureczek2017-pb. Only one study [@ozawa2016determining] identified time series characteristics by first conducting a Fourier transformation, to convert data from time to frequency domain, followed by $k$-means to cluster by greatest frequency. @Motlagh2019-yj suggests that the time feature extraction is limited by the type of noisy, patchy, and unequal time series common in residential customers and addresses model-based clustering by transforming the series into other objects such as structure or set of parameters which can be more easily characterized and clustered. @chicco2010renyi addresses information theory-based clustering such as Shannon or Renyi entropy and its variations. @Melnykov2013-sp discusses how outliers, noisy observations and scattered observations can complicate estimating mixture model parameters and hence the partitions. None of these methods focuses on exploring heterogeneity in repetitive patterns based on the dynamics of multiple temporal dependencies using probability distributions, which forms the basis of the methodology reported here.

<!-- Given none of the methods use probability distributions across cyclic granularity to explore heterogeneity in different repetitive behaviors, we present a way to do that by presenting similarity through probability distributions across cyclic granularities.  -->


This paper is organized as follows. Section \ref{sec:methodology} provides the clustering methodology. Section \ref{sec:validation} shows data designs to validate our methods. Section \ref{sec:application-gracsr} discusses the application of the method to a subset of the real data. Finally, we summarize our results and discuss possible future directions in Section \ref{sec:discussion}.



<!-- A typical clustering technique includes the following steps: (a) establishing distance (dissimilarity) and similarity through feature or model extraction and selection; and (b) selecting the clustering algorithm design. Distance measures could be time-domain based or frequency-domain (fast Fourier transform). -->



<!-- A model-based clustering works by transforming the series into other other objects such as structure or set of parameters which can be more easily characterized and clustered [@Motlagh2019-yj]. [@chicco2010renyi] addresses information theory-based clustering such as Shannon or Renyi entropy and its variations. The essential temporal characteristics of the curves are defined or extracted using feature-based clustering. -->






<!-- This work -->
<!-- This is similar to a stochastic approach [@Motlagh2019-yj) to clustering, which proposes interpreting electricity demand as a random process and extracting time series characteristics, or a model of the series, to enable unsupervised clustering. Unsupervised clustering is only as good as the features that are extracted/selected or the distance metrics that were utilized. Well-designed additional features may collect characteristics that default features cannot. Based on the underlying structure of the temporal data, this article offers new distance metric and features for clustering and applies them to actual smart-meter data. Firstly, the distance metric is based on probability distribution, which in our knowledge is the first attempt to cluster smart meter data using probability distributions. These recorded time series are asynchronous, with varying time lengths for different houses and missing observations. Taking probability distributions helps to deal with such data, while helping with dimension reduction in one hand but not losing too much information due to aggregation. Secondly, we recognise that most clustering algorithms only provide hourly energy profiles during the day, but this approach provides a wider approach to the issue, seeking to group consumers with similar shapes over all important cyclic granularities. Since cyclic granularities are considered instead of linear granularities, clustering would group customers that have similar repetitive behavior across more than one cyclic granularities across which patterns are expected to be significant.  -->


<!-- common similarity measures -->

<!-- electricity data structure -->

<!-- common similarity measures used there -->


<!-- lit review -->




## Clustering methodology {#sec:methodology}


<!-- In contrast to models, a feature-based strategy is used to explicitly define or automatically extract the curves’ key time features, for instance by application of PCA on the daily curves [ --


<!-- Most papers discussed in Tureczek2017-pb fail to accept smart meter readings as time series data, a data type which contains a temporal component. The omission of the essential time series features in the analysis leads to the application of methods that are not designed for handling temporal components. K-Means ignores autocorrelation, unless the input data is pre-processed. The clusters identified in the papers are validated by a variety of indices, with the most prevalent -->
<!-- being the cluster dispersion index (CDI) [22–24], the Davies–Bouldin index (DBI) [25,26] and the mean index adequacy (MIA) [8,13]. -->


<!-- The data set solely contains readings from smart meters and no information about the consumers' specific physical, geographical, or behavioral attributes. As a result, no attempt is made to explain why consumption varies. Instead, this work investigates how much energy usage heterogeneity can be found in smart meter data and what some of the most common electricity use patterns are. -->

<!-- Because of this importance, countless approaches to estimate time series similarity have been proposed.  -->


<!--The foundation of our method is unsupervised clustering algorithms based exclusively on the time series data. The proposed methods leverage the intrinsic data structure hidden within cyclic temporal granularities.--> The existing work on clustering probability distributions assumes we have independent and identically distributed samples $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. <!--So going back to the smart meter example, $f_i(v)$ is the distribution of customer $i$ and $v$ is electricity demand.--> In our approach, instead of considering the probability distributions of the linear time series, we compare them across different categories of a cyclic granularity. We can consider categories of an individual cyclic granularity ($A$) or combination of categories for two interacting granularities ($A, B$) to have a distribution, where $A$ and $B$ are defined as $A = \{a_j: j=1, 2, \dots J\}$ and $B = \{b_k: k = 1, 2, \dots K\}$. For example, let us consider two cyclic granularities, $A$ and $B$, representing hour-of-day and day-of-week, respectively. Then $A = \{0, 1, 2, \dots, 23\}$ and $B = \{Mon, Tue, Wed, \dots, Sun\}$. In case individual granularities are considered, there are $J = 24$ distributions of the form $f_{i,j}(v)$ or $K = 7$ distributions of the form $f_{i,k}(v)$ for each customer $i$. In case of interaction, $J \times K=168$ distributions of the form $f_{i,j, k}(v)$ could be conceived for each customer $i$. Hence clustering these customers is equivalent to clustering these collections of conditional univariate probability distributions. Towards this goal, the next step is to decide how to measure distances between collections of univariate probability distributions. <!--There are multiple ways to measure similarities depending on the aim of the analysis. <!--This paper focuses on looking at the (dis) similarity between underlying distributions that may have resulted in different patterns across different cyclic temporal granularities, that eventually have resulted in the (dis) similarity between time series. --> Here, we describe two approaches for finding distances between time series. Both of these approaches may be useful in a practical context, and produce very different but equally useful customer groupings. The distances can be supplied to any usual clustering algorithm, including $k$-means or hierarchical clustering, to group observations into a smaller more homogeneous collection. <!--These clusters may be commonly associated with real-world data segmentation. However, since the data is unlabeled a priori, more information is required to corroborate this.--> The flow of the procedures is illustrated in Figure \ref{fig:flowchart}.


```{r flowchart, fig.cap = "Flow chart illustrating the pipeline for our method for clustering time series.", out.width="100%"}
knitr::include_graphics("img/flowchart.png")

```

### Selecting granularities

@Gupta2021-hakear provide a distance measure ($\wpd$) for determining the significance of a cyclic granularity, and a ranking of multiple cyclic granularities. (This extends to harmonies, pairs of granularities that might interact with each other.) We define "significant" granularities as those with significant distributional differences across at least one category. The reason for subsetting granularities in this way is that clustering algorithms perform badly in the presence of nuisance variables. Granularities that do not have some difference between categories are likely to be nuisance variables. It should be noted that all of the time series in a collection may not have the same set of significant granularities.<!--This is the approach for managing this generate a subset ($S_c$) of significant granularities across a collection of time series:--> This is the approach for generating a subset ($S_c$) of significant granularities across a collection of time series:

 (a) Remove granularities from the comprehensive list that are not significant for any time series.

 (b) Select only the granularities that are significant for the majority of time series.

<!--There will be time series in both cases where one or a few selected granularities are boring. Even in that case, having this group of observations with no interesting patterns at a granularity that regularly discovers patterns may be valuable. If, on the other hand, the granularities under examination are truly important for a group of data, unique patterns may be identified while clustering them.-->



### Data transformation

The shape and scale of the distribution of the measured variable (e.g. energy usage) affects distance calculations. Skewed distributions need to be symmetrized. Scales of individuals need to be standardized, because clustering is to select similar patterns, not magnitude of usage. (Organizing individuals based on magnitude can be achieved simply by sorting on a statistic like the average value across time.) <!--Observations often have a somewhat skewed time series distribution and their ranges might vary greatly. Statistical transformations are employed to bring all of them to the same range or normalize each observation. This is important because we are not interested in trivial clusters that vary in magnitudes, but rather in uncovering comparable patterns of distributional differences between categories.--> For the JS-based approaches, two data transformation techniques are recommended, normal-quantile transform (NQT) and robust scaling (RS).<!-- NQT is used in @Gupta2021-hakear prior sorting granularities.-- $\wpd$, which is the foundation of wpd-based distances. XX wpd was not yet introduced--> While @Gupta2021-hakear already use NQT when computing $\wpd$, it could be useful to standardize it for the selected set of significant granularities prior to computing the distances.
    
- RS: The normalized $i^{th}$ observation is denoted by $v_{norm} = \frac{v_t - q_{0.5}}{q_{0.75}-q_{0.25}}$, where $v_t$ is the actual value at the $t^{th}$ time point and $q_{0.25}$, $q_{0.5}$ and $q_{0.75}$ are the $25^{th}$, $50^{th}$ and $75^{th}$ percentiles of the time series for the $i^{th}$ observation. Note that $v_{norm}$ has zero mean and median, but otherwise the shape does not change.

- NQT: The raw data for all observations is individually transformed [@Krzysztofowicz1997-bv], so that the transformed data follows a standard normal distribution. NQT will symmetrize skewed distributions. <!--As a result, determining which raw distribution was used is difficult using the modified distribution.--> A drawback is that any multimodality will be concealed. This should be checked prior to applying NQT. 

<!-- NQT, however, is an useful transformation which often improves the clustering performance. Since our variables are cyclic granularities which are derived from linear granularities, NQT ensures that the conditional distribution across each variable is also normalised. -->


<!-- is not a problem for implementation of this methodology as distributions are characterized by quantiles and the order of the quantiles is reserved under NQT. -->


<!-- First, the original data is ranked in ascending order and the probabilities $P(Y<=y(i)) = i/(n+1)$ are attached to $y(i)$, in terms of their ranking order. A NQT based transformation is applied by computing from a standard normal distribution a variable $\eta(i)$, which corresponds to the same probability $P(\eta< \eta(i)) = i/n+1$.By doing this, the new variables $\eta(i)$ will be marginally distributed according to standard Normal, N(0,1). -->



<!-- It is worth noting that when studying these similarities, a variety of objectives may be pursued. One objective could be to group time series with similar shapes over all relevant cyclic granularities. In this scenario, the variation in customers within each group is in magnitude rather than shape, while the variation between groups is only in shape. There -->
<!-- are distance measures are used for shape-based clustering [Ding et al. 2008; Wang -->
<!-- et al. 2013] and many more but none of them look at the probability distributions while computing similarity. Moreover, most distance measures offer similar shape across just one dimension. For example, we often see "similar" daily energy profiles across hours of the day, but we suggest a broader approach to the problem, aiming to group consumers with similar distributional shape across all significant cyclic granularities. Another purpose of clustering could be to group customers that have similar differences in patterns across all major cyclic granularities, capturing similar jumps across categories regardless of the overall shape. For example, in the first goal, similar shapes across hours of the day will be grouped together, resulting in customers with similar behavior across all hours of the day, whereas in the second goal, any similar big-enough jumps across hours of the day will be clubbed together, regardless of which hour of the day it is. Both of these objectives may be useful in a practical context and, depending on the data set, may or may not propose the same customer classification. Depending on the goal of clustering, the distance metric for defining similarity would be different. These distance metrics could be fed into a clustering algorithm to break large data sets into subgroups that can then be analyzed separately. These clusters may be commonly associated with real-world data segmentation. However, since the data is unlabeled a priori, more information is required to corroborate this. This section presents the work flow of the methodology:  -->

### Data pre-preprocessing

Computationally in R, the data is assumed to be a "tsibble object" [@wang2020tsibble] equipped with an index variable representing inherent ordering from past to present and a key variable that defines observational units over time. The measured variable for an observation is a time-indexed sequence of values. This sequence, however, could be shown in several ways. A shuffle of the raw sequence may represent hourly consumption throughout a day, a week, or a year. Cyclic granularities can be expressed in terms of the index set in the tsibble data structure.

The data object will change when cyclic granularities are computed, as multiple observations will be categorized into levels of the granularity, thus inducing multiple probability distributions. Directly computing Jensen-Shannon distances between the entire probability distributions can be computationally intensive. Thus it is recommended that quantiles are used to characterize the probability distributions. In the final data object, each category of a cyclic granularity corresponds to a list of numbers which is composed of a few quantiles.


### Distance metrics

<!-- Considering each individual or combined categories of cyclic granularities to have an underlying distribution lead to a collection of conditional distributions for each customer $i$. -->


The total (dis) similarity between each pair of customers is obtained by combining the distances between the collections of conditional distributions. This needs to be done in a way such that the resulting metric is a distance metric, and could be fed into the clustering algorithm. Two types of distance metrics are considered:

<!-- The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated. -->

#### JS-based distances

This distance metric considers two time series to be similar if the distributions of each category of an individual cyclic granularity or combination of categories for interacting cyclic granularities are similar. In this study, the distribution for each category is characterized using deciles (can potentially consider any list of quantiles), and the distances between distributions are calculated using the Jensen-Shannon distances [@Menendez1997-in], which are symmetric and thus could be used as a distance measure.


The sum of the distances between two observations $x$ and $y$ in terms of a cyclic granularity $A$ is defined as $$S^A_{x,y} = \sum_{j\in A} D(x_j,y_j)$$ where $D$ is the Jensen-Shannon distances, $x_j$ is the set of quantiles over the values filtered by $j^{th}$ level of granularity $A$ for observation $x$ (similar for $y$).

The sum of the distances between two observations $x$ and $y$ in terms of a pair of cyclic granularities $(A, B)$ is defined as $$S^{A,B}_{x,y} = \sum_{(j, k) \in (A, B)} D(x_{jk},y_{jk})$$ $x_{jk}$ is the set of quantiles over the values filtered by the combination of $j^{th}$ level of granularity $A$ and $k^{th}$ level of granularity $B$ for observation $x$ (similar for $y$). 

After determining the distance between two series in terms of one granularity, we must combine them to produce a distance based on all significant granularities. When combining distances from individual $L$ cyclic granularities $C_l$ with $n_l$ levels, $$S_{x, y} = \sum_{l \in L}S^{C_l}_{x,y}/n_l$$ is employed, which is also a distance metric since it is the sum of JS distances. This approach is expected to yield groups, such that the variation in observations within each group is in magnitude rather than distributional pattern, while the variation between groups is only in distributional pattern across categories.



#### wpd-based distances

We compute weighted pairwise distances $\wpd$ [@Gupta2021-hakear] for all considered granularities for all observations. $\wpd$ is designed to capture the maximum variation in the measured variable explained by an individual cyclic granularity or their interaction. It is estimated by the maximum pairwise distances between distributions across consecutive categories normalized by appropriate parameters. A higher value of $\wpd$ indicates that some interesting patterns are expected, whereas a lower value would indicate otherwise.

Once we have chosen $\wpd$ as a relevant feature for characterizing the distributions across one cyclic granularity, we have to decide how we combine differences between the multiple features (corresponding to multiple granularities) into a single number. The Euclidean distance between them is chosen, with the granularities acting as variables and $\wpd$ representing the value under each variable. With this approach, we should expect the observations with similar $\wpd$ values to be clustered together. Thus, this approach is useful for grouping observations that have a similar significance of patterns across different granularities. Similar significance does not imply a similar pattern, which is where this technique varies from JS-based distances, which detect differences in patterns across categories.





<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of customers showing similar periodic behavior by considering $\wpd$ values for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- grouping probability distributions across a harmony. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. The following are some of the advantages of our proposed method. -->




### Clustering

#### Number of clusters

Determining the number of clusters is typically a difficult task. Many metrics have been defined for choosing clusters. Most metrics for choosing the optimal number of clusters are based on comparing distances between observations within a class to those distances between observations between classes, which makes the assumption that there are some separated clusters. Some common procedures include the gap statistic [@tibshirani2001estimating], average silhouette width [@rousseeuw1987silhouettes], Dunn index [@Dunn1973-gk] and the separation index (*sindex*) [@Hennig2019-eq; @Hennig2014-ah]. These are constructed by balancing within-cluster homogeneity and between-cluster separation. 


All of the common approaches can give contradictory suggestions for the optimal number of clusters, particularly when the data does not naturally break into groups, or in the presence of nuisance variables (no contribution to clustering) or nuisance observations (inlying and outlying observations falling between clusters). There is no one best metric, which is perhaps a reason why so many metrics exist. 

In this work, we have chosen to use *sindex*. It is a very simple but effective metric. This is computed by averaging the smallest 10% of inter-cluster distances. It is relatively robust to nuisance observations. The value of *sindex* always decreases, and sharp drops in value indicate candidates for the optimal number of clusters. The number of clusters corresponding to the value **before the drop** is the recommendation. 

.<!--determined by calculating the distance between each observation and the cluster to which it does not belong. Then, the least 10% of these distances are averaged to only evaluate places near the cluster edges.-->


#### Algorithm

With a way to obtain pairwise distances, any clustering algorithm can be employed that supports the given distance metric as input. A good comprehensive list of algorithms can be found in @Xu2015-ja based on traditional ways like partition, hierarchy, or more recent approaches like distribution, density, and others. We employ agglomerative hierarchical clustering in conjunction with Ward's linkage. Hierarchical cluster techniques fuse neighboring points sequentially to form bigger clusters, beginning with a full pairwise distance matrix. The distance between clusters is described using a "linkage technique". This agglomerative approach successively merges the pair of clusters with the shortest between-cluster distance using Ward's linkage method.

<!--Hierarchical algorithms are one of the most widely used, can operate with data of any shape, has reasonable scalability, and the number of clusters is not needed as a parameter.-->


#### Characterization of clusters

Cluster characterization is an important final stage of a cluster analysis. The primary purpose is to compare the homogeneity within a cluster to the heterogeneity of clusters. This can be done numerically, by tabulating cluster means and standard deviations [@dasu2005grouping], and visually using methods for graphics multivariate data. @Cook2007-qe provide visual examples using both tours [@asimov1985grand] and parallel coordinate plots [@wegman1990hyperdimensional]. Dimension reduction techniques like principal component analysis [@jolliffe2016principal], multidimensional scaling (MDS) [@borg2005modern], t-distributed stochastic neighbor embedding (t-SNE) [@van2008visualizing] and linear discriminant analysis (LDA) [@fisher1936use] are also useful. <!-- A Parallel Coordinates Plot features parallel axes for each variable and each axis is linked by lines. Changing the axes may reveal patterns or relationships between variables for categorical variables. However, for categories with cyclic temporal granularities, preserving the underlying ordering of time is more desirable. Displaying cluster statistics is useful when we have larger problems and it is difficult to read the parallel coordinate plots due to congestion. All of MDS, PCA and t-SNE use a distance or dissimilarity matrix to construct a reduced-dimension space representation, but their goals are diverse. Multidimensional scaling [@borg2005modern) seeks to maintain the distances between pairs of data points, with an emphasis on pairings of distant points in the original space. The t-SNE embedding will compress data points that are close in high-dimensional space. Tour is a collection of interpolated linear projections of multivariate data into lower-dimensional space. The cluster characterization approach varies depending on the distance metric used. Parallel coordinate plots, scatter plot matrices, MDS or PCA are potentially useful ways to characterize clusters using wpd-based distances. For JS-based distances, plotting cluster statistics is beneficial for characterization and variable importance could be displayed through parallel coordinate plots. -->



<!-- CUT IT SHORT -->

<!-- (a) _Parallel coordinate plots_ [@wegman1990hyperdimensional) are often used to visualize high-dimensional and multivariate data, allowing visual grouping and pattern detection. A Parallel Coordinates Plot features parallel axes for each variable. Each axis is linked by lines. Changing the axes may reveal patterns or relationships between variables for categorical variables. However, for categories with cyclic temporal granularities, preserving the underlying ordering of time is more desirable. -->

<!-- (b) _Scatterplot matrix_ contains pairwise scatter plots of the p variables. Pairwise scatter plots are useful for figuring out how variables relate to each other and how factors determine the clustering. -->

<!-- (c) _Displaying cluster statistics_ are useful when we have larger problems and it is difficult to read the Parallel coordinate plots due to congestion. [@dasu2005grouping) -->

<!-- (d) _MDS, PCA and t-SNE_ While all of them use a distance or dissimilarity matrix to construct a reduced-dimension space representation, their goals are diverse. PCA seeks to retain data variance. Multidimensional scaling [@borg2005modern) seeks to maintain the distances between pairs of data points, with an emphasis on pairings of distant points in the original space. t-SNE, on the other hand, is concerned with preserving neighborhood data points. The t-SNE embedding will compress data points which are close in high-dimensional space. -->

<!-- (e) _Tour_ is a collection of interpolated linear projections of multivariate data into lower-dimensional space. As a result, the viewer may observe the high-dimensional data's shadows from a low-dimensional perspective. -->

<!-- The cluster characterization approach varies depending on the distance metric used. Parallel coordinate plots, scatter plot matrices, MDS or PCA are potentially useful ways to characterize clusters using wpd-based distances. For JS-based distances, plotting cluster statistics is beneficial for characterization and variable importance could be displayed through parallel coordinate plots. -->


<!-- This part of the work uses R packages `GGally` [@R-GGally), `Rtsne` [@R-tsne), `ggplot2` (Wickham2009pk), `tour` [@wickham2011tourr), `stats` [@R-language). -->


<!-- - A random sample of the original data is taken for clustering analysis and includes missing and noisy observations (detailed description in Appendix) -->

<!-- - All harmonies are computed for each customer in the sample. -->
<!-- Cyclic granularities which are clashes for all customers in the sample are removed. -->

<!-- - It is worth noting that a number of other solutions may be considered at the pre-processing stage of the method. We have considered a) Normal-Quantile Transform and b) Robust transformation. -->

<!-- - Two methods are considered for computing dissimilarity between two customers. The first one involves computing according to one granularity is computed as the sum of the JS distances between distribution of all the categories of the granularity. When we consider more than one granularity, we consider the sum of the average distances for all the granularity so that the combined metric is also a distance. -->

<!-- - Given the scale of dissimilarity among the energy readings, the model chooses optimal number of clusters -->

<!-- - Once clusters have been allocated, the groups are explored visually. -->

<!-- - Results are reported and compared. -->

<!-- Two methods are used for computing distances between subjects and then hierarchical clustering algorithm is used. -->

<!-- The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a customer and the underlying variable as the electricity demand. So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. -->

<!-- We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote -->

<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of customers showing similar periodic behavior by considering $\wpd$ values for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- ### Notations -->

<!-- Consider an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a household and the underlying variable as the electricity demand. Further consider a cyclic granularity of the form $B = \{ b_k: k = 1, 2, \dots, K\}$. Each customer consists of collection of probability distributions. -->


<!-- So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote $i^{th}$ and $j^{th}$ customer respectively. -->



<!-- a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. -->




<!-- ### A single or pair of granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->

<!-- - _Pre-processing step_ -->

<!-- Robust scaling method or NQT used for each customer. -->


<!-- - _NQT_ -->


<!-- - _Treatment to outliers_ -->






<!-- ### Many granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->


<!-- 1. Compute quantiles of distributions across each category of the cyclic granularity -->
<!-- 2. Compute JS distance between customers for each each category of the cyclic granularity -->
<!-- 3. Total distance between customers computed as sum of JS distances for all hours -->
<!-- 4. Cluster using this distance with hierarchical clustering algorithm (method "Ward.D") -->

<!-- _Pro:_  -->
<!-- - distance metric makes sense to group different shapes together  -->
<!-- - simulation results look great on typical designs  -->
<!-- _Cons:_   -->
<!-- - Can only take one granularity at once   -->
<!-- - Clustering a big blob of points together whereas the aim is to groups these big blob into smaller ones   -->

<!-- ### Multiple-granularities -->

<!-- _Description:_   -->

<!-- Choose all significant granularities and compute wpd for all these granularities for all customers. Distance between customers is taken as the euclidean distances between them with the granularities being the variables and wpd being the value under each variable for which Euclidean distance needs to be measured.   -->
<!-- _Pro:_   -->
<!-- - Can only take many granularities at once -->
<!-- - can apply variable selection PCP and other interesting clustering techniques -->
<!-- - simulation results look great on typical designs -->
<!-- - splitting the data into similar sized groups   -->
<!-- _Cons:_   -->
<!-- - distance metric does not make sense to split the data into similar shaped clusters  -->

## Validation {#sec:validation}

To validate our clustering methods, we have created several different data designs containing different granularity features. <!--to see where one method works better than the other and where they might give us the same outcome --or the effect of missing data <!---and trends on the proposed methods.--> There are three circular granularities $g1$, $g2$ and $g3$ with categories denoted by $\{g10,g11\}$, $\{g20, g21, g22\}$ and $\{g30, g31, g32, g33, g34\}$ and levels $n_{g_1}=2$, $n_{g_2}=3$ and $n_{g_3}=5$. These categories could be integers or some more meaningful labels. For example, the granularity "day-of-week" could be either represented by $\{0, 1, 2, \dots, 6\}$ or $\{Mon, Tue, \dots, Sun\}$. Here categories of $g1$, $g2$ and $g3$ are represented by $\{0, 1\}$, $\{0, 1, 2\}$ and $\{0, 1, 2, 3, 4\}$ respectively. A continuous measured variable $v$ of length $T$ indexed by $\{0, 1, \dots T-1\}$ is simulated such that it follows the structure across $g1$, $g2$ and $g3$. We constructed independent replications of all data designs $R = \{25, 250, 500\}$ to investigate if our proposed clustering method can discover distinct designs in small, medium, and big numbers of series. All designs employ $T=\{300, 1000, 5000\}$ sample sizes to evaluate small, medium, and large-sized series. Variations in method performance may be due to different jumps between categories. So a mean difference of $\mu = \{1, 2, 5\}$ between categories is considered. The performance of the approaches varies with the number of granularities which has interesting patterns across its categories. So three scenarios are considered to accommodate that. Table \ref{tab:range-parameter} shows the range of parameters considered for each scenario.


<!-- ```{r range-parameter, fig.cap="The range of parameters used for the validation study, for the three different scenarios, number of simulations for each design, differences between means across granularities and series lengths. "} -->
<!-- knitr::include_graphics("img/simulation_table.png") -->
<!-- ``` -->


```{r range-parameter}
scenario = c("S1", "S2","S3")
designs = c(5, 4, 4)
niter = list (c(25, 20, 500),
              c(25, 20, 500),
              c(25, 20, 500))
mu = list (c(1, 2, 5),
           c(1, 2, 5),
           c(1, 2, 5))
nT = list (c(300, 1000, 5000),
           c(300, 1000, 5000),
           c(300, 1000, 5000))

tibble::tibble(scenario, designs, niter, mu, nT) %>% rlang::set_names("scenario", "designs", "R", "$\\mu$", "T") %>% 
  #tibble::tibble(scenario, " designs" = designs, "R" =niter,  "$\\mu$" = mu, "T"=nT) %>% 
  knitr::kable(format = "latex",
             escape = FALSE,
             caption = "The range of parameters used for the validation study, for the three different scenarios, number of simulations ($R$) for each design, differences between means ($\\mu$) across granularities and series lengths ($T$).")%>%  kableExtra::collapse_rows(columns = 3:5)
```





<!-- The results for $T=300$ and $R=25$ is shown, that means we have $25$ time series each with length $300$. The rest of the results could be found in the supplementary paper. -->


### Data generation

<!-- An ARMA (p,q) process is used to generate series, where $p$ and $q$ are selected at random such that the series is stationary. The various designs on $g1$, $g2$, and $g3$ are introduced by adding matching designs to this series' innovations. The innovations are considered to have a normal distribution, although they follow the same pattern as the designs. To eliminate the effect of starting values, the first 500 observations in each series are discarded. -->

Each category or combination of categories from $g1$, $g2$ and $g3$ are assumed to come from the same distribution, a subset of them from the same distribution, a subset of them from separate distributions, or all from different distributions, resulting in various data designs. As the methods ignore the linear progression of time, there is little value in adding time dependency to the data generating process. The data type is set to be "continuous," and the setup is assumed to be Gaussian. When the distribution of a granularity is "fixed", it means distributions across categories do not vary and are considered to be from N (0,1). $\mu$ alters in the "varying" designs, leading to varying distributions across categories.


<!-- An ARMA (p,q) process is used to generate series, where $p$ and $q$ are selected at random such that the series is stationary. The various designs on $g1$, $g2$, and $g3$ are introduced by adding matching designs to this series' innovations. The innovations are considered to have a normal distribution, although they follow the same pattern as the designs. To eliminate the effect of starting values, the first 500 observations in each series are discarded. -->

<!-- It is often reasonable to construct a time series using properties such as trend, seasonality, and auto-correlation. However, when examining distributions across categories of cyclic granularities, these time series features are lost or addressed independently by considering seasonal fluctuations through cyclic granularities. Because the time span during which an entity is observed in order to ascertain its behavior is not very long, -->
<!-- the behavior of the entity will not change drastically and hence the time series can be assumed to remain stationary throughout the observation period. If the observation period is very long (for e.g more than 3 years), property, physical or geographical attributes might change leading to a non-stationary time series. But such a scenario is not considered here and the resulting clusters are assumed to be time invariant in the observation period.  -->


### Data designs

#### Individual granularities

**Scenario 1 (S1) - All granularities significant:**
<!-- Consider a case where all the three granularities $g1$, $g2$ and $g3$ would be responsible for making the designs distinct. That would mean, the pattern for each of $g1$, $g2$ and $g3$ will change for at least one design. We consider a situation with all the null cases corresponding to no difference in distribution across categories, that is, all categories follow the same distribution N(0,1). -->
Consider the instance where $g1$, $g2$, and $g3$ all contribute to design distinction. This means that each granularity will have significantly different patterns at least across one of the designs to be clustered. In Table \ref{tab:tab-dist-design} various distributions across categories are considered (top) which lead to different designs (bottom). Figure \ref{fig:plot-3gran-new} shows the simulated variable's linear (left) and cyclic (right) representations for each of these five designs. The structural difference in the time series variable is impossible to discern from the linear view, with all of them looking very similar. The shift in structure may be seen clearly in the distribution of cyclic granularities. The following scenarios use solely graphical displays across cyclic granularities to highlight distributional differences in categories.

**Scenario 2 (S2) - Few significant granularities:**
This is the case where one granularity will remain the same across all designs. We consider the case where the distribution of $v$ varies across $g2$ levels for all designs, across $g3$ levels for a few designs, and $g1$ does not vary across designs. The proposed design is shown in Figure \ref{fig:gran2and1-clubbed}(b).

**Scenario 3 (S3) - Only one significant granularity:**
Only one granularity is responsible for identifying the designs in this case. This is depicted in Figure \ref{fig:gran2and1-clubbed} (right) where only $g3$ affects the designs significantly.

```{r tab-distribution}
```


```{r tab-design}
```


```{r tab-dist-design}
```



```{r generate-D3change}

```

```{r generate-design-3change}

```



```{r plot-3gran-new, out.width="100%", fig.cap = "The linear (left) and cyclic (right) representation is shown under scenario S1 using line plots and boxplots respectively. Each row represents a design. Distributions of categories across $g1$, $g2$ and $g3$ change across at least one design as can be observed in the cyclic representation. It is not possible to comprehend these structural differences in patterns just by looking at or considering the linear representation."}

```


```{r generate-design-new}

```


```{r generate-design-2gran-data}

```


```{r generate-design-2gran-plot}

```


```{r generate-design-1gran-data}

```


```{r generate-design-1gran-plot}

```


```{r gran2and1-clubbed, fig.cap=" Boxplots showing distributions of categories across different designs (rows) and granularities (columns) for scenarios S2 and S3. In S2, $g2$, $g3$ change across at least one design but $g1$ remains constant. Only $g3$ changes across different designs in S3."}
#gran2_change  + gran1_change + plot_layout(widths = c(1,1))
#ggpubr::ggarrange(gran1_change, gran2_change, labels = c("a", "b"))
```

#### Interaction of granularities

The proposed methods could be extended when two granularities of interest interact and we want to group subjects based on the interaction of the two granularities. Consider a group that has a different weekday and weekend behavior in the summer but not in the winter. This type of combined behavior across granularities can be discovered by evaluating the distribution across combinations of categories for different interacting granularities (weekend/weekday and month-of-year in this example). As a result, in this scenario, we analyze a combination of categories generated from different distributions. Display of design and related results can be found in supplementary material.




<!-- When two granularities of interest interact, the connection between a granularity and the measured variable is determined by the value of the other interacting granularity. This happens when the effects of the two granularities on the measured variable are not additive. For simplicity, consider a case with just two interacting granularities $g1$ and $g2$ of interest. As opposed to the last case, where we could play with the distribution of $5$ individual categories, with interaction we can play with the distribution of $6$ combination of categories. Consider $4$ designs in Figure \ref{fig:} where different distributions are assumed for different designs to get some distinction across designs. For example, in application, think about the scenario when customers need to grouped basis their joint behavior across hour-of-day and month-of-year. -->

<!-- | Granularity type                                                   	| # Significant 	| # Replications 	| -->
<!-- |--------------------------------------------------------------------	|---------------	|----------------	| -->
<!-- | **Individual**  <br><br># obs: 300, 500, 2000  <br># clusters: 4/5 	| 1/2/3         	| 25, 100, 200   	| -->
<!-- | **Interaction**  <br><br># obs: 500, 2000  <br># clusters: 4       	| 2           	| 25, 100, 200   	| -->


### Visual exploration of results

All of the approaches were fitted to each data design and to each combination of the considered parameters. The formed clusters have to match the design, be well separated, and have minimal intra-cluster variation. <!--It is possible to study these desired clustering traits visually in a more comprehensive way than just looking at index values.--> MDS and parallel coordinate graphs are used to demonstrate the findings, as well as an index value plot to provide direction on the number of clusters. JS-based approaches corresponding to NQT and RS are referred to as JS-NQT and JS-RS respectively. In the following plots, results for JS-NQT are reported, and results with JS-RS or wpd-based distances are in the supplementary material.

Figure \ref{fig:sindex-plot-validation} shows *sindex* plotted against the number of clusters ($k$) for the range of mean differences (rows) under the different scenarios (columns). This can be used to determine the number of clusters for each scenario. When *sindex* for each scenario are examined, it appears that $k = \{5, 4, 4\}$ is justified for scenarios S1, S2, and S3, respectively, given the sharp decrease in *sindex* from that value of $k$. Thus, the number of clusters corresponds to the number of designs that were originally considered in each scenario.

Figure \ref{fig:mds-plot-validation} shows separation of our clusters. It can be observed that in all scenarios and for different mean differences, clusters are separated. However, the separation increases with an increase in mean differences across scenarios. This is intuitive because, as the difference between categories increases, it gets easier for the methods to correctly distinguish the designs.

Figure \ref{fig:parcoord-sim} depicts a parallel coordinate plot with the vertical bar showing total inter-cluster distances with regard to granularities $g1$, $g2$, and $g3$ for all simulation settings and scenarios. So one line in the figure shows the inter-cluster distances for one simulation setting and scenarios vary across facets. The lines are not colored by group since the purpose is to highlight the contribution of the factors to categorization rather than class separation. Panel S1 shows that no variable stands out in the clustering, but the following two panels show that {g1} and {g1, g2} have very low inter-cluster distances, meaning that they did not contribute to the clustering. It is worth noting that these facts correspond to our original assumptions when developing the scenarios, which incorporate distributional differences over three (S1), two (S2), and one (S3) significant granularities. Hence, Figure \ref{fig:parcoord-sim} (S1), (S2), and (S3) validate the construction of scenarios (S1), (S2), and (S3) respectively.

The JS-RS and wpd-based methods perform worse for $nT=300$, then improve for higher $nT$ evaluated in the study. However, a complete year of data is the minimum requirement to capture distributional differences in winter and summer profiles, for example. Even if the data is only available for a month, $nT$ with half-hourly data is expected to be at least $1000$. As a result, as long as the performance is promising for higher $nT$, this is not a challenge.

In our study sample, the method JS-NQT outperforms the method JS-RS for smaller differences between categories. More testing, however, would be needed to to be confident in this conclusion.


```{r sindex-data-validation}

```


```{r sindex-plot-validation, fig.cap = "Choosing optimal cluster number across the range of scenarios and mean differences used in the validation study, using the cluster separation index (sindex) for the JS-NQT. S1 has a sharp decrease in sindex from 5 to 6, whereas S2 and S3 have a decrease from 4 to 5, especially when mean difference is large, providing the recommended number of clusters to be 5, 4, 4, respectively. This precisely reflects the structure in designs that we would hope the clustering could recover."}
```


```{r mds-data-validation}

```


```{r mds-plot-validation, fig.cap = "MDS summary plots to illustrate the cluster separation for the range of mean differences (rows) under the different scenarios (columns). It can be observed that clusters become more compact and separated for higher mean differences between categories across all scenarios. Between scenarios, separation is least prominent corresponding to Scenario (S3) where only one granularity is responsible for distinguishing the clusters."}

```


```{r parcoord-sim, fig.cap = "Exploring the contribution of granularities in the clustering for scenarios S1, S2, S3, using parallel coordinate plots. Inter-cluster distances are displayed vertically. All three granularities $g1$, $g2$, and $g3$ have high inter-cluster distances for S1, suggesting all are important. In S2 $g1$ and in S3 both $g1$ and $g2$ have smaller inter-cluster distances, indicating that they did not contribute to clustering."}

```






<!-- "Parallel coordinate plots are used to identify key variables for classification. Each line in this figure represents a formed group for a certain set of simulated parameters. (a), (b) and (c) corresponds to Scenarios (a), (b) and (c). The cyclic granularities $g1$, $g2$, and $g3$ are plotted on the x-axis, while total inter-cluster distances are plotted on the y-axis. Each line in this figure represents a group for a certain configuration of the various parameters ($nT$, $diff$, $R$). (c) demonstrates that intercluster distances are only significant for the variable $g3$, meaning that the clusters formed are primarily due to $g3$. This corresponds to our design in Scenario (S3), in which distributional differences were implemented solely for g3. (b) demonstrates the role of g2 and g3 in clustering. (a) depicts a heterogeneous pattern of inter-cluster distances across variables, indicating that no single variable is to account for the clusters." -->

<!-- A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be explored. -->


```{r, eval=FALSE}
# code in append_3gran_change.R
table <- read_rds("data/append_3gran_change.rds")
table %>% kable()
```


<!-- starts getting better with increasing difference and get worse with increasing number of replications. Length of series do not show to have any effect on the performance of the methods. It does not depend on if time series is ar or arma. -->


<!-- - confusion matrix could be used for showing results if proper labeling is used -->

<!-- - write about features that we have spiked into the data set -->
<!-- - write about you incorporated noise -->
<!-- - What is the additional structure you can incorporate that will lead to failing of method1 and method2? -->
<!-- - And both gives the same result? Basically say when method 1 works better than method 2 and vice versa! -->

<!-- - -->

## Application {#sec:application-gracsr}

Clustering with the new distances is illustrated on the smart meter energy usage for a sample of customers from @smart-meter. The full data contains half-hourly general supply in kWh for 13,735 customers, resulting in 344,518,791 observations in total.<!--In most cases, electricity data is expected to have multiple seasonal patterns like daily, weekly or annual. We do not learn about these repetitive behaviors from the linear view because too many measurements all squeezed in that representation. Hence we transition into looking at cyclic granularities, that can potentially provide more insight on their repetitive behavior.--> The raw data for these consumers is of unequal length, with varying starting and end dates. Additionally, there were missing values in many series. (The supplementary material contains details from checking for systematic missingness.) Because our proposed methods evaluate probability distributions rather than raw data, these data issues are not problematic, unless there is any systematic structure related to granularities.

Huge data sets present more complications for clustering. Clustering algorithms work well when there are well-separated clusters, with no nuisance variables or nuisance observations. When converting a series to granularities, many variables (each level of a granularity) are generated, possibly creating a slew of nuisance variables. Some customers may have a mix of energy use patterns, which could be considered nuisance observations located between major clusters. For this reason, we have chosen to select a small group of customers with relatively distinct and different patterns in order to illustrate the clustering more simply. Figure \ref{fig:hod-ind-group-png} shows the distribution across *hod*, *moy* and *wnwd* for the set of $24$ customers used to illustrate clustering. The customers are displayed in two columns of $12$ for space reasons. Each row, of each column, represents the profile of a single customer across different variables. Each customer is associated with an identifier of the form [$a$-$b$], where $a \in \{1, 2, \dots, 24\}$ represents the customer-prototype id and $b \in \{1, 2, \dots, 5\}$ indicates the label of the prototype in which a customer was placed. This is often a good approach to tackling a big analysis task, to start with a simpler task. The approach, however, is applicable to all customers.

As a result, we dissect the larger problem and test our solutions on a small sample of prototype customers. To do this, data is first filtered to generate a small sample, and then significant cyclic granularities (variables) for them are chosen (as described in Section \ref{sec:datafilter}). The sample set is subsequently examined along all dimensions of interest, to ensure that they reveal some patterns across at least one specified variable (as described in Section \ref{sec:prototype}). <!--By grouping the prototypes using our methods in Section \ref{sec:clustering} and assessing their meaning, the study hopes to unravel some of the heterogeneities observed in energy usage data.--> Because the data does not contain additional customer characteristics, we cannot explain why consumption varies, but can only identify how it varies. 


### Data filtering and variable selection {#sec:datafilter}

The steps for customer filtering and variable selection were:

1. Choose a smaller subset of randomly selected $600$ customers with no implicit missing values for 2013.
2. Obtain $\wpd$ for all cyclic granularities considered for these customers. It was found that *hod* (hour-of-day), *moy* (month-of-year) and *wnwd* (weekend/weekday) are significant for most customers. We use these three granularities while clustering.
3. Remove customers whose data for an entire category of *hod*, *moy* or *wnwd* is empty. For example, a customer who does not have data for an entire month is excluded because their monthly behavior cannot be analyzed.
4. Remove customers whose energy consumption is 0 in all deciles. These are the clients whose consumption is likely to remain essentially flat and with no intriguing repeated patterns that we are interested in studying.


### Selecting prototypes {#sec:prototype}

<!-- Why instance selection -->


It is common to filter data prior to fitting a supervised classification model using instance selection [@olvera2010review] which removes observations that might impede the model building. For clustering, this is analogous to identifying and removing nuisance observations. Prototype selection is more severe than instance selection, because only a handful of cases is selected. @Cutler1994-hm proposed a method called archetypal analysis which has inspired this approach but the procedure we have used follows @Fan2021-bq. First, dimension reduction such as t-SNE, MDS or PCA is used to project the data into a 2D space. Second, a few "anchor" customers far apart in 2D space are selected. Additional close neighbors to the anchors are selected. To check the selections relative to the full set of variables, we used a tour linked to a t-SNE layout using the R package `liminal` [@R-liminal]. This ensured that the final sample of clustered customers were also far apart in the high-dimensional space. (See the supplementary materials for further details.)<!--Few of these customers have similar distribution across *moy* and some are similar in their *hod* distribution. -->


```{r prototype-data-pick}

```

```{r assemble}

```

```{r data-pick}
```


<!-- - can be done in several ways -->

<!-- - show raw data of moy, hod, wkndwday patterns for them -->


<!-- # ```{r hod-moy-wkndwday plots, fig.cap="The distribution across moy, hod and wkndwday for the selected designs. Few are similar in their hod pattern, while others are similar in *moy* behavior. Some customers have distinct behavior as compared to all other customers.For example, although patterns across wkndwday do not look distinctly different for most customers, there is one household for whom weekend behavior is standing out from the rest."} -->
<!-- # ``` -->



### Clustering results {#sec:clustering}

Clustering of the $24$ prototypes <!--and the data subset consisting of $353$ customers--> was conducted with all three distances, JS-NQT, JS-RS and WPD, and is summarized in Figures \ref{fig:opt-cluster-tsne-jsd}, \ref{fig:groups-4and5} and \ref{fig:summary-plot-wpd}. The t-SNE visualization suggests that there are four well-separated clusters. It is possible that because the representation is only 2D, the fifth group from the original prototype selection is distinctly different in high dimensions. The *sindex* plots for the three methods indicate some disagreement: JS-NQT suggests $3$, JS-RS suggests $2$ or $5$ and WPD suggests $3$ or $5$. JS-RS would appear to match the original prototypes with the five cluster solution, but it actually differs. Even though the *sindex* for JS-NQT suggests three clusters, the five cluster solution more closely matches the original prototypes. The WPD clustering provides a different grouping of the customers, and even though it disagrees with the original prototypes it is a useful grouping.<!--While plotting the clusters, RS-based scaling is applied to each customer, so that no customer dominates the shape of the summarized clusters because of magnitude. The plotting scales are not displayed since we want to emphasize comparable patterns rather than scales. The idea is that a customer in a cluster may have low total energy usage, but their behavior may be quite similar to a customer with high usage with respect to distributional pattern or significance across cyclic granularities.-->

<!--### JS-NQT and JS-RS-->

<!--Figure \ref{fig:opt-cluster-tsne-jsd} The *sindex* is plotted against the number of clusters for clustering based on JS-based (JS-NQT) distances in Figure \ref{fig:opt-cluster-tsne-jsd}. The *sindex* falls from around $0.56$ when $k=3$ to $0.54$ when $k$ increases. In our study, we use $k=4, 5$.--> Figure \ref{fig:groups-4and5} displays the summarized distributions across $4$ and $5$ clusters from JS-NQT clustering in (a) and (b) respectively, and helps to characterize each cluster. In the quantile plots the line represents the median, and the region shows the area between the $25^{th}$ and $75^{th}$ percentiles. The only difference between the four and five cluster solution is that A-4 divides further into B-4 and B-5. This additional division makes a clearer clustering, because it resolves the heterogeneity in *moy* creating a group (B-5, customers 1-3) which has a winter peak in usage, and a group (B-4, customers 16-20) which has a start of the year peak in usage. B-2 (customers 4-9) and B-1 (customers 21-24) have distinctive *hod* patterns but are both heterogeneous in *moy* and *wnwd*. B-3 (customers 10-15) has peak usage at the end of the year, but is heterogeneous on *hod* and *wnwd*. This clustering almost agrees with the clusters visible in the t-SNE plot.

<!-- A-4 is subdivided into B-4 and B-5, each of which has a distinct shape across *moy* and *wnwd*. B-4 (id: 16-20) is distinguished by higher energy consumption in the first few months of the year and greater variability in weekend usage. B-5 (id: 1-3) has higher consumption in the middle of the year (winter months) and a similar weekday-weekend inter-quartile range. When $k = 4$ is used, these two groups merge to form A-4, which has a *moy* profile of higher usage in both the beginning and middle of the year, which is not representative of the individuals. It may be worthwhile to compare Figures \ref{fig:opt-cluster-tsne-jsd} and \ref{fig:hod-ind-group-png} to see if the summarized distributions across groups accurately characterized the groupings. If it has, then the majority of the members of the group should have a similar profile.-->


<!-- Our methodology is useful for grouping similar distributions over *hod* and *moy* and they are placed closely for easy comparison. Few groups have mixed patterns across *hod* and *moy*, but few have all customers in the group having a similar profile.-->

<!--  It appears that the aim of grouping comparable distributions across categories of multiple temporal dependencies has been accomplished to some extent. -->

```{r all-data}
```


```{r clustering}

```


```{r groups-24}
```


```{r data-pick-robust}

```


```{r hod-moy-wkndwday-data}

```



```{r opt-clusters-jsd}

```

```{r data-pick-wpd}
```

```{r opt_clusters-wpd}
```


```{r summary-group-wpd}
```



```{r tsne-fit}
```


```{r tsne-df}
```


```{r tsne-xy}

```


```{r all-data-rs}

```


```{r clustering-rs}

```


```{r opt-clusters-jsd-rs}

```


```{r opt-clusters-all}

```



```{r hod-ind-group}
```


```{r hod-ind-group-png, fig.cap = "The distribution of electricity demand across individual customers over three granularities *hod*, *moy*, and *wnwd* are shown for the $24$ selected customers using quantile and box plots. They are split into batches of $12$ in (a) and (b), with each row in (a) or (b) representing a customer. The number indicates a unique customer id and a prototype id."}
```



```{r opt-cluster-tsne-jsd, fig.cap = "Clustering summaries: (a) t-SNE computed on the $24$ selected customers, and (b) separation index (*sindex*) for $2$-$10$ clusters using JS-NQT, JS-RS and WPD. Various choices in number of clusters would be recommended. Four clusters are visible in t-SNE, although it might hide a fifth cluster because dimension reduction to 2D may be insufficient to see the difference. JS-NQT suggests $3$, JS-RS suggests $2$ or $5$ and WPD suggests $3$ or $5$."}
#include_graphics("img/opt_clust_parenthesis.png")
```

<!-- It appears that the aim of grouping comparable distributions over considered variables has been accomplished to some extent. -->



```{r data-heatmap-hod-group-new}
```

```{r data-heatmap-moy-group-new}

```


```{r data-heatmap-wkndwday-group}

```



```{r combined-groups-js, fig.cap = "The distribution of electricity demand across clusters over *hod* (a), *moy* (b), and *wnwd* (c) are shown through quantile plots. Group 1 (customers 1-3), Group 2 (customers 4-9), Group 3 (customers 10-15), Group 4 (customers 16-20), Group 5 (customers 21-24). Groups 2 and 5 appear to have a *hod* pattern among its members (morning and evening peak), whereas groups 1, 3, and 5 appear to have a *moy* pattern (higher usage in mid-year). With their conditional dependencies on *hod* or *moy* categories, *wnwd* differences may not be discernible on their own. (c) demonstrates that the distribution of *hod* conditional on *wnwd* is also not different, except for the tails. This might be because most of these chosen customers do not have a *wnwd* pattern. It is useful to compare the raw distributions with the summarized distributions to establish that the majority of individuals in the group share the same characteristics. To facilitate this comparison, the individual distributions are sorted by their JS-based groupings.", eval = FALSE}

```


<!-- The widest within-cluster gap (widest_gap) is high till $k=5$ and then have a gradual decrease. Number of -->

```{r opt-clusters, fig.cap=" Different cluster statistics plotted as a function of number of clusters for JS-based (a) and wpd-based (b) approaches. avg within is not too useful for revealing any sharp change in average distance between clusters with increasing number of clusters. For (a), sindex sees a sharp fall at $k = 4$ and then do not decrease until $k=16$ and the widest gap is high till $k=5$. For (b), sindex sees a fall at $k=3$ and then at $k=7$ and widest gap has a sharp fall at $k=6$.", eval = FALSE}
#knitr::include_graphics("img/opt_clusters.png")
```

<!-- GroupA and Group B may represent different set of customers as they are groupings based on different number of clusters. -->

```{r groups-4and5, fig.cap = "Summary plots for four and five clusters from JS-NQT showing the distribution of electricity demand combined for all members over *hod*, *moy*, and *wnwd*. Groups A-1 (customers 21-24), A-2 (customers 4-9), and A-3 (customers 10-15) profiles correspond to Groups B-1, B-2, and B-3, respectively. Cluster A-4 splits into B-4 (customers 16-20) and B-5 (customers 1-3) to produce the five clusters, which better resolves the *moy* distribution.", fig.height=5}

```

(ref:summary-plot-wpd) Summary plots for three (a) and five (b) clusters from WPD approach showing the $\wpd$ values of each customers across *hod*, *moy*, and *wnwd* through a parallel coordinate plot. P-1 and P-2 groups correspond to Q-1 and Q-2, respectively. Cluster P-3 is subdivided into Q-3, Q-4, and Q-5. P-1 (customers 16, 18) is distinguished by high $\wpd$ on *wnwd* values. P-2 has lower $\wpd$ than *moy* and *wnwd* for *hod*. P-3 operates in the opposite way as P-2, with larger $\wpd$ for *hod* in comparison to *moy* and *wnwd*. For the $5$ cluster solution, this group is divided into Q-3, Q-4, Q-5, which are distinct due to their different relative significance of *moy* and *wnwd*.

```{r summary-plot-wpd, fig.cap = "(ref:summary-plot-wpd)", fig.height=6}
```

```{r summary-table-wpd}

```


<!--### WPD-->

<!-- In Figure \ref{fig:opt-cluster-tsne-jsd}, the *sindex* is plotted against the number of clusters for clustering based on wpd-based distances. The scales are very different from those used in JS-based clustering. The *sindex* falls from around $20$ when $k=3$ to $19$ when $k=4$ and further decreases to $17.5$ with $k>7$. Thus, there are few choices available for the number of clusters. Figure \ref{fig:summary-plot-wpd} whos the wpd values for the three different granularities of the 24 customers, coloured by the 6 cluster solution. XXX REWORK THIS WHEN NEW CLUSTER RESULTS ARE SHOWN<!--The groupings with $k=4$ is shown through a parallel coordinate plot with the three significant cyclic granularities. The variables are sorted according to their separation across classes (rather than their overall variation between classes). This means that *moy* is the most important variable in distinguishing the groups, followed by *hod* and *wnwd*. The two customers (id: 16 and 18) that have significant differences between *wnwd* categories (in Figure \ref{fig:hod-ind-group-png}) stand out from the rest of the customers. Group-4 has a higher $\wpd$ for *hod* than *moy* or *wnwd*. Group-2 and Group-3 have the most distinct patterns across *moy*. Group-1 is a mixed group that has strong patterns on at least one of the three variables. When $k=3$ is used, Group-2 and Group-3 are merged because their patterns relative to *moy*, *hod* and *wnwd* are the same. The findings vary from JS-based clustering, yet it is a helpful grouping.-->

Figure \ref{fig:summary-plot-wpd} shows the $\wpd$ values of the $24$ customers over *hod*, *moy*, and *wnwd* , colored by $3$ (a) and $5$ (b) clusters from WPD clustering using a parallel coordinate plot. The variables ($\wpd$ for different granularities) are 
standardized prior to clustering using WPD. In the display, the variables are sorted according to their separation across groups. This means that *wnwd* is the most important variable in distinguishing the groups, followed by *hod* and *moy* for both (a) and (b).
Groups P-1 and P-2 correspond to Q-1 and Q-2 respectively. Cluster P-3 splits into Q-3, Q-4 and Q-5. Customers 16 and 18 are characterized by unusual high values of $\wpd$ on *wnwd* compared to the rest of the customers and hence form a group. This could again be verified from Figure \ref{fig:hod-ind-group-png}, where these were the only two customers with a difference in their *wnwd* behavior. They are represented by P-1. P-2 has lower $\wpd$ for *hod* than *moy* and *wnwd*. P-3 behaves opposite to P-2 with higher $\wpd$ for *hod* compared to *moy* and *wnwd*. These are the customers who have some significant pattern across *hod* and this can again be validated by looking at Figure \ref{fig:hod-ind-group-png}. For a $5$ cluster solution, this group gets split into Q-3, Q-4 and Q-5 characterizing different relative significance of *moy* and *wnwd*. For example, Q-4 and Q-5 have almost no pattern across *moy*, but Q-3 has a *moy* pattern and thus it is reasonable to split them. The patterns could be different, but they are significant. Q-4 and Q-5 are separated because of their different significance of *hod*. All of these can also be verified from Table \ref{tab:summary-table-wpd} which shows the cluster summaries with members and median values of $\wpd$ for the three variables. 

<!-- *hod* has very low median values of $\wpd$ for the members in the cluster, implying at least half of these customers do not have an interesting *hod* pattern. -->


```{r wpd-clustering}

```


<!-- # ```{r summary-plot-wpd, fig.cap="Summary of the four cluster results from WPD, shown as a parallel coordinate plot and table. XXX TO BE REVISED WHEN DIFFERENT CLUSTER SOLUTION IS SHOWN", message=FALSE, warning=FALSE, fig.height=6} -->
<!-- # #include_graphics("img/summary-plot-wpd.png") -->
<!-- # ``` -->

In summary, none of the methods captured the five original prototypes exactly. JS-NQT was almost identical, but WPD produced quite a different grouping. This is quite a reasonable result and illustrates both the difficulties of clustering to obtain a particularly expected grouping and the ability to learn unexpected patterns in the data. It is possible that the JS-based distances were distracted by the presence of nuisance variables, levels of the granularities that do not contribute to clustering. This would also be supported by the results of the validation study, where clustering was less effective in S2 and S3, where only some granularities had differences between levels. Clustering using WPD is expected to produce quite different results because it will group only by overall value of a granularity, not a particular pattern. A cluster summary like Figure \ref{fig:groups-4and5} is not possible because there may be different but equally interesting patterns (e.g. high evening *hod* and high daytime *hod*) in the same cluster. Simply it provides information that across a collection of customers this specific cluster has interesting patterns in a granularity (e.g. *hod*). One would need to post-process the cluster to separate specific patterns.  

<!--Things become far more complicated when we consider a larger data set with more uncertainty, as they do with any clustering problem. Summarizing distributions across clusters with varied or outlying customers can result in a shape that does not represent the group. Furthermore, combining heterogeneous customers may result in similar-looking final clusters that are not effective for visually differentiating them. It is also worth noting that the weekend/weekday behavior in the given case does not characterize most selected customers. This, however, will not be true for all of the customers in the data set. If more extensive prototype selection is used, resulting in more comprehensive prototypes in the data set, this method might be used to classify the entire data set into these prototype behaviors. However, the goal of this section was to have a few customers that have significant patterns over one or more cyclic granularities, apply our methodology to cluster them, and demonstrate that the method produces useful clusters.-->



## Discussion {#sec:discussion-gracsr}

We offer two approaches for calculating pairwise distances between time series based on probability distributions over multiple cyclic granularities at once. Depending on the goal of the clustering, these distance metrics, when fed into a hierarchical clustering algorithm using Ward's linkage, yield meaningful clusters. Probability distributions provide an intuitive method to characterize noisy, patchy, long, and unequal-length time series data. Distributions over cyclic granularities help to characterize the formed clusters in terms of their repeating behavior over these cyclic granularities. Furthermore, unlike earlier efforts that group customers based on behavior across only one cyclic granularity (such as hour-of-day), our method is more comprehensive in detecting clusters with repeated patterns at all relevant granularities. <!--The JS-based approaches are faster than wpd-based approaches.-->

There are a few areas to extend this research. First, larger data sets with more uncertainty complicate matters, as is true for any clustering task. Characterizing clusters with varied or outlying customers can result in a shape that does not represent the group. Moreover, integrating heterogeneous consumers may result in visually identical end clusters, which are potentially not useful. Hence, a way of appropriately scaling it up to many customers such that anomalies are removed before clustering would be useful for bringing forth meaningful, compact and separated clusters. Secondly, we have assumed the time series to be stationary, and hence the distributions are assumed to remain constant for the observation period. In reality, however, it might change. For the smart meter example, the distribution for a customer moving to a different house or changing electrical equipment can change drastically. Our current approach can not detect these dynamic changes. Thirdly, it is possible that for a few customers, data for some categories from the list of considered significant granularities are missing. In our application, we have removed those customers and done the analysis but the metrics used should be able to incorporate those customers with such structured missingness. Finally, $\wpd$ is computationally heavy even under parallel computation. Future work can make the computations more efficient so that they are easily scalable to a large number of customers. Moreover, experiments can also be run with non-hierarchy based clustering algorithms to verify if these distances work better with other algorithms.


## Acknowledgments {-}

The authors thank the ARC Centre of Excellence for Mathematical and Statistical Frontiers [(ACEMS)](https://acems.org.au/home) for supporting this research. Sayani Gupta was partially funded by [Data61 CSIRO](https://data61.csiro.au/) during her PhD. The Monash eResearch Centre and eSolutions-Study Support Services supported this research in part through the resource usage of the MonARCH HPC Cluster. The Github repository, [github.com/Sayani07/paper-gracsr](https://github.com/Sayani07/paper-gracsr), contains all materials required to reproduce this article and the code is also available online in the supplementary materials. This article was created with R [@R-language], `knitr` [@knitr; @R-knitr] and `rmarkdown` [@rmarkdown; @R-rmarkdown]. Graphics are produced with `ggplot2` [@Wickham2009pk] and `GGally` [@R-GGally].

## Supplementary materials {- #supplement-gracsr}

**Data and scripts:** Data sets and R codes to reproduce all figures in this article  
(validation.R, application.R)

\noindent
**Supplementary paper:** Additional graphics and R codes to reproduce it  
(paper-supplementary.pdf, paper-supplementary.Rmd, supplementary.R)

\noindent
**R-package:** To implement the ideas provided in this research, the open-source R package `gracsr` is available on Github (https://github.com/Sayani07/gracsr).
