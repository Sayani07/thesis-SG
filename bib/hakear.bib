@ARTICLE{Gupta2021-gravitas,
   title     = "Visualizing probability distributions across bivariate cyclic
                temporal granularities",
   author    = "Gupta, Sayani and Hyndman, Rob J and Cook, Dianne and Unwin,
                Antony",
   abstract  = "AbstractDeconstructing a time index into time granularities can
                assist in exploration and automated analysis of large temporal
                data sets. This paper describes classes of time deconstructions
                using linear and cyclic time granularities. Linear granularities
                respect the linear progression of time such as hours, days,
                weeks and months. Cyclic granularities can be circular such as
                hour-of-the-day, quasi-circular such as day-of-the-month, and
                aperiodic such as public holidays. The hierarchical structure of
                granularities creates a nested ordering: hour-of-the-day and
                second-of-the-minute are single-order-up. Hour-of-the-week is
                multiple-order-up, because it passes over day-of-the-week.
                Methods are provided for creating all possible granularities for
                a time index. A recommendation algorithm provides an indication
                whether a pair of granularities can be meaningfully examined
                together (a ?harmony?), or when they cannot (a ?clash?). Time
                granularities can be used to create data visualizations to
                explore for periodicities, associations and anomalies. The
                granularities form categorical variables (ordered or unordered)
                which induce groupings of the observations. Assuming a numeric
                response variable, the resulting graphics are then displays of
                distributions compared across combinations of categorical
                variables.The methods implemented in the open source R package
                gravitas are consistent with a tidy workflow, with probability
                distributions examined using the range of graphics available in
                ggplot2.",
   journal   = "Journal of Computational \& Graphical Statistics",
   year      =  2021,
   note = {to appear}
 } 

@Book{Wickham2009pk,
   author = {Hadley Wickham},
   title = {ggplot2: Elegant Graphics for Data Analysis},
   publisher = {Springer-Verlag New York},
   year = {2016},
   url = {http://ggplot2.org},
   doi = {10.1007/978-3-319-24277-4}
 }

@book{aigner2011visualization,
  title={Visualization of time-oriented data},
  author={Aigner, Wolfgang and Miksch, Silvia and Schumann, Heidrun and Tominski, Christian},
  year={2011},
  publisher={Springer Science \& Business Media},
  doi = {10.1007/978-0-85729-079-3}

}

@article{wang2020calendar,
  author = {Earo Wang and Dianne Cook and Rob J Hyndman},
  title = {Calendar-based graphics for visualizing people's daily schedules},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {3},
  pages = {490-502},
  year  = {2020},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2020.1715226}
}

@INCOLLECTION{Bettini1998-ed,
  title     = "A glossary of time granularity concepts",
  booktitle = "Temporal Databases: Research and Practice",
  author    = "Bettini, Claudio and Dyreson, Curtis E and Evans, William S and
               Snodgrass, Richard T and Wang, X Sean",
  editor    = "Etzion, Opher and Jajodia, Sushil and Sripada, Suryanarayana",
  publisher = "Springer",
  pages     = "406--413",
  year      =  1998,
  address   = "Berlin, Heidelberg",
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1007/BFb0053711}
}

@ARTICLE{Grolemund2011-vm,
    title = {Dates and Times Made Easy with {lubridate}},
    author = {Garrett Grolemund and Hadley Wickham},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {40},
    number = {3},
    pages = {1--25},
    url = {http://www.jstatsoft.org/v40/i03/},
   doi = {10.18637/jss.v040.i03}
}


@inproceedings{Goodwin_2012,
  doi = {10.1109/vast.2012.6400545},
  year = 2012,
  month = {oct},
  publisher = {{IEEE}},
  author = {Sarah Goodwin and Jason Dykes},
  title = {Visualising variations in household energy consumption},
	booktitle = {2012 {IEEE} Conference on Visual Analytics Science and Technology ({VAST})}
}


@article{Dyreson_2000,
	doi = {10.1109/69.868908},
	year = 2000,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {12},
	number = {4},
	pages = {568--587},
	author = {C.E. Dyreson and W.S. Evans and H. Lin and R.T. Snodgrass},
	title = {Efficiently supporting temporal granularities},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering}
}

@ARTICLE{Ning_2002,
   title    = "An Algebraic Representation of Calendars",
   author   = "Ning, Peng and Wang, Xiaoyang Sean and Jajodia, Sushil",
   abstract = "This paper uses an algebraic approach to define temporal
               granularities and calendars. All the granularities in a calendar
               are expressed as algebraic expressions based on a single
               ``bottom'' granularity. The operations used in the algebra
               directly reflect the ways with which people construct new
               granularities from existing ones, and hence yield more natural
               and compact granularities definitions. Calendar is formalized on
               the basis of the algebraic operations, and properties of
               calendars are studied. As a step towards practical applications,
               the paper also presents algorithms for granule conversions
               between granularities in a calendar.",
    journal = {Annals of Mathematics and Artificial Intelligence},
   volume   =  36,
   number   =  1,
   pages    = "5--38",
   month    =  sep,
   year     =  2002,
   doi = {10.1023/A:1015835418881}
 }

@article{wang2020tsibble,
  author = {Earo Wang and Dianne Cook and Rob J Hyndman},
  title = {A new tidy data structure to support exploration and modeling of temporal data},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {3},
  pages = {466-478},
  year  = {2020},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2019.1695624}
}

@BOOK{Reingold2001-kf,
   title     = "Calendrical Calculations",
   edition = "Millennium Edition",
   author    = "Reingold, Edward M and Dershowitz, Nachum",
   abstract  = "This new edition of the successful calendars book is being
                published at the turn of the millennium and expands the
                treatment of the previous edition to new calendars and variants.
                As interest grows in the impact of seemingly arbitrary
                calendrical systems upon our daily lives, this book frames the
                world in a completely algorithmic form. The book gives a
                description of twenty-five calendars and how they relate to one
                another: the Gregorian (current civil), ISO (International
                Organization for Standardization), Egyptian (and nearly
                identical Armenian), Julian (old civil), Coptic, Ethiopic,
                Islamic (Moslem), modern Persian (both astronomical and
                arithmetic forms), Baha'i (both present and future forms),
                Hebrew (Jewish), Mayan (long count, haab, and tzolkin), Balinese
                Pawukon, French Revolutionary (both astronomical and arithmetic
                forms), Chinese (and nearly identical Japanese), old Hindu
                (solar and lunisolar), and modern Hindu (solar and lunisolar).
                Easy conversion among these calendars is a by-product of the
                approach, as is the determination of secular and religious
                holidays. Calendrical Calculations makes accurate calendrical
                algorithms readily available for computer use with LISP,
                Mathematica, and Java code for all the algorithms included on
                the CD, and updates are available on the Web. This book will be
                a valuable resource for working programmers as well as a fount
                of useful algorithmic tools for computer scientists. In
                addition, the lay reader will find the historical setting and
                general calendar descriptions of great interest.",
   publisher = "Cambridge University Press",
   month     =  aug,
   year      =  2001,
   language  = "en",
   doi = {10.1017/9781107415058}
 }

@BOOK{Wilkinson1999-nk,
  title    = "The Grammar of Graphics",
  author   = "Wilkinson, Leland",
  year     =  1999,
  publisher = {Springer},
  address = {New York}
}

@ARTICLE{Mcgill1978-hg,
  title     = "Variations of Box Plots",
  author    = "Mcgill, Robert and Tukey, John W and Larsen, Wayne A",
  abstract  = "Abstract Box plots display batches of data. Five values from a
               set of data are conventionally used; the extremes, the upper and
               lower hinges (quartiles), and the median. Such plots are
               becoming a widely used tool in exploratory data analysis and in
               preparing visual summaries for statisticians and
               nonstatisticians alike. Three variants of the basic display,
               devised by the authors, are described. The first visually
               incorporates a measure of group size; the second incorporates an
               indication of rough significance of differences between medians;
               the third combines the features of the first two. These
               techniques are displayed by examples.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  32,
  number    =  1,
  pages     = "12--16",
  month     =  feb,
  year      =  1978,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1080/00031305.1978.10479236}
}


@ARTICLE{Hofmann2017-sg,
  title     = "{Letter-Value} Plots: Boxplots for Large Data",
  author    = "Hofmann, Heike and Wickham, Hadley and Kafadar, Karen",
  abstract  = "ABSTRACTBoxplots are useful displays that convey rough
               information about the distribution of a variable. Boxplots were
               designed to be drawn by hand and work best for small datasets,
               where detailed estimates of tail behavior beyond the quartiles
               may not be trustworthy. Larger datasets afford more precise
               estimates of tail behavior, but boxplots do not take advantage
               of this precision, instead presenting large numbers of extreme,
               though not unexpected, observations. Letter-value plots address
               this problem by including more detailed information about the
               tails using ?letter values,? an order statistic defined by
               Tukey. Boxplots display the first two letter values (the median
               and quartiles); letter-value plots display further letter values
               so far as they are reliable estimates of their corresponding
               quantiles. We illustrate letter-value plots with real data that
               demonstrate their usefulness for large datasets. All graphics
               are created using the R package lvplot, and code and data are
               available in the supplementary materials.",
  journal   = "Journal of Computational \& Graphical Statistics",
  publisher = "Taylor \& Francis",
  volume    =  26,
  number    =  3,
  pages     = "469--477",
  month     =  jul,
  year      =  2017,
  keywords  = "Boxplots;Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1080/10618600.2017.1305277}
}


@ARTICLE{Hintze1998-zi,
  title     = "Violin Plots: A Box {Plot-Density} Trace Synergism",
  author    = "Hintze, Jerry L and Nelson, Ray D",
  abstract  = "[Many modifications build on Tukey's original box plot. A
               proposed further adaptation, the violin plot, pools the best
               statistical features of alternative graphical representations of
               batches of data. It adds the information available from local
               density estimates to the basic summary statistics inherent in
               box plots. This marriage of summary statistics and density shape
               into a single plot provides a useful tool for data analysis and
               exploration.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  52,
  number    =  2,
  pages     = "181--184",
  year      =  1998,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.2307/2685478}
}

@ARTICLE{Hyndman1996-ft,
  title     = "Computing and Graphing Highest Density Regions",
  author    = "Hyndman, Rob J",
  abstract  = "Many statistical methods involve summarizing a probability
               distribution by a region of the sample space covering a
               specified probability. One method of selecting such a region is
               to require it to contain points of relatively high density.
               Highest density regions are particularly useful for displaying
               multimodal distributions and, in such cases, may consist of
               several disjoint subsets-one for each local mode. In this paper,
               I propose a simple method for computing a highest density region
               from any given (possibly multivariate) density f(x) that is
               bounded and continuous in x. Several examples of the use of
               highest density regions in statistical graphics are also given.
               A new form of boxplot is proposed based on highest density
               regions; versions in one and two dimensions are given. Highest
               density regions in higher dimensions are also discussed and
               plotted.",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  50,
  number    =  2,
  pages     = "120--126",
  year      =  1996,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.2307/2684423}
}









@MISC{Healey2017-tv,
  title        = "Perception in Visualization",
  booktitle    = "{NC} State University Computer Science",
  author       = "Healey, Christopher G",
  month        =  oct,
  year         =  2017,
  howpublished = "\url{https://www.csc2.ncsu.edu/faculty/healey/PP/index.html}",
  note         = "Accessed: 2018-12-20",
  keywords     = "Literature Review Chapter 1;Literature Review Chapter
                  1/Criterion for bad/good plots;Literature Review 01Chapter
                  SG/Criterion for bad/good plots"
}

@INPROCEEDINGS{Van_Wijk_undated-nj,
  title     = "The Value of Visualization",
  booktitle = "{IEEE} Visualization 2005 - ({VIS'05})",
  author    = "van Wijk, J J",
  keywords  = "Literature Review Chapter 1;Literature Review Chapter
               1/Criterion for bad/good plots;Literature Review 01Chapter
               SG/Criterion for bad/good plots"
}

@ARTICLE{Wainer1984-co,
  title     = "How to Display Data Badly",
  author    = "Wainer, Howard",
  abstract  = "[Methods for displaying data badly have been developing for many
               years, and a wide variety of interesting and inventive schemes
               have emerged. Presented here is a synthesis yielding the 12 most
               powerful techniques that seem to underlie many of the
               realizations found in practice. These 12 (the dirty dozen) are
               identified and illustrated.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  38,
  number    =  2,
  pages     = "137--147",
  year      =  1984,
  keywords  = "Literature Review Chapter 1;Literature Review Chapter
               1/Criterion for bad/good plots;Literature Review 01Chapter
               SG/Criterion for bad/good plots"
}

@BOOK{Playfair2005-ew,
  title     = "Playfair's Commercial and Political Atlas and Statistical
               Breviary",
  author    = "Playfair, William",
  abstract  = "A scientific revolution began at the end of the 18th century
               with the creation and popularization of the graphic display of
               data by Scottish inventor William Playfair, who introduced the
               line graph, bar chart, and pie chart into statistics. His
               remarkable Atlas demonstrated how much could be learned if one
               plotted data graphically and looked for suggestive patterns to
               provide evidence for pursuing research. In the Statistical
               Breviary, Playfair invented the pie chart and expanded upon this
               concept to facilitate the comparison of the resources of
               European countries. Playfair's work has great relevance to
               contemporary science, but finding copies of his original
               versions is very difficult. This re-issuance of two of his
               classic works, with new explanatory material, allows access to
               his wisdom for the first time in two centuries. In full color
               exactly as Playfair hand-colored the original, this volume
               includes exact duplicates of the third edition of his classic
               Atlas as well as the Statistical Breviary. An additional feature
               is the inclusion of annotations and an extensive biography of
               the remarkable inventor. Howard Wainer is Distinquished Research
               Scientist at the National Board of Examiners and Adjunct
               Professor of Statistics at the Wharton School of the University
               of Pennsylvania. He has a Ph.D. in Psychometrics from Princeton
               University. Professor Wainer is the author of fifteen previous
               books, most recently, Graphic Discovery: A Trout in the Milk and
               Other Visual Adventures (2005).",
  publisher = "Cambridge University Press",
  year      =  2005,
  keywords  = "Literature Review 01Chapter SG/Criterion for bad/good plots",
}

@PHDTHESIS{Henkin2018-gs,
  title    = "A framework for hierarchical time-oriented data visualisation",
  author   = "Henkin, R",
  year     =  2018,
  school   = "City, University of London",
  keywords = "Literature Review 01Chapter SG/Time Granularities and
              Visualization"
}

@ARTICLE{Wickham_undated-vr,
  title    = "40 years of boxplots",
  author   = "Wickham, Hadley and Stryjewski, Lisa",
  keywords = "Boxplots;Literature Review 01Chapter SG/Time Granularities and
              Visualization"
}



@ARTICLE{Cleveland1982-ag,
  title     = "Graphical Methods for Seasonal Adjustment",
  author    = "Cleveland, William S and Terpenning, Irma J",
  abstract  = "[Graphical displays deserve to be a routine part of seasonal
               adjustment, and their routine use is now feasible because of the
               current revolution in computer graphical hardware and software.
               We present and illustrate seven graphical displays that provide
               powerful tools for judging the adequacy of the seasonal
               adjustment of a series and for understanding the behavior of the
               trend, seasonal, and irregular components.]",
  journal={Journal of the American Statistical Association},
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  77,
  number    =  377,
  pages     = "52--62",
  year      =  1982,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}

@BOOK{Wills2011-yv,
  title     = "Visualizing Time: Designing Graphical Representations for
               Statistical Data",
  author    = "Wills, Graham",
  abstract  = "Art, or Science? Which of these is the right way to think of the
               field of visualization? This is not an easy question to answer,
               even for those who have many years experience in making
               graphical depictions of data with a view to help people
               understand it and take action. In this book, Graham Wills
               bridges the gap between the art and the science of visually
               representing data. He does not simply give rules and advice, but
               bases these on general principles and provide a clear path
               between them This book is concerned with the graphical
               representation of time data and is written to cover a range of
               different users. A visualization expert designing tools for
               displaying time will find it valuable, but so also should a
               financier assembling a report in a spreadsheet, or a medical
               researcher trying to display gene sequences using a commercial
               statistical package.",
  publisher = "Springer",
  month     =  dec,
  year      =  2011,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
}

@INPROCEEDINGS{Van_Wijk1999-um,
  title     = "Cluster and calendar based visualization of time series data",
  booktitle = "Proceedings 1999 {IEEE} Symposium on Information Visualization
               ({InfoVis'99})",
  author    = "Van Wijk, J J and Van Selow, E R",
  abstract  = "A new method is presented to get an insight into univariate time
               series data. The problem addressed is how to identify patterns
               and trends on multiple time scales (days, weeks, seasons)
               simultaneously. The solution presented is to cluster similar
               daily data patterns, and to visualize the average patterns as
               graphs and the corresponding days on a calendar. This
               presentation provides a quick insight into both standard and
               exceptional patterns. Furthermore, it is well suited to
               interactive exploration. Two applications, numbers of employees
               present and energy consumption, are presented.",
  pages     = "4--9",
  month     =  oct,
  year      =  1999,
  keywords  = "data visualisation;time series;pattern
               recognition;graphs;cluster based visualization;calendar based
               visualization;univariate time series data;pattern
               clustering;graphs;employees;energy consumption;Calendars;Data
               visualization;Time series analysis;Energy consumption;Time
               measurement;Data analysis;Fourier transforms;Mathematics;Pattern
               analysis;Data mining;Literature Review 01Chapter SG/Time
               Granularities and Visualization"
}

@BOOK{Hyndman2018-ev,
  title     = "Forecasting: principles and practice",
  author    = "Hyndman, Rob J and Athanasopoulos, George",
  abstract  = "Forecasting is required in many situations. Stocking an
               inventory may require forecasts of demand months in advance.
               Telecommunication routing requires traffic forecasts a few
               minutes ahead. Whatever the circumstances or time horizons
               involved, forecasting is an important aid in effective and
               efficient planning.This textbook provides a comprehensive
               introduction to forecasting methods and presents enough
               information about each method for readers to use them sensibly.",
  publisher = "OTexts",
  year      =  2018,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  language  = "en",
  url = {OTexts.com/fpp2}
}



@INCOLLECTION{Jensen1998-qn,
  title     = "The consensus glossary of temporal database concepts ---
               February 1998 version",
  booktitle = "Temporal Databases: Research and Practice",
  author    = "Jensen, Christian S and Dyreson, Curtis E and B{\"o}hlen,
               Michael and Clifford, James and Elmasri, Ramez and Gadia, Shashi
               K and Grandi, Fabio and Hayes, Pat and Jajodia, Sushil and
               K{\"A}fer, Wolfgang and Kline, Nick and Lorentzos, Nikos and
               Mitsopoulos, Yannis and Montanari, Angelo and Nonen, Daniel and
               Peressi, Elisa and Pernici, Barbara and Roddick, John F and
               Sarda, Nandlal L and Scalas, Maria Rita and Segev, Arie and
               Snodgrass, Richard T and Soo, Mike D and Tansel, Abdullah and
               Tiberio, Paolo and Wiederhold, Gio",
  editor    = "Etzion, Opher and Jajodia, Sushil and Sripada, Suryanarayana",
  abstract  = "This document1 contains definitions of a wide range of concepts
               specific to and widely used within temporal databases. In
               addition to providing definitions, the document also includes
               explanations of concepts as well as discussions of the adopted
               names.",
  publisher = "Springer",
  pages     = "367--405",
  year      =  1998,
  address   = "Berlin, Heidelberg",
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}





% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Tukey1977-jx,
  title     = "Exploratory data analysis",
  author    = "Tukey, John W",
  abstract  = "We were together learning how to use the analysis of variance,
               and perhaps it is worth while stating an impression that I have
               formed-that the analysis of variance, which may perhaps be
               called a statistical method, because that term is a very
               ambiguous one---is not a mathematical theorem, but rather a
               convenient method of arranging the arithmetic. Just as in
               arithmetical textbooks---if we can recall their contents---we
               were given rules for arranging how to find the greatest common
               measure, and how to work out a sum in practice, and were âŠ",
  publisher = "Addison-Wesley",
  address = "Reading, Mass.",
  year      =  1977,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}

@ARTICLE{Benjamini1988-io,
  title     = "Opening the Box of a Boxplot",
  author    = "Benjamini, Yoav",
  abstract  = "Abstract Variations of the boxplot are suggested, in which the
               sides of the box are used to convey information about the
               density of the values in a batch. The ease of computation by
               hand of the original boxplot had to be sacrificed, as the
               variations are computer-intensive. Still, the plots were
               implemented on a desktop personal computer (Apple Macintosh), in
               a way designed to keep their ease of computation by computer.
               The result is a dynamic display of densities and summaries.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  42,
  number    =  4,
  pages     = "257--262",
  month     =  nov,
  year      =  1988,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}





@ARTICLE{Brehmer2017-st,
  title    = "Timelines Revisited: A Design Space and Considerations for
              Expressive Storytelling",
  author   = "Brehmer, Matthew and Lee, Bongshin and Bach, Benjamin and Riche,
              Nathalie Henry and Munzner, Tamara",
  abstract = "There are many ways to visualize event sequences as timelines. In
              a storytelling context where the intent is to convey multiple
              narrative points, a richer set of timeline designs may be more
              appropriate than the narrow range that has been used for
              exploratory data analysis by the research community. Informed by
              a survey of 263 timelines, we present a design space for
              storytelling with timelines that balances expressiveness and
              effectiveness, identifying 14 design choices characterized by
              three dimensions: representation, scale, and layout. Twenty
              combinations of these choices are viable timeline designs that
              can be matched to different narrative points, while smooth
              animated transitions between narrative points allow for the
              presentation of a cohesive story, an important aspect of both
              interactive storytelling and data videos. We further validate
              this design space by realizing the full set of viable timeline
              designs and transitions in a proof-of-concept sandbox
              implementation that we used to produce seven example timeline
              stories. Ultimately, this work is intended to inform and inspire
              the design of future tools for storytelling with timelines.",
  journal  = "IEEE Trans. Vis. Comput. Graph.",
  volume   =  23,
  number   =  9,
  pages    = "2151--2164",
  month    =  sep,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization",
}

@ARTICLE{Fan_Du2017-xv,
  title    = "Coping with Volume and Variety in Temporal Event Sequences:
              Strategies for Sharpening Analytic Focus",
  author   = "{Fan Du} and Shneiderman, Ben and Plaisant, Catherine and Malik,
              Sana and Perer, Adam",
  abstract = "The growing volume and variety of data presents both
              opportunities and challenges for visual analytics. Addressing
              these challenges is needed for big data to provide valuable
              insights and novel solutions for business, security, social
              media, and healthcare. In the case of temporal event sequence
              analytics it is the number of events in the data and variety of
              temporal sequence patterns that challenges users of visual
              analytic tools. This paper describes 15 strategies for sharpening
              analytic focus that analysts can use to reduce the data volume
              and pattern variety. Four groups of strategies are proposed: (1)
              extraction strategies, (2) temporal folding, (3) pattern
              simplification strategies, and (4) iterative strategies. For each
              strategy, we provide examples of the use and impact of this
              strategy on volume and/or variety. Examples are selected from 20
              case studies gathered from either our own work, the literature,
              or based on email interviews with individuals who conducted the
              analyses and developers who observed analysts using the tools.
              Finally, we discuss how these strategies might be combined and
              report on the feedback from 10 senior event sequence analysts.",
  journal  = "IEEE Trans. Vis. Comput. Graph.",
  volume   =  23,
  number   =  6,
  pages    = "1636--1649",
  month    =  jun,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization",
}

@ARTICLE{Beck2017-em,
  title    = "A Taxonomy and Survey of Dynamic Graph Visualization",
  author   = "Beck, Fabian and Burch, Michael and Diehl, Stephan and Weiskopf,
              Daniel",
  abstract = "Abstract Dynamic graph visualization focuses on the challenge of
              representing the evolution of relationships between entities in
              readable, scalable and effective diagrams. This work surveys the
              growing number of approaches in this discipline. We derive a
              hierarchical taxonomy of techniques by systematically
              categorizing and tagging publications. While static graph
              visualizations are often divided into node-link and matrix
              representations, we identify the representation of time as the
              major distinguishing feature for dynamic graph visualizations:
              either graphs are represented as animated diagrams or as static
              charts based on a timeline. Evaluations of animated approaches
              focus on dynamic stability for preserving the viewer's mental map
              or, in general, compare animated diagrams to timeline-based ones.
              A bibliographic analysis provides insights into the organization
              and development of the field and its community. Finally, we
              identify and discuss challenges for future research. We also
              provide feedback from experts, collected with a questionnaire,
              which gives a broad perspective of these challenges and the
              current state of the field.",
  booktitle={Computer Graphics Forum},
  volume   =  36,
  number   =  1,
  pages    = "133--159",
  month    =  jan,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization"
}

@ARTICLE{Beck_undated-sc,
  title    = "The State of the Art in Visualizing Dynamic Graphs",
  author   = "Beck, Fabian and Burch, Michael and Diehl, Stephan and Weiskopf,
              Daniel",
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization"
}


@Manual{Earo-Sugrrants,
  title = {sugrrants: Supporting Graphs for Analysing Time Series},
  author = {Earo Wang and Di Cook and Rob Hyndman},
  year = {2018},
  note = {R package version 0.1.5},
  url = {https://CRAN.R-project.org/package=sugrrants},
}

@ARTICLE{Wainer1992-ml,
   title     = "Understanding Graphs and Tables",
   author    = "Wainer, Howard",
   abstract  = "Quantitative phenomena can be displayed effectively in a variety
                of ways, but to do so requires an understanding of both the
                structure of the phenomena and the limitations of candidate
                display formats. This article (a) re-counts three historic
                instances of the vital role data displays played in important
                discoveries, (b) provides three levels of information that form
                the basis of a theory of display to help us better measure both
                display quality and human graphicacy, and (c) describes three
                steps to improve the quality of tabular presentation.",
   journal   = "Educ. Res.",
   publisher = "American Educational Research Association",
   volume    =  21,
   number    =  1,
   pages     = "14--23",
   month     =  jan,
   year      =  1992
   }

   @BOOK{Unwin2016-aq,
    title     = "Graphical Data Analysis with {R}",
    author    = "Unwin, Antony",
    abstract  = "See How Graphics Reveal Information Graphical Data Analysis with
                 R shows you what information you can gain from graphical
                 displays. The book focuses on why you draw graphics to display
                 data and which graphics to draw (and uses R to do so). All the
                 datasets are available in R or one of its packages and the R
                 code is available at rosuda.org/GDA. Graphical data analysis is
                 useful for data cleaning, exploring data structure, detecting
                 outliers and unusual groups, identifying trends and clusters,
                 spotting local patterns, evaluating modelling output, and
                 presenting results. This book guides you in choosing graphics
                 and understanding what information you can glean from them. It
                 can be used as a primary text in a graphical data analysis
                 course or as a supplement in a statistics course. Colour
                 graphics are used throughout.",
    publisher = "CRC Press",
    month     =  apr,
    year      =  2016,
  }

  @BOOK{Cleveland1993-fn,
    title     = "Visualizing Data",
    author    = "Cleveland, William S",
    publisher = "Hobart Press",
    year      =  1993
  }


   @BOOK{Maimon2010-ac,
     title     = "Data Mining and Knowledge Discovery Handbook",
     author    = "Maimon, Oded and Rokach, Lior",
     abstract  = "Knowledge Discovery demonstrates intelligent computing at its
                  best, and is the most desirable and interesting end-product of
                  Information Technology. To be able to discover and to extract
                  knowledge from data is a task that many researchers and
                  practitioners are endeavoring to accomplish. There is a lot of
                  hidden knowledge waiting to be discovered -- this is the
                  challenge created by today's abundance of data. Data Mining and
                  Knowledge Discovery Handbook, 2nd Edition organizes the most
                  current concepts, theories, standards, methodologies, trends,
                  challenges and applications of data mining (DM) and knowledge
                  discovery in databases (KDD) into a coherent and unified
                  repository. This handbook first surveys, then provides
                  comprehensive yet concise algorithmic descriptions of methods,
                  including classic methods plus the extensions and novel methods
                  developed recently. This volume concludes with in-depth
                  descriptions of data mining applications in various
                  interdisciplinary industries including finance, marketing,
                  medicine, biology, engineering, telecommunications, software,
                  and security. Data Mining and Knowledge Discovery Handbook, 2nd
                  Edition is designed for research scientists, libraries and
                  advanced-level students in computer science and engineering as a
                  reference. This handbook is also suitable for professionals in
                  industry, for computing applications, information systems
                  management, and strategic research management.",
     publisher = "Springer Science \& Business Media",
     month     =  sep,
     year      =  2010,
   }

    @ARTICLE{Zhang2003-bj,
      title    = "Spatial Cone Tree: An Auxiliary Search Structure for
                  Correlation-based Similarity Queries on Spatial Time Series Data",
      author   = "Zhang, Pusheng and Shekhar, Shashi and Kumar, Vipin and Huang,
                  Yan",
      abstract = "ResearchGate is a network dedicated to science and research.
                  Connect, collaborate and discover scientific publications, jobs
                  and conferences. All for free.",
      month    =  nov,
      year     =  2003
    }

    @ARTICLE{Camossi2006-lg,
  title    = "A multigranular object-oriented framework supporting
              spatio-temporal granularity conversions",
  author   = "Camossi, Elena and Bertolotti, Mario and Bertino, Elisa",
  abstract = "Several application domains require handling spatio-temporal
              data. However, traditional Geographic Information Systems (GIS)
              and database models do not adequately support temporal aspects of
              spatial data. A crucial issue relates to the choice of the
              appropriate granularity. Unfortunately, while a formalisation of
              the concept of temporal granularity has been proposed and widely
              adopted, no consensus exists on the notion of spatial
              granularity. In this paper, we address these open problems, by
              proposing a formal definition of spatial granularity and by
              designing a spatio-temporal framework for the management of
              spatial and temporal information at different granularities. We
              present a spatio-temporal extension of the ODMG type system with
              specific types for defining multigranular spatio-temporal
              properties. Granularity conversion functions are introduced to
              obtain attributes values at different spatial and temporal
              granularities.",
  series   = "Cyber Center Publications",
  year     =  2006
}

@ARTICLE{Euzenat2005-de,
   title  = "Time Granularity",
   author = "Euzenat, Jerome and Montanari, Angelo",
   year   =  2005
 }

@ARTICLE{Grolemund2011-qf,
   title   = "Dates and times made easy with lubridate",
   author  = "Grolemund, Garrett and Wickham, Hadley and {Others}",
   journal = "J. Stat. Softw.",
   volume  =  40,
   number  =  3,
   pages   = "1--25",
   year    =  2011
 }

 @MANUAL{smart-meter,
  title       = {Smart-Grid Smart-City Customer Trial Data},
  author      = "{Department of the Environment and Energy}",
  institution = "Department of the Environment and Energy, Australia",
  year        =  2018,
  url         = "https://data.gov.au/dataset/4e21dea3-9b87-4610-94c7-15a8a77907ef",
  address     = "Australian Government, Department of the Environment and Energy"
}


@ARTICLE{ieee-irish,
  author={R. J. {Hyndman} and X. {Liu} and P. {Pinson}},
  journal={IEEE Power and Energy Magazine},
  title={Visualizing Big Energy Data: Solutions for This Crucial Component of Data Analysis},
  year={2018},
  volume={16},
  number={3},
  pages={18-25},
  doi={10.1109/MPE.2018.2801441}}

@Book{knitr,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  url = {https://yihui.name/knitr/},
  doi = {10.1201/b15166}
}

@Book{rmarkdown,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  url = {https://bookdown.org/yihui/rmarkdown},
  doi = {10.1201/9781138359444}
}



@Manual{R-gravitas,
    title = {{gravitas}: Explore Probability Distributions for Bivariate Temporal
Granularities},
    author = {Sayani Gupta and Rob Hyndman and Di Cook and Antony Unwin},
    year = {2019},
    note = {R package version 0.1.0},
    url = {https://CRAN.R-project.org/package=gravitas},
  }

@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2019},
  note = {R package version 0.8.3},
  url = {https://CRAN.R-project.org/package=dplyr},
}


@Manual{R-magrittr,
  title = {magrittr: A Forward-Pipe Operator for R},
  author = {Stefan Milton Bache and Hadley Wickham},
  year = {2014},
  note = {R package version 1.5},
  url = {https://CRAN.R-project.org/package=magrittr},
}

@Manual{R-rlang,
  title = {rlang: Functions for Base Types and Core R and 'Tidyverse' Features},
  author = {Lionel Henry and Hadley Wickham},
  year = {2019},
  note = {R package version 0.4.0},
  url = {https://CRAN.R-project.org/package=rlang},
}

@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Lionel Henry},
  year = {2019},
  note = {http://tidyr.tidyverse.org, https://github.com/tidyverse/tidyr},
}
@Manual{R-tsibble,
  title = {tsibble: Tidy Temporal Data Frames and Tools},
  author = {Earo Wang and Di Cook and Rob Hyndman and Mitchell O'Hara-Wild},
  year = {2019},
  note = {R package version 0.8.3.9000},
  url = {https://tsibble.tidyverts.org},
}

@Manual{R-knitr,
    title = {knitr: A General-Purpose Package for Dynamic Report Generation in R},
    author = {Yihui Xie},
    year = {2020},
    note = {R package version 1.28},
    url = {https://yihui.org/knitr/},
  }

@Manual{R-rmarkdown,
    title = {rmarkdown: Dynamic Documents for R},
    author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
    year = {2020},
    note = {R package version 2.1},
    url = {https://github.com/rstudio/rmarkdown},
  }

@Manual{R-language,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
  }


 @ARTICLE{Potter2010-qc,
   title    = "Visualizing Summary Statistics and Uncertainty",
   author   = "Potter, K and Kniss, J and Riesenfeld, R and Johnson, C R",
   abstract = "Abstract The graphical depiction of uncertainty information is
               emerging as a problem of great importance. Scientific data sets
               are not considered complete without indications of error,
               accuracy, or levels of confidence. The visual portrayal of this
               information is a challenging task. This work takes inspiration
               from graphical data analysis to create visual representations
               that show not only the data value, but also important
               characteristics of the data including uncertainty. The canonical
               box plot is reexamined and a new hybrid summary plot is presented
               that incorporates a collection of descriptive statistics to
               highlight salient features of the data. Additionally, we present
               an extension of the summary plot to two dimensional
               distributions. Finally, a use-case of these new plots is
               presented, demonstrating their ability to present high-level
               overviews as well as detailed insight into the salient features
               of the underlying data distribution.",
   booktitle={Computer Graphics Forum},
   volume   =  29,
   number   =  3,
   pages    = "823--832",
   month    =  aug,
   year     =  2010,
   doi = {10.1111/j.1467-8659.2009.01677.x}
 }


@book{Grolemund2018-po,
   title    = "{R} for data science",
   author   = "Grolemund, Garrett and Wickham, Hadley",
   publisher = {O'Reilly Media},
   abstract = "This book will teach you how to do data science with R: You'll
               learn how to get your data into R, get it into the most useful
               structure, transform it, visualise it and model it. In this book,
               you will find a practicum of skills for data science. Just as a
               chemist learns how to clean test tubes and stock a lab, you'll
               learn how to clean data and draw plots---and many other things
               besides. These are the skills that allow data science to happen,
               and here you will find the best practices for doing each of these
               things with R. You'll learn how to use the grammar of graphics,
               literate programming, and reproducible research to save time.
               You'll also learn how to manage cognitive resources to facilitate
               discoveries when wrangling, visualising, and exploring data.",
   year     =  2017
 }



#### NOT NEEDED

 @ARTICLE{Bettini2000-vy,
   title    = "Symbolic representation of user-defined time granularities",
   author   = "Bettini, Claudio and De Sibi, Roberto",
   abstract = "In the recent literature on time representation, an effort has
               been made to characterize the notion of time granularity and the
               relationships between granularities. The main goals are having a
               common framework for their specification, and allowing the
               interoperability of systems adopting different time
               granularities. This paper considers the mathematical
               characterization of finite and periodic time granularities, and
               investigates the requirements for a user-friendly symbolic
               formalism that could be used for their specification. Instead of
               proposing yet another formalism, the paper analyzes the
               expressiveness of known symbolic formalisms for the
               representation of granularities, using the mathematical
               characterization as a reference model. Based on this analysis, a
               significant extension to the collection formalism defined in [15]
               is proposed, in order to capture a practically interesting class
               of periodic granularities.",
   journal = {Annals of Mathematics and Artificial Intelligence},
   volume   =  30,
   number   =  1,
   pages    = "53--92",
   month    =  jun,
   year     =  2000
 }

@ARTICLE{Hyndman1996-ty,
   title     = "Sample Quantiles in Statistical Packages",
   author    = "Hyndman, Rob J and Fan, Yanan",
   abstract  = "Abstract There are a large number of different definitions used
                for sample quantiles in statistical computer packages. Often
                within the same package one definition will be used to compute a
                quantile explicitly, while other definitions may be used when
                producing a boxplot, a probability plot, or a QQ plot. We
                compare the most commonly implemented sample quantile
                definitions by writing them in a common notation and
                investigating their motivation and some of their properties. We
                argue that there is a need to adopt a standard definition for
                sample quantiles so that the same answers are produced by
                different packages and within each package. We conclude by
                recommending that the median-unbiased estimator be used because
                it has most of the desirable properties of a quantile estimator
                and can be defined independently of the underlying distribution.",
   journal   = "Am. Stat.",
   publisher = "Taylor \& Francis",
   volume    =  50,
   number    =  4,
   pages     = "361--365",
   month     =  nov,
   year      =  1996
 }

@BOOK{Silverman1986-cl,
   title     = "Density estimation for statistics and data analysis",
   author    = "Silverman, B W",
   publisher = "Chapman and Hall",
   series    = "Monographs on statistics and applied probability ; [26]",
   year      =  1986,
   address   = "London ; New York",
   keywords  = "Estimation theory"
 }

@ARTICLE{Jones1992-pj,
   title    = "Estimating densities, quantiles, quantile densities and density
               quantiles",
   author   = "Jones, M C",
   abstract = "To estimate the quantile density function (the derivative of the
               quantile function) by kernel means, there are two alternative
               approaches. One is the derivative of the kernel quantile
               estimator, the other is essentially the reciprocal of the kernel
               density estimator. We give ways in which the former method has
               certain advantages over the latter. Various closely related
               smoothing issues are also discussed.",
   journal  = "Ann. Inst. Stat. Math.",
   volume   =  44,
   number   =  4,
   pages    = "721--727",
   month    =  dec,
   year     =  1992
 }

@BOOK{Bettini2000-qk,
   title     = "Time Granularities in Databases, Data Mining, and Temporal
                Reasoning",
   author    = "Bettini, Claudio and Jajodia, Sushil and Wang, Sean",
   abstract  = "Calendar units, such as months and days, clock units, such as
                hours and seconds, and specialized units, such as business days
                and academic years, play a major role in a wide range of
                information system applications. System support for reasoning
                about these units, called granularities in this book, is
                important for the efficient design, use, and implementation of
                such applications. The book deals with several aspects of
                temporal information and provides a unifying model for
                granularities. It is intended for computer scientists and
                engineers who are interested in the formal models and technical
                development of specific issues. Practitioners can learn about
                critical aspects that must be taken into account when designing
                and implementing databases supporting temporal information.
                Lecturers may find this book useful for an advanced course on
                databases. Moreover, any graduate student working on time
                representation and reasoning, either in data or knowledge bases,
                should definitely read it.",
   publisher = "Springer Science \& Business Media",
   month     =  jul,
   year      =  2000,
 }

@BOOK{Embrechts2013-pd,
   title     = "Modelling Extremal Events: for Insurance and Finance",
   author    = "Embrechts, Paul and Kl{\"u}ppelberg, Claudia and Mikosch, Thomas",
   abstract  = "Both in insurance and in finance applications, questions
                involving extremal events (such as large insurance claims, large
                fluctuations in financial data, stock market shocks, risk
                management, ...) play an increasingly important role. This book
                sets out to bridge the gap between the existing theory and
                practical applications both from a probabilistic as well as from
                a statistical point of view. Whatever new theory is presented is
                always motivated by relevant real-life examples. The numerous
                illustrations and examples, and the extensive bibliography make
                this book an ideal reference text for students, teachers and
                users in the industry of extremal event methodology.",
   publisher = "Springer Science \& Business Media",
   month     =  jan,
   year      =  2013,
 }

 @ARTICLE{Grosse2002-ex,
   title    = "Analysis of symbolic sequences using the {Jensen-Shannon}
               divergence",
   author   = "Grosse, Ivo and Bernaola-Galv{\'a}n, Pedro and Carpena, Pedro and
               Rom{\'a}n-Rold{\'a}n, Ram{\'o}n and Oliver, Jose and Stanley, H
               Eugene",
   abstract = "We study statistical properties of the Jensen-Shannon divergence
               D, which quantifies the difference between probability
               distributions, and which has been widely applied to analyses of
               symbolic sequences. We present three interpretations of D in the
               framework of statistical physics, information theory, and
               mathematical statistics, and obtain approximations of the mean,
               the variance, and the probability distribution of D in random,
               uncorrelated sequences. We present a segmentation method based on
               D that is able to segment a nonstationary symbolic sequence into
               stationary subsequences, and apply this method to DNA sequences,
               which are known to be nonstationary on a wide range of different
               length scales.",
   journal  = "Phys. Rev. E Stat. Nonlin. Soft Matter Phys.",
   volume   =  65,
   number   = "4 Pt 1",
   pages    = "041905",
   month    =  apr,
   year     =  2002,
 }



@article{Menendez1997-in,
  title={The {Jensen-Shannon} divergence},
  author={Men{\'e}ndez, M L and Pardo, J A and Pardo, L and Pardo, M C},
  journal={Journal of the Franklin Institute},
  volume={334},
  number={2},
  pages={307--318},
  year={1997},
  publisher={Elsevier}
} 

 

@ARTICLE{Lin1991-fj,
   title    = "Divergence measures based on the Shannon entropy",
   author   = "Lin, J",
   abstract = "A novel class of information-theoretic divergence measures baseda
               on the Shannon entropy is introduced. Unlike the well-known
               Kullback divergences, the new measures do not require the
               condition of absolute continuity to be satisfied by the
               probability distributions involved. More importantly, their close
               relationship with the variational distance and the probability of
               misclassification error are established in terms of bounds. These
               bounds are crucial in many applications of divergence measures.
               The measures are also well characterized by the properties of
               nonnegativity, finiteness, semiboundedness, and boundedness.",
   journal={IEEE Transactions on Information theory},
   volume   =  37,
   number   =  1,
   pages    = "145--151",
   month    =  jan,
   year     =  1991,
   keywords = "entropy;information theory;information theory;Shannon
               entropy;divergence measures;variational distance;probability of
               misclassification
               error;nonnegativity;finiteness;semiboundedness;boundedness;Entropy;Probability
               distribution;Upper bound;Pattern analysis;Signal analysis;Signal
               processing;Pattern recognition;Taxonomy;Genetics;Computer science"
 }

@BOOK{De_Haan2007-yx,
   title     = "Extreme Value Theory: An Introduction",
   author    = "de Haan, Laurens and Ferreira, Ana",
   abstract  = "This treatment of extreme value theory is unique in book
                literature in that it focuses on some beautiful theoretical
                results along with applications. All the main topics covering
                the heart of the subject are introduced to the reader in a
                systematic fashion so that in the final chapter even the most
                recent developments in the theory can be understood. Key to the
                presentation is the concentration on the probabilistic and
                statistical aspects of extreme values without major emphasis on
                such related topics as regular variation, point processes,
                empirical distribution functions, and Brownian motion. The work
                is an excellent introduction to extreme value theory at the
                graduate level, requiring only some mathematical maturity.",
   publisher = "Springer Science \& Business Media",
   month     =  dec,
   year      =  2007,
 }

@ARTICLE{Kullback1951-jy,
   title     = "On Information and Sufficiency",
   author    = "Kullback, S and Leibler, R A",
   journal={The annals of mathematical statistics},
   publisher = "Institute of Mathematical Statistics",
   volume    =  22,
   number    =  1,
   pages     = "79--86",
   year      =  1951
 }


@ARTICLE{Lin1991-fj,
  title    = "Divergence measures based on the Shannon entropy",
  author   = "Lin, J",
  abstract = "A novel class of information-theoretic divergence measures based
              on the Shannon entropy is introduced. Unlike the well-known
              Kullback divergences, the new measures do not require the
              condition of absolute continuity to be satisfied by the
              probability distributions involved. More importantly, their close
              relationship with the variational distance and the probability of
              misclassification error are established in terms of bounds. These
              bounds are crucial in many applications of divergence measures.
              The measures are also well characterized by the properties of
              nonnegativity, finiteness, semiboundedness, and boundedness.",
  journal={IEEE Transactions on Information theory},
  volume   =  37,
  number   =  1,
  pages    = "145--151",
  month    =  jan,
  year     =  1991,
  keywords = "entropy;information theory;information theory;Shannon
              entropy;divergence measures;variational distance;probability of
              misclassification
              error;nonnegativity;finiteness;semiboundedness;boundedness;Entropy;Probability
              distribution;Upper bound;Pattern analysis;Signal analysis;Signal
              processing;Pattern recognition;Taxonomy;Genetics;Computer science"
}


@INPROCEEDINGS{Dang2014-tw,
   title     = "{ScagExplorer}: Exploring Scatterplots by Their Scagnostics",
   booktitle = "2014 {IEEE} Pacific Visualization Symposium",
   author    = "Dang, T N and Wilkinson, L",
   abstract  = "A scatter plot displays a relation between a pair of variables.
                Given a set of v variables, there are v(v- 1)/2 pairs of
                variables, and thus the same number of possible pair wise
                scatter plots. Therefore for even small sets of variables, the
                number of scatter plots can be large. Scatter plot matrices
                (SPLOMs) can easily run out of pixels when presenting
                high-dimensional data. We introduce a theoretical method and a
                testbed for assessing whether our method can be used to guide
                interactive exploration of high-dimensional data. The method is
                based on nine characterizations of the 2D distributions of
                orthogonal pair wise projections on a set of points in
                multidimensional Euclidean space. Working directly with these
                characterizations, we can locate anomalies for further analysis
                or search for similar distributions in a large SPLOM with more
                than a hundred dimensions. Our testbed, ScagExplorer, is
                developed in order to evaluate the feasibility of handling huge
                collections of scatter plots.",
   pages     = "73--80",
   month     =  mar,
   year      =  2014,
   keywords  = "computer graphics;data handling;graph theory;interactive
                systems;ScagExplorer;variables;pairwise scatterplots;scatterplot
                matrices;high-dimensional data;theoretical method;interactive
                exploration;2D distributions;orthogonal pairwise
                projections;multidimensional Euclidean space;large
                SPLOM;Clustering algorithms;Silicon;Layout;Educational
                institutions;Data visualization;Correlation;Time series
                analysis;Scagnostics;Scatterplot matrix;High-Dimensional Visual
                Analytics;Leader algorithm;forced-directed layout;Design
                MethodologyPattern analysis"
 }

@inproceedings{wilkinson2005graph,
  title={Graph-theoretic scagnostics},
  author={Wilkinson, Leland and Anand, Anushka and Grossman, Robert},
  booktitle={IEEE Symposium on Information Visualization, 2005. INFOVIS 2005.},
  pages={157--164},
  year={2005},
  organization={IEEE}
}

@article{tukey1988computer,
  title={Computer graphics and exploratory data analysis: An introduction},
  author={Tukey, John W and Tukey, Paul A},
  journal={The Collected Works of John W. Tukey: Graphics: 1965-1985},
  volume={5},
  pages={419},
  year={1988},
  publisher={Wadsworth Publishing Company}
}


@Article{inference,
  author = {Andreas Buja and Dianne Cook and Heike Hofmann and Michael Lawrence and Eun-Kyung Lee and Deborah F. Swayne and Hadley Wickham},
  doi = {10.1098/rsta.2009.0120},
  journal = {Royal Society Philosophical Transactions A},
  number = {1906},
  pages = {4361–4383},
  title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
  volume = {367},
  year = {2009},
  bdsk-url-1 = {http://dx.doi.org/10.1098/rsta.2009.0120},
}

@ARTICLE{Majumder2013-hb,
   title     = "Validation of Visual Statistical Inference, Applied to Linear
                Models",
   author    = "Majumder, Mahbubul and Hofmann, Heike and Cook, Dianne",
   abstract  = "Statistical graphics play a crucial role in exploratory data
                analysis, model checking, and diagnosis. The lineup protocol
                enables statistical significance testing of visual findings,
                bridging the gulf between exploratory and inferential
                statistics. In this article, inferential methods for statistical
                graphics are developed further by refining the terminology of
                visual inference and framing the lineup protocol in a context
                that allows direct comparison with conventional tests in
                scenarios when a conventional test exists. This framework is
                used to compare the performance of the lineup protocol against
                conventional statistical testing in the scenario of fitting
                linear models. A human subjects experiment is conducted using
                simulated data to provide controlled conditions. Results suggest
                that the lineup protocol performs comparably with the
                conventional tests, and expectedly outperforms them when data
                are contaminated, a scenario where assumptions required for
                performing a conventional test are violated. Surprisingly,
                visual tests have higher power than the conventional tests when
                the effect size is large. And, interestingly, there may be some
                super-visual individuals who yield better performance and power
                than the conventional test even in the most difficult tasks.
                Supplementary materials for this article are available online.",
   journal={Journal of the American Statistical Association},
   publisher = "Taylor \& Francis",
   volume    =  108,
   number    =  503,
   pages     = "942--956",
   month     =  sep,
   year      =  2013
 }

@Manual{R-patchwork,
    title = {patchwork: The Composer of Plots},
    author = {Thomas Lin Pedersen},
    year = {2020},
    note = {R package version 1.1.0},
    url = {https://CRAN.R-project.org/package=patchwork},
  }



@ARTICLE{Bogner2012-az,
   title     = "Technical Note: The normal quantile transformation and its
                application in a flood forecasting system",
   author    = "Bogner, K and Pappenberger, F and Cloke, H L",
   abstract  = "Abstract. The Normal Quantile Transform (NQT) has been used in
                many hydrological and meteorological applications in order to
                make the Cumulated Distribution Function (CDF) of the observed,
                simulated and forecast river discharge, water level or
                precipitation data Gaussian. It is also the heart of the
                meta-Gaussian model for assessing the total predictive
                uncertainty of the Hydrological Uncertainty Processor (HUP)
                developed by Krzysztofowicz. In the field of geo-statistics this
                transformation is better known as the Normal-Score Transform. In
                this paper some possible problems caused by small sample sizes
                when applying the NQT in flood forecasting systems will be
                discussed and a novel way to solve the problem will be outlined
                by combining extreme value analysis and non-parametric
                regression methods. The method will be illustrated by examples
                of hydrological stream-flow forecasts.",
   journal   = "Hydrol. Earth Syst. Sci.",
   publisher = "Copernicus GmbH",
   volume    =  16,
   number    =  4,
   pages     = "1085--1094",
   month     =  apr,
   year      =  2012,
   copyright = "https://creativecommons.org/licenses/by/3.0/",
 }

@ARTICLE{JSD,
  author={J. {Lin}},
  journal={IEEE Transactions on Information Theory},
  title={Divergence measures based on the Shannon entropy},
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  doi={10.1109/18.61115}}

@ARTICLE{Krzysztofowicz1997-bv,
   title     = "Transformation and normalization of variates with specified
                distributions",
   author    = "Krzysztofowicz, Roman",
   abstract  = "Given two continuous random variables X and Y, with specified
                strictly increasing cumulative distribution functions F and G,
                respectively, the one-to-one transform t which maps one variate
                into another, say Y=t(X), has an analytic form, t(X)=G−1(F(X))
                or t(X)=G−1(1−F(X)), depending upon whether t is increasing or
                decreasing. This fact of probability theory is reviewed and
                compared with another method for finding t that was recently
                proposed. Applications to system identification, normalization
                of a variate, and normalization of a sample are briefly
                discussed.",
   journal   = "J. Hydrol.",
   publisher = "Elsevier BV",
   volume    =  197,
   number    = "1-4",
   pages     = "286--292",
   month     =  oct,
   year      =  1997,
 }

@ARTICLE{Lin1991-pt,
   title    = "Divergence measures based on the Shannon entropy",
   author   = "Lin, J",
   abstract = "A novel class of information-theoretic divergence measures based
               on the Shannon entropy is introduced. Unlike the well-known
               Kullback divergences, the new measures do not require the
               condition of absolute continuity to be satisfied by the
               probability distributions involved. More importantly, their close
               relationship with the variational distance and the probability of
               misclassification error are established in terms of bounds. These
               bounds are crucial in many applications of divergence measures.
               The measures are also well characterized by the properties of
               nonnegativity, finiteness, semiboundedness, and boundedness.",
   journal={IEEE Transactions on Information theory},
   volume   =  37,
   number   =  1,
   pages    = "145--151",
   month    =  jan,
   year     =  1991,
   keywords = "Entropy;Probability distribution;Upper bound;Pattern
               analysis;Signal analysis;Signal processing;Pattern
               recognition;Taxonomy;Genetics;Computer science"
 }


@BOOK{Faraway2016-uk,
   title     = "Extending the Linear Model with {R} : Generalized Linear, Mixed
                Effects and Nonparametric Regression Models, Second Edition",
   author    = "Faraway, Julian J",
   abstract  = "Start Analyzing a Wide Range of Problems Since the publication
                of the bestselling, highly recommended first edition, R has
                considerably",
   publisher = "Chapman and Hall/CRC",
   edition   = "2nd Edition",
   month     =  mar,
   year      =  2016
 }

@Book{knitr,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  url = {https://yihui.name/knitr/},
  doi = {10.1201/b15166}
}

@book{edgington2007randomization,
  title={Randomization tests},
  author={Edgington, Eugene and Onghena, Patrick},
  year={2007},
  publisher={CRC press}
}

@article{wertheimer1938gestalt,
  title={Gestalt theory.},
  author={Wertheimer, Max},
  year={1938},
  publisher={Kegan Paul, Trench, Trubner & Company}
}