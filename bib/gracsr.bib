@Book{Wickham2009pk,
   author = {Hadley Wickham},
   title = {ggplot2: Elegant Graphics for Data Analysis},
   publisher = {Springer-Verlag New York},
   year = {2016},
   isbn = {978-3-319-24277-4},
   url = {http://ggplot2.org},
   doi = {10.1007/978-3-319-24277-4}
 }

@book{aigner2011visualization,
  title={Visualization of time-oriented data},
  author={Aigner, Wolfgang and Miksch, Silvia and Schumann, Heidrun and Tominski, Christian},
  year={2011},
  publisher={Springer Science \& Business Media},
  doi = {10.1007/978-0-85729-079-3}
  
}

@article{wang2020calendar,
  author = {Earo Wang and Dianne Cook and Rob J Hyndman},
  title = {Calendar-based graphics for visualizing people's daily schedules},
  journal = {Journal of Computational \& Graphical Statistics},
  volume = {29},
  number = {3},
  pages = {490-502},
  year  = {2020},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2020.1715226}
}

@INCOLLECTION{Bettini1998-ed,
  title     = "A glossary of time granularity concepts",
  booktitle = "Temporal Databases: Research and Practice",
  author    = "Bettini, Claudio and Dyreson, Curtis E and Evans, William S and
               Snodgrass, Richard T and Wang, X Sean",
  editor    = "Etzion, Opher and Jajodia, Sushil and Sripada, Suryanarayana",
  publisher = "Springer Berlin Heidelberg",
  pages     = "406--413",
  year      =  1998,
  address   = "Berlin, Heidelberg",
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1007/BFb0053711}
}

@ARTICLE{Grolemund2011-vm,
    title = {Dates and Times Made Easy with {lubridate}},
    author = {Garrett Grolemund and Hadley Wickham},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {40},
    number = {3},
    pages = {1--25},
    url = {http://www.jstatsoft.org/v40/i03/},
   doi = {10.18637/jss.v040.i03}
}
 

@inproceedings{Goodwin_2012,
  doi = {10.1109/vast.2012.6400545}, 
  year = 2012,
  month = {oct},
  publisher = {{IEEE}},
  author = {Sarah Goodwin and Jason Dykes},
  title = {Visualising variations in household energy consumption},
	booktitle = {2012 {IEEE} Conference on Visual Analytics Science and Technology ({VAST})}
}


@article{Dyreson_2000,
	doi = {10.1109/69.868908},
	year = 2000,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {12},
	number = {4},
	pages = {568--587},
	author = {C.E. Dyreson and W.S. Evans and H. Lin and R.T. Snodgrass},
	title = {Efficiently supporting temporal granularities},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering}
}

@ARTICLE{Ning_2002,
   title    = "An Algebraic Representation of Calendars",
   author   = "Ning, Peng and Wang, Xiaoyang Sean and Jajodia, Sushil",
   abstract = "This paper uses an algebraic approach to define temporal
               granularities and calendars. All the granularities in a calendar
               are expressed as algebraic expressions based on a single
               ``bottom'' granularity. The operations used in the algebra
               directly reflect the ways with which people construct new
               granularities from existing ones, and hence yield more natural
               and compact granularities definitions. Calendar is formalized on
               the basis of the algebraic operations, and properties of
               calendars are studied. As a step towards practical applications,
               the paper also presents algorithms for granule conversions
               between granularities in a calendar.",
   journal  = "Ann. Math. Artif. Intell.",
   volume   =  36,
   number   =  1,
   pages    = "5--38",
   month    =  sep,
   year     =  2002,
   doi = {10.1023/A:1015835418881}
 }
 
@article{wang2019tsibble,
  Author = {Earo Wang and Dianne Cook and Rob J Hyndman},
  Title = {A new tidy data structure to support exploration and modeling of temporal data},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2020},
  note = {to appear},
  doi = {10.1080/10618600.2019.1695624}
}

@BOOK{Reingold2001-kf,
   title     = "Calendrical Calculations",
   edition = "Millennium Edition",
   author    = "Reingold, Edward M and Dershowitz, Nachum",
   abstract  = "This new edition of the successful calendars book is being
                published at the turn of the millennium and expands the
                treatment of the previous edition to new calendars and variants.
                As interest grows in the impact of seemingly arbitrary
                calendrical systems upon our daily lives, this book frames the
                world in a completely algorithmic form. The book gives a
                description of twenty-five calendars and how they relate to one
                another: the Gregorian (current civil), ISO (International
                Organization for Standardization), Egyptian (and nearly
                identical Armenian), Julian (old civil), Coptic, Ethiopic,
                Islamic (Moslem), modern Persian (both astronomical and
                arithmetic forms), Baha'i (both present and future forms),
                Hebrew (Jewish), Mayan (long count, haab, and tzolkin), Balinese
                Pawukon, French Revolutionary (both astronomical and arithmetic
                forms), Chinese (and nearly identical Japanese), old Hindu
                (solar and lunisolar), and modern Hindu (solar and lunisolar).
                Easy conversion among these calendars is a by-product of the
                approach, as is the determination of secular and religious
                holidays. Calendrical Calculations makes accurate calendrical
                algorithms readily available for computer use with LISP,
                Mathematica, and Java code for all the algorithms included on
                the CD, and updates are available on the Web. This book will be
                a valuable resource for working programmers as well as a fount
                of useful algorithmic tools for computer scientists. In
                addition, the lay reader will find the historical setting and
                general calendar descriptions of great interest.",
   publisher = "Cambridge University Press",
   month     =  aug,
   year      =  2001,
   language  = "en",
   doi = {10.1017/9781107415058}
 }

@BOOK{Wilkinson1999-nk,
  title    = "The Grammar of Graphics",
  author   = "Wilkinson, Leland",
  year     =  1999,
  publisher = {Springer},
  address = {New York}
}

@ARTICLE{Mcgill1978-hg,
  title     = "Variations of Box Plots",
  author    = "Mcgill, Robert and Tukey, John W and Larsen, Wayne A",
  abstract  = "Abstract Box plots display batches of data. Five values from a
               set of data are conventionally used; the extremes, the upper and
               lower hinges (quartiles), and the median. Such plots are
               becoming a widely used tool in exploratory data analysis and in
               preparing visual summaries for statisticians and
               nonstatisticians alike. Three variants of the basic display,
               devised by the authors, are described. The first visually
               incorporates a measure of group size; the second incorporates an
               indication of rough significance of differences between medians;
               the third combines the features of the first two. These
               techniques are displayed by examples.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  32,
  number    =  1,
  pages     = "12--16",
  month     =  feb,
  year      =  1978,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1080/00031305.1978.10479236}
}


@ARTICLE{Hofmann2017-sg,
  title     = "{Letter-Value} Plots: Boxplots for Large Data",
  author    = "Hofmann, Heike and Wickham, Hadley and Kafadar, Karen",
  abstract  = "ABSTRACTBoxplots are useful displays that convey rough
               information about the distribution of a variable. Boxplots were
               designed to be drawn by hand and work best for small datasets,
               where detailed estimates of tail behavior beyond the quartiles
               may not be trustworthy. Larger datasets afford more precise
               estimates of tail behavior, but boxplots do not take advantage
               of this precision, instead presenting large numbers of extreme,
               though not unexpected, observations. Letter-value plots address
               this problem by including more detailed information about the
               tails using ?letter values,? an order statistic defined by
               Tukey. Boxplots display the first two letter values (the median
               and quartiles); letter-value plots display further letter values
               so far as they are reliable estimates of their corresponding
               quantiles. We illustrate letter-value plots with real data that
               demonstrate their usefulness for large datasets. All graphics
               are created using the R package lvplot, and code and data are
               available in the supplementary materials.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  26,
  number    =  3,
  pages     = "469--477",
  month     =  jul,
  year      =  2017,
  keywords  = "Boxplots;Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.1080/10618600.2017.1305277}
}


@ARTICLE{Hintze1998-zi,
  title     = "Violin Plots: A Box {Plot-Density} Trace Synergism",
  author    = "Hintze, Jerry L and Nelson, Ray D",
  abstract  = "[Many modifications build on Tukey's original box plot. A
               proposed further adaptation, the violin plot, pools the best
               statistical features of alternative graphical representations of
               batches of data. It adds the information available from local
               density estimates to the basic summary statistics inherent in
               box plots. This marriage of summary statistics and density shape
               into a single plot provides a useful tool for data analysis and
               exploration.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  52,
  number    =  2,
  pages     = "181--184",
  year      =  1998,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.2307/2685478}
}

@ARTICLE{Hyndman1996-ft,
  title     = "Computing and Graphing Highest Density Regions",
  author    = "Hyndman, Rob J",
  abstract  = "Many statistical methods involve summarizing a probability
               distribution by a region of the sample space covering a
               specified probability. One method of selecting such a region is
               to require it to contain points of relatively high density.
               Highest density regions are particularly useful for displaying
               multimodal distributions and, in such cases, may consist of
               several disjoint subsets-one for each local mode. In this paper,
               I propose a simple method for computing a highest density region
               from any given (possibly multivariate) density f(x) that is
               bounded and continuous in x. Several examples of the use of
               highest density regions in statistical graphics are also given.
               A new form of boxplot is proposed based on highest density
               regions; versions in one and two dimensions are given. Highest
               density regions in higher dimensions are also discussed and
               plotted.",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  50,
  number    =  2,
  pages     = "120--126",
  year      =  1996,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  doi = {10.2307/2684423}
}









@MISC{Healey2017-tv,
  title        = "Perception in Visualization",
  booktitle    = "{NC} State University Computer Science",
  author       = "Healey, Christopher G",
  month        =  oct,
  year         =  2017,
  howpublished = "\url{https://www.csc2.ncsu.edu/faculty/healey/PP/index.html}",
  note         = "Accessed: 2018-12-20",
  keywords     = "Literature Review Chapter 1;Literature Review Chapter
                  1/Criterion for bad/good plots;Literature Review 01Chapter
                  SG/Criterion for bad/good plots"
}

@INPROCEEDINGS{Van_Wijk_undated-nj,
  title     = "The Value of Visualization",
  booktitle = "{IEEE} Visualization 2005 - ({VIS'05})",
  author    = "van Wijk, J J",
  keywords  = "Literature Review Chapter 1;Literature Review Chapter
               1/Criterion for bad/good plots;Literature Review 01Chapter
               SG/Criterion for bad/good plots"
}

@ARTICLE{Wainer1984-co,
  title     = "How to Display Data Badly",
  author    = "Wainer, Howard",
  abstract  = "[Methods for displaying data badly have been developing for many
               years, and a wide variety of interesting and inventive schemes
               have emerged. Presented here is a synthesis yielding the 12 most
               powerful techniques that seem to underlie many of the
               realizations found in practice. These 12 (the dirty dozen) are
               identified and illustrated.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  38,
  number    =  2,
  pages     = "137--147",
  year      =  1984,
  keywords  = "Literature Review Chapter 1;Literature Review Chapter
               1/Criterion for bad/good plots;Literature Review 01Chapter
               SG/Criterion for bad/good plots"
}

@BOOK{Playfair2005-ew,
  title     = "Playfair's Commercial and Political Atlas and Statistical
               Breviary",
  author    = "Playfair, William",
  abstract  = "A scientific revolution began at the end of the 18th century
               with the creation and popularization of the graphic display of
               data by Scottish inventor William Playfair, who introduced the
               line graph, bar chart, and pie chart into statistics. His
               remarkable Atlas demonstrated how much could be learned if one
               plotted data graphically and looked for suggestive patterns to
               provide evidence for pursuing research. In the Statistical
               Breviary, Playfair invented the pie chart and expanded upon this
               concept to facilitate the comparison of the resources of
               European countries. Playfair's work has great relevance to
               contemporary science, but finding copies of his original
               versions is very difficult. This re-issuance of two of his
               classic works, with new explanatory material, allows access to
               his wisdom for the first time in two centuries. In full color
               exactly as Playfair hand-colored the original, this volume
               includes exact duplicates of the third edition of his classic
               Atlas as well as the Statistical Breviary. An additional feature
               is the inclusion of annotations and an extensive biography of
               the remarkable inventor. Howard Wainer is Distinquished Research
               Scientist at the National Board of Examiners and Adjunct
               Professor of Statistics at the Wharton School of the University
               of Pennsylvania. He has a Ph.D. in Psychometrics from Princeton
               University. Professor Wainer is the author of fifteen previous
               books, most recently, Graphic Discovery: A Trout in the Milk and
               Other Visual Adventures (2005).",
  publisher = "Cambridge University Press",
  year      =  2005,
  keywords  = "Literature Review 01Chapter SG/Criterion for bad/good plots",
  language  = "en"
}

@PHDTHESIS{Henkin2018-gs,
  title    = "A framework for hierarchical time-oriented data visualisation",
  author   = "Henkin, R",
  year     =  2018,
  school   = "City, University of London",
  keywords = "Literature Review 01Chapter SG/Time Granularities and
              Visualization"
}

@ARTICLE{Wickham_undated-vr,
  title    = "40 years of boxplots",
  author   = "Wickham, Hadley and Stryjewski, Lisa",
  keywords = "Boxplots;Literature Review 01Chapter SG/Time Granularities and
              Visualization"
}



@ARTICLE{Cleveland1982-ag,
  title     = "Graphical Methods for Seasonal Adjustment",
  author    = "Cleveland, William S and Terpenning, Irma J",
  abstract  = "[Graphical displays deserve to be a routine part of seasonal
               adjustment, and their routine use is now feasible because of the
               current revolution in computer graphical hardware and software.
               We present and illustrate seven graphical displays that provide
               powerful tools for judging the adequacy of the seasonal
               adjustment of a series and for understanding the behavior of the
               trend, seasonal, and irregular components.]",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  77,
  number    =  377,
  pages     = "52--62",
  year      =  1982,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}

@BOOK{Wills2011-yv,
  title     = "Visualizing Time: Designing Graphical Representations for
               Statistical Data",
  author    = "Wills, Graham",
  abstract  = "Art, or Science? Which of these is the right way to think of the
               field of visualization? This is not an easy question to answer,
               even for those who have many years experience in making
               graphical depictions of data with a view to help people
               understand it and take action. In this book, Graham Wills
               bridges the gap between the art and the science of visually
               representing data. He does not simply give rules and advice, but
               bases these on general principles and provide a clear path
               between them This book is concerned with the graphical
               representation of time data and is written to cover a range of
               different users. A visualization expert designing tools for
               displaying time will find it valuable, but so also should a
               financier assembling a report in a spreadsheet, or a medical
               researcher trying to display gene sequences using a commercial
               statistical package.",
  publisher = "Springer",
  month     =  dec,
  year      =  2011,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  language  = "en"
}

@INPROCEEDINGS{Van_Wijk1999-um,
  title     = "Cluster and calendar based visualization of time series data",
  booktitle = "Proceedings 1999 {IEEE} Symposium on Information Visualization
               ({InfoVis'99})",
  author    = "Van Wijk, J J and Van Selow, E R",
  abstract  = "A new method is presented to get an insight into univariate time
               series data. The problem addressed is how to identify patterns
               and trends on multiple time scales (days, weeks, seasons)
               simultaneously. The solution presented is to cluster similar
               daily data patterns, and to visualize the average patterns as
               graphs and the corresponding days on a calendar. This
               presentation provides a quick insight into both standard and
               exceptional patterns. Furthermore, it is well suited to
               interactive exploration. Two applications, numbers of employees
               present and energy consumption, are presented.",
  pages     = "4--9",
  month     =  oct,
  year      =  1999,
  keywords  = "data visualisation;time series;pattern
               recognition;graphs;cluster based visualization;calendar based
               visualization;univariate time series data;pattern
               clustering;graphs;employees;energy consumption;Calendars;Data
               visualization;Time series analysis;Energy consumption;Time
               measurement;Data analysis;Fourier transforms;Mathematics;Pattern
               analysis;Data mining;Literature Review 01Chapter SG/Time
               Granularities and Visualization"
}

@BOOK{Hyndman2018-ev,
  title     = "Forecasting: principles and practice",
  author    = "Hyndman, Rob J and Athanasopoulos, George",
  abstract  = "Forecasting is required in many situations. Stocking an
               inventory may require forecasts of demand months in advance.
               Telecommunication routing requires traffic forecasts a few
               minutes ahead. Whatever the circumstances or time horizons
               involved, forecasting is an important aid in effective and
               efficient planning.This textbook provides a comprehensive
               introduction to forecasting methods and presents enough
               information about each method for readers to use them sensibly.",
  publisher = "OTexts",
  year      =  2018,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization",
  language  = "en",
  url = {OTexts.com/fpp2}
}



@INCOLLECTION{Jensen1998-qn,
  title     = "The consensus glossary of temporal database concepts ---
               February 1998 version",
  booktitle = "Temporal Databases: Research and Practice",
  author    = "Jensen, Christian S and Dyreson, Curtis E and B{\"o}hlen,
               Michael and Clifford, James and Elmasri, Ramez and Gadia, Shashi
               K and Grandi, Fabio and Hayes, Pat and Jajodia, Sushil and
               K{\"A}fer, Wolfgang and Kline, Nick and Lorentzos, Nikos and
               Mitsopoulos, Yannis and Montanari, Angelo and Nonen, Daniel and
               Peressi, Elisa and Pernici, Barbara and Roddick, John F and
               Sarda, Nandlal L and Scalas, Maria Rita and Segev, Arie and
               Snodgrass, Richard T and Soo, Mike D and Tansel, Abdullah and
               Tiberio, Paolo and Wiederhold, Gio",
  editor    = "Etzion, Opher and Jajodia, Sushil and Sripada, Suryanarayana",
  abstract  = "This document1 contains definitions of a wide range of concepts
               specific to and widely used within temporal databases. In
               addition to providing definitions, the document also includes
               explanations of concepts as well as discussions of the adopted
               names.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "367--405",
  year      =  1998,
  address   = "Berlin, Heidelberg",
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}





% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Tukey1977-jx,
  title     = "Exploratory data analysis",
  author    = "Tukey, John W",
  abstract  = "We were together learning how to use the analysis of variance,
               and perhaps it is worth while stating an impression that I have
               formed-that the analysis of variance, which may perhaps be
               called a statistical method, because that term is a very
               ambiguous one---is not a mathematical theorem, but rather a
               convenient method of arranging the arithmetic. Just as in
               arithmetical textbooks---if we can recall their contents---we
               were given rules for arranging how to find the greatest common
               measure, and how to work out a sum in practice, and were âŠ",
  publisher = "Addison-Wesley",
  address = "Reading, Mass.",
  year      =  1977,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}

@ARTICLE{Benjamini1988-io,
  title     = "Opening the Box of a Boxplot",
  author    = "Benjamini, Yoav",
  abstract  = "Abstract Variations of the boxplot are suggested, in which the
               sides of the box are used to convey information about the
               density of the values in a batch. The ease of computation by
               hand of the original boxplot had to be sacrificed, as the
               variations are computer-intensive. Still, the plots were
               implemented on a desktop personal computer (Apple Macintosh), in
               a way designed to keep their ease of computation by computer.
               The result is a dynamic display of densities and summaries.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  42,
  number    =  4,
  pages     = "257--262",
  month     =  nov,
  year      =  1988,
  keywords  = "Literature Review 01Chapter SG/Time Granularities and
               Visualization"
}





@ARTICLE{Brehmer2017-st,
  title    = "Timelines Revisited: A Design Space and Considerations for
              Expressive Storytelling",
  author   = "Brehmer, Matthew and Lee, Bongshin and Bach, Benjamin and Riche,
              Nathalie Henry and Munzner, Tamara",
  abstract = "There are many ways to visualize event sequences as timelines. In
              a storytelling context where the intent is to convey multiple
              narrative points, a richer set of timeline designs may be more
              appropriate than the narrow range that has been used for
              exploratory data analysis by the research community. Informed by
              a survey of 263 timelines, we present a design space for
              storytelling with timelines that balances expressiveness and
              effectiveness, identifying 14 design choices characterized by
              three dimensions: representation, scale, and layout. Twenty
              combinations of these choices are viable timeline designs that
              can be matched to different narrative points, while smooth
              animated transitions between narrative points allow for the
              presentation of a cohesive story, an important aspect of both
              interactive storytelling and data videos. We further validate
              this design space by realizing the full set of viable timeline
              designs and transitions in a proof-of-concept sandbox
              implementation that we used to produce seven example timeline
              stories. Ultimately, this work is intended to inform and inspire
              the design of future tools for storytelling with timelines.",
  journal  = "IEEE Trans. Vis. Comput. Graph.",
  volume   =  23,
  number   =  9,
  pages    = "2151--2164",
  month    =  sep,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization",
  language = "en"
}

@ARTICLE{Fan_Du2017-xv,
  title    = "Coping with Volume and Variety in Temporal Event Sequences:
              Strategies for Sharpening Analytic Focus",
  author   = "{Fan Du} and Shneiderman, Ben and Plaisant, Catherine and Malik,
              Sana and Perer, Adam",
  abstract = "The growing volume and variety of data presents both
              opportunities and challenges for visual analytics. Addressing
              these challenges is needed for big data to provide valuable
              insights and novel solutions for business, security, social
              media, and healthcare. In the case of temporal event sequence
              analytics it is the number of events in the data and variety of
              temporal sequence patterns that challenges users of visual
              analytic tools. This paper describes 15 strategies for sharpening
              analytic focus that analysts can use to reduce the data volume
              and pattern variety. Four groups of strategies are proposed: (1)
              extraction strategies, (2) temporal folding, (3) pattern
              simplification strategies, and (4) iterative strategies. For each
              strategy, we provide examples of the use and impact of this
              strategy on volume and/or variety. Examples are selected from 20
              case studies gathered from either our own work, the literature,
              or based on email interviews with individuals who conducted the
              analyses and developers who observed analysts using the tools.
              Finally, we discuss how these strategies might be combined and
              report on the feedback from 10 senior event sequence analysts.",
  journal  = "IEEE Trans. Vis. Comput. Graph.",
  volume   =  23,
  number   =  6,
  pages    = "1636--1649",
  month    =  jun,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization",
  language = "en"
}

@ARTICLE{Beck2017-em,
  title    = "A Taxonomy and Survey of Dynamic Graph Visualization",
  author   = "Beck, Fabian and Burch, Michael and Diehl, Stephan and Weiskopf,
              Daniel",
  abstract = "Abstract Dynamic graph visualization focuses on the challenge of
              representing the evolution of relationships between entities in
              readable, scalable and effective diagrams. This work surveys the
              growing number of approaches in this discipline. We derive a
              hierarchical taxonomy of techniques by systematically
              categorizing and tagging publications. While static graph
              visualizations are often divided into node-link and matrix
              representations, we identify the representation of time as the
              major distinguishing feature for dynamic graph visualizations:
              either graphs are represented as animated diagrams or as static
              charts based on a timeline. Evaluations of animated approaches
              focus on dynamic stability for preserving the viewer's mental map
              or, in general, compare animated diagrams to timeline-based ones.
              A bibliographic analysis provides insights into the organization
              and development of the field and its community. Finally, we
              identify and discuss challenges for future research. We also
              provide feedback from experts, collected with a questionnaire,
              which gives a broad perspective of these challenges and the
              current state of the field.",
  journal  = "Comput. Graph. Forum",
  volume   =  36,
  number   =  1,
  pages    = "133--159",
  month    =  jan,
  year     =  2017,
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization"
}

@ARTICLE{Beck_undated-sc,
  title    = "The State of the Art in Visualizing Dynamic Graphs",
  author   = "Beck, Fabian and Burch, Michael and Diehl, Stephan and Weiskopf,
              Daniel",
  keywords = "Literature Review Chapter 1;Literature Review Chapter 1/Dynamic
              Graph Visualization;Literature Review 01Chapter SG/Dynamic Graph
              Visualization"
}


@Manual{Earo-Sugrrants,
  title = {sugrrants: Supporting Graphs for Analysing Time Series},
  author = {Earo Wang and Di Cook and Rob J Hyndman},
  year = {2018},
  note = {R package version 0.1.5},
  url = {https://CRAN.R-project.org/package=sugrrants},
}


@ARTICLE{Wainer1992-ml,
   title     = "Understanding Graphs and Tables",
   author    = "Wainer, Howard",
   abstract  = "Quantitative phenomena can be displayed effectively in a variety
                of ways, but to do so requires an understanding of both the
                structure of the phenomena and the limitations of candidate
                display formats. This article (a) re-counts three historic
                instances of the vital role data displays played in important
                discoveries, (b) provides three levels of information that form
                the basis of a theory of display to help us better measure both
                display quality and human graphicacy, and (c) describes three
                steps to improve the quality of tabular presentation.",
   journal   = "Educ. Res.",
   publisher = "American Educational Research Association",
   volume    =  21,
   number    =  1,
   pages     = "14--23",
   month     =  jan,
   year      =  1992
   }

   @BOOK{Unwin2016-aq,
    title     = "Graphical Data Analysis with {R}",
    author    = "Unwin, Antony",
    abstract  = "See How Graphics Reveal Information Graphical Data Analysis with
                 R shows you what information you can gain from graphical
                 displays. The book focuses on why you draw graphics to display
                 data and which graphics to draw (and uses R to do so). All the
                 datasets are available in R or one of its packages and the R
                 code is available at rosuda.org/GDA. Graphical data analysis is
                 useful for data cleaning, exploring data structure, detecting
                 outliers and unusual groups, identifying trends and clusters,
                 spotting local patterns, evaluating modelling output, and
                 presenting results. This book guides you in choosing graphics
                 and understanding what information you can glean from them. It
                 can be used as a primary text in a graphical data analysis
                 course or as a supplement in a statistics course. Colour
                 graphics are used throughout.",
    publisher = "CRC Press",
    month     =  apr,
    year      =  2016,
    language  = "en"
  }

  @BOOK{Cleveland1993-fn,
    title     = "Visualizing Data",
    author    = "Cleveland, William S",
    publisher = "Hobart Press",
    year      =  1993
  }


   @BOOK{Maimon2010-ac,
     title     = "Data Mining and Knowledge Discovery Handbook",
     author    = "Maimon, Oded and Rokach, Lior",
     abstract  = "Knowledge Discovery demonstrates intelligent computing at its
                  best, and is the most desirable and interesting end-product of
                  Information Technology. To be able to discover and to extract
                  knowledge from data is a task that many researchers and
                  practitioners are endeavoring to accomplish. There is a lot of
                  hidden knowledge waiting to be discovered -- this is the
                  challenge created by today's abundance of data. Data Mining and
                  Knowledge Discovery Handbook, 2nd Edition organizes the most
                  current concepts, theories, standards, methodologies, trends,
                  challenges and applications of data mining (DM) and knowledge
                  discovery in databases (KDD) into a coherent and unified
                  repository. This handbook first surveys, then provides
                  comprehensive yet concise algorithmic descriptions of methods,
                  including classic methods plus the extensions and novel methods
                  developed recently. This volume concludes with in-depth
                  descriptions of data mining applications in various
                  interdisciplinary industries including finance, marketing,
                  medicine, biology, engineering, telecommunications, software,
                  and security. Data Mining and Knowledge Discovery Handbook, 2nd
                  Edition is designed for research scientists, libraries and
                  advanced-level students in computer science and engineering as a
                  reference. This handbook is also suitable for professionals in
                  industry, for computing applications, information systems
                  management, and strategic research management.",
     publisher = "Springer Science \& Business Media",
     month     =  sep,
     year      =  2010,
     language  = "en"
   }

    @ARTICLE{Zhang2003-bj,
      title    = "Spatial Cone Tree: An Auxiliary Search Structure for
                  Correlation-based Similarity Queries on Spatial Time Series Data",
      author   = "Zhang, Pusheng and Shekhar, Shashi and Kumar, Vipin and Huang,
                  Yan",
      abstract = "ResearchGate is a network dedicated to science and research.
                  Connect, collaborate and discover scientific publications, jobs
                  and conferences. All for free.",
      month    =  nov,
      year     =  2003
    }

    @ARTICLE{Camossi2006-lg,
  title    = "A multigranular object-oriented framework supporting
              spatio-temporal granularity conversions",
  author   = "Camossi, Elena and Bertolotti, Mario and Bertino, Elisa",
  abstract = "Several application domains require handling spatio-temporal
              data. However, traditional Geographic Information Systems (GIS)
              and database models do not adequately support temporal aspects of
              spatial data. A crucial issue relates to the choice of the
              appropriate granularity. Unfortunately, while a formalisation of
              the concept of temporal granularity has been proposed and widely
              adopted, no consensus exists on the notion of spatial
              granularity. In this paper, we address these open problems, by
              proposing a formal definition of spatial granularity and by
              designing a spatio-temporal framework for the management of
              spatial and temporal information at different granularities. We
              present a spatio-temporal extension of the ODMG type system with
              specific types for defining multigranular spatio-temporal
              properties. Granularity conversion functions are introduced to
              obtain attributes values at different spatial and temporal
              granularities.",
  series   = "Cyber Center Publications",
  year     =  2006
}

@ARTICLE{Euzenat2005-de,
   title  = "Time Granularity",
   author = "Euzenat, Jerome and Montanari, Angelo",
   year   =  2005
 }
 
@ARTICLE{Grolemund2011-qf,
   title   = "Dates and times made easy with lubridate",
   author  = "Grolemund, Garrett and Wickham, Hadley and {Others}",
   journal = "J. Stat. Softw.",
   volume  =  40,
   number  =  3,
   pages   = "1--25",
   year    =  2011
 }
 
 @MANUAL{smart-meter,
  title       = {Smart-Grid Smart-City Customer Trial Data},
  author      = "{Department of the Environment and Energy}",
  institution = "Department of the Environment and Energy, Australia",
  year        =  2018,
  url         = "https://data.gov.au/dataset/4e21dea3-9b87-4610-94c7-15a8a77907ef",
  urldate     = {2018-11-19},
  address     = "Australian Government, Department of the Environment and Energy"
}
 
@Book{knitr,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  url = {https://yihui.name/knitr/},
  doi = {10.1201/b15166}
}

@Book{rmarkdown,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J. J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  url = {https://bookdown.org/yihui/rmarkdown},
  doi = {10.1201/9781138359444}
}


@Manual{R-gravitas,
  title = {gravitas: Explore Probability Distributions for Bivariate Temporal
Granularities},
  author = {Sayani Gupta and Rob J Hyndman and Di Cook and Antony Unwin},
  year = {2020},
  note = {R package version 0.1.3},
  url = {https://github.com/Sayani07/gravitas/},
}

@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2019},
  note = {R package version 0.8.3},
  url = {https://CRAN.R-project.org/package=dplyr},
}


@Manual{R-magrittr,
  title = {magrittr: A Forward-Pipe Operator for R},
  author = {Stefan Milton Bache and Hadley Wickham},
  year = {2014},
  note = {R package version 1.5},
  url = {https://CRAN.R-project.org/package=magrittr},
}

@Manual{R-rlang,
  title = {rlang: Functions for Base Types and Core R and 'Tidyverse' Features},
  author = {Lionel Henry and Hadley Wickham},
  year = {2019},
  note = {R package version 0.4.0},
  url = {https://CRAN.R-project.org/package=rlang},
}

@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Lionel Henry},
  year = {2019},
  note = {http://tidyr.tidyverse.org, https://github.com/tidyverse/tidyr},
}

@Manual{R-tsibble,
  title = {tsibble: Tidy Temporal Data Frames and Tools},
  author = {Earo Wang and Di Cook and Rob J Hyndman and Mitchell O'Hara-Wild},
  year = {2020},
  note = {R package version 0.9.1},
  url = {https://CRAN.R-project.org/package=tsibble},
}

@Manual{R-knitr,
    title = {knitr: A General-Purpose Package for Dynamic Report Generation in R},
    author = {Yihui Xie},
    year = {2020},
    note = {R package version 1.28},
    url = {https://yihui.org/knitr/},
  }
  
@Manual{R-rmarkdown,
    title = {rmarkdown: Dynamic Documents for R},
    author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
    year = {2020},
    note = {R package version 2.1},
    url = {https://github.com/rstudio/rmarkdown},
  }

@Manual{R-language,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
  

@Manual{R-liminal,
    title = {liminal: Multivariate Data Visualization with Tours and Embeddings},
    author = {Stuart Lee},
    year = {2021},
    note = {R package version 0.1.2},
    url = {https://CRAN.R-project.org/package=liminal},
  }


 @ARTICLE{Potter2010-qc,
   title    = "Visualizing Summary Statistics and Uncertainty",
   author   = "Potter, K and Kniss, J and Riesenfeld, R and Johnson, C R",
   abstract = "Abstract The graphical depiction of uncertainty information is
               emerging as a problem of great importance. Scientific data sets
               are not considered complete without indications of error,
               accuracy, or levels of confidence. The visual portrayal of this
               information is a challenging task. This work takes inspiration
               from graphical data analysis to create visual representations
               that show not only the data value, but also important
               characteristics of the data including uncertainty. The canonical
               box plot is reexamined and a new hybrid summary plot is presented
               that incorporates a collection of descriptive statistics to
               highlight salient features of the data. Additionally, we present
               an extension of the summary plot to two dimensional
               distributions. Finally, a use-case of these new plots is
               presented, demonstrating their ability to present high-level
               overviews as well as detailed insight into the salient features
               of the underlying data distribution.",
   journal  = "Comput. Graph. Forum",
   volume   =  29,
   number   =  3,
   pages    = "823--832",
   month    =  aug,
   year     =  2010,
   doi = {10.1111/j.1467-8659.2009.01677.x}
 }
 

@book{Grolemund2018-po,
   title    = "{R} for data science",
   author   = "Grolemund, Garrett and Wickham, Hadley",
   publisher = {O'Reilly Media},
   abstract = "This book will teach you how to do data science with R: You'll
               learn how to get your data into R, get it into the most useful
               structure, transform it, visualise it and model it. In this book,
               you will find a practicum of skills for data science. Just as a
               chemist learns how to clean test tubes and stock a lab, you'll
               learn how to clean data and draw plots---and many other things
               besides. These are the skills that allow data science to happen,
               and here you will find the best practices for doing each of these
               things with R. You'll learn how to use the grammar of graphics,
               literate programming, and reproducible research to save time.
               You'll also learn how to manage cognitive resources to facilitate
               discoveries when wrangling, visualising, and exploring data.",
   year     =  2017
 }
 
 

#### NOT NEEDED

 @ARTICLE{Bettini2000-vy,
   title    = "Symbolic representation of user-defined time granularities",
   author   = "Bettini, Claudio and De Sibi, Roberto",
   abstract = "In the recent literature on time representation, an effort has
               been made to characterize the notion of time granularity and the
               relationships between granularities. The main goals are having a
               common framework for their specification, and allowing the
               interoperability of systems adopting different time
               granularities. This paper considers the mathematical
               characterization of finite and periodic time granularities, and
               investigates the requirements for a user-friendly symbolic
               formalism that could be used for their specification. Instead of
               proposing yet another formalism, the paper analyzes the
               expressiveness of known symbolic formalisms for the
               representation of granularities, using the mathematical
               characterization as a reference model. Based on this analysis, a
               significant extension to the collection formalism defined in [15]
               is proposed, in order to capture a practically interesting class
               of periodic granularities.",
   journal  = "Ann. Math. Artif. Intell.",
   volume   =  30,
   number   =  1,
   pages    = "53--92",
   month    =  jun,
   year     =  2000
 }

@ARTICLE{Hyndman1996-ty,
   title     = "Sample Quantiles in Statistical Packages",
   author    = "Hyndman, Rob J and Fan, Yanan",
   abstract  = "Abstract There are a large number of different definitions used
                for sample quantiles in statistical computer packages. Often
                within the same package one definition will be used to compute a
                quantile explicitly, while other definitions may be used when
                producing a boxplot, a probability plot, or a QQ plot. We
                compare the most commonly implemented sample quantile
                definitions by writing them in a common notation and
                investigating their motivation and some of their properties. We
                argue that there is a need to adopt a standard definition for
                sample quantiles so that the same answers are produced by
                different packages and within each package. We conclude by
                recommending that the median-unbiased estimator be used because
                it has most of the desirable properties of a quantile estimator
                and can be defined independently of the underlying distribution.",
   journal   = "Am. Stat.",
   publisher = "Taylor \& Francis",
   volume    =  50,
   number    =  4,
   pages     = "361--365",
   month     =  nov,
   year      =  1996
 }

@BOOK{Silverman1986-cl,
   title     = "Density estimation for statistics and data analysis",
   author    = "Silverman, B W",
   publisher = "Chapman and Hall",
   series    = "Monographs on statistics and applied probability ; [26]",
   year      =  1986,
   address   = "London ; New York",
   keywords  = "Estimation theory"
 }

@ARTICLE{Jones1992-pj,
   title    = "Estimating densities, quantiles, quantile densities and density
               quantiles",
   author   = "Jones, M C",
   abstract = "To estimate the quantile density function (the derivative of the
               quantile function) by kernel means, there are two alternative
               approaches. One is the derivative of the kernel quantile
               estimator, the other is essentially the reciprocal of the kernel
               density estimator. We give ways in which the former method has
               certain advantages over the latter. Various closely related
               smoothing issues are also discussed.",
   journal  = "Ann. Inst. Stat. Math.",
   volume   =  44,
   number   =  4,
   pages    = "721--727",
   month    =  dec,
   year     =  1992
 }

@BOOK{Bettini2000-qk,
   title     = "Time Granularities in Databases, Data Mining, and Temporal
                Reasoning",
   author    = "Bettini, Claudio and Jajodia, Sushil and Wang, Sean",
   abstract  = "Calendar units, such as months and days, clock units, such as
                hours and seconds, and specialized units, such as business days
                and academic years, play a major role in a wide range of
                information system applications. System support for reasoning
                about these units, called granularities in this book, is
                important for the efficient design, use, and implementation of
                such applications. The book deals with several aspects of
                temporal information and provides a unifying model for
                granularities. It is intended for computer scientists and
                engineers who are interested in the formal models and technical
                development of specific issues. Practitioners can learn about
                critical aspects that must be taken into account when designing
                and implementing databases supporting temporal information.
                Lecturers may find this book useful for an advanced course on
                databases. Moreover, any graduate student working on time
                representation and reasoning, either in data or knowledge bases,
                should definitely read it.",
   publisher = "Springer Science \& Business Media",
   month     =  jul,
   year      =  2000,
   language  = "en"
 }
 
@BOOK{Embrechts2013-pd,
   title     = "Modelling Extremal Events: for Insurance and Finance",
   author    = "Embrechts, Paul and Kl{\"u}ppelberg, Claudia and Mikosch, Thomas",
   abstract  = "Both in insurance and in finance applications, questions
                involving extremal events (such as large insurance claims, large
                fluctuations in financial data, stock market shocks, risk
                management, ...) play an increasingly important role. This book
                sets out to bridge the gap between the existing theory and
                practical applications both from a probabilistic as well as from
                a statistical point of view. Whatever new theory is presented is
                always motivated by relevant real-life examples. The numerous
                illustrations and examples, and the extensive bibliography make
                this book an ideal reference text for students, teachers and
                users in the industry of extremal event methodology.",
   publisher = "Springer Science \& Business Media",
   month     =  jan,
   year      =  2013,
   language  = "en"
 }
 
 @ARTICLE{Grosse2002-ex,
   title    = "Analysis of symbolic sequences using the {Jensen-Shannon}
               divergence",
   author   = "Grosse, Ivo and Bernaola-Galv{\'a}n, Pedro and Carpena, Pedro and
               Rom{\'a}n-Rold{\'a}n, Ram{\'o}n and Oliver, Jose and Stanley, H
               Eugene",
   abstract = "We study statistical properties of the Jensen-Shannon divergence
               D, which quantifies the difference between probability
               distributions, and which has been widely applied to analyses of
               symbolic sequences. We present three interpretations of D in the
               framework of statistical physics, information theory, and
               mathematical statistics, and obtain approximations of the mean,
               the variance, and the probability distribution of D in random,
               uncorrelated sequences. We present a segmentation method based on
               D that is able to segment a nonstationary symbolic sequence into
               stationary subsequences, and apply this method to DNA sequences,
               which are known to be nonstationary on a wide range of different
               length scales.",
   journal  = "Phys. Rev. E Stat. Nonlin. Soft Matter Phys.",
   volume   =  65,
   number   = "4 Pt 1",
   pages    = "041905",
   month    =  apr,
   year     =  2002,
   language = "en"
 }
 

@ARTICLE{Menendez1997-in,
   title    = "The {Jensen-Shannon} divergence",
   author   = "Men{\'e}ndez, M L and Pardo, J A and Pardo, L and Pardo, M C",
   abstract = "In this paper we investigate the Jensen-Shannon parametric
               divergence for testing goodness-of-fit for point estimation. Most
               of the work presented is an analytical study of the asymptotic
               differences between different members of the family proposed in
               goodness of fit, together with an examination of closer
               approximations to the exact distribution of these statistics than
               the commonly used chi-squared distribution. Finally the minimum
               Jensen-Shannon divergence estimates are introduced and compared
               with other well-known estimators by computer simulation.",
   journal  = "J. Franklin Inst.",
   volume   =  334,
   number   =  2,
   pages    = "307--318",
   month    =  mar,
   year     =  1997
 }


@ARTICLE{Lin1991-fj,
   title    = "Divergence measures based on the Shannon entropy",
   author   = "Lin, J",
   abstract = "A novel class of information-theoretic divergence measures based
               on the Shannon entropy is introduced. Unlike the well-known
               Kullback divergences, the new measures do not require the
               condition of absolute continuity to be satisfied by the
               probability distributions involved. More importantly, their close
               relationship with the variational distance and the probability of
               misclassification error are established in terms of bounds. These
               bounds are crucial in many applications of divergence measures.
               The measures are also well characterized by the properties of
               nonnegativity, finiteness, semiboundedness, and boundedness.",
   journal  = "IEEE Trans. Inf. Theory",
   volume   =  37,
   number   =  1,
   pages    = "145--151",
   month    =  jan,
   year     =  1991,
   keywords = "entropy;information theory;information theory;Shannon
               entropy;divergence measures;variational distance;probability of
               misclassification
               error;nonnegativity;finiteness;semiboundedness;boundedness;Entropy;Probability
               distribution;Upper bound;Pattern analysis;Signal analysis;Signal
               processing;Pattern recognition;Taxonomy;Genetics;Computer science"
 } 

@BOOK{De_Haan2007-yx,
   title     = "Extreme Value Theory: An Introduction",
   author    = "de Haan, Laurens and Ferreira, Ana",
   abstract  = "This treatment of extreme value theory is unique in book
                literature in that it focuses on some beautiful theoretical
                results along with applications. All the main topics covering
                the heart of the subject are introduced to the reader in a
                systematic fashion so that in the final chapter even the most
                recent developments in the theory can be understood. Key to the
                presentation is the concentration on the probabilistic and
                statistical aspects of extreme values without major emphasis on
                such related topics as regular variation, point processes,
                empirical distribution functions, and Brownian motion. The work
                is an excellent introduction to extreme value theory at the
                graduate level, requiring only some mathematical maturity.",
   publisher = "Springer Science \& Business Media",
   month     =  dec,
   year      =  2007,
   language  = "en"
 }
 
@ARTICLE{Kullback1951-jy,
   title     = "On Information and Sufficiency",
   author    = "Kullback, S and Leibler, R A",
   journal   = "Ann. Math. Stat.",
   publisher = "Institute of Mathematical Statistics",
   volume    =  22,
   number    =  1,
   pages     = "79--86",
   year      =  1951
 }
 
 @ARTICLE{Tureczek2018-ha,
   title     = "Electricity Consumption Clustering Using Smart Meter Data",
   author    = "Tureczek, Alexander and Nielsen, Per Sieverts and Madsen, Henrik",
   abstract  = "Electricity smart meter consumption data is enabling utilities
                to analyze consumption information at unprecedented granularity.
                Much focus has been directed towards consumption clustering for
                diversifying tariffs; through modern clustering methods, cluster
                analyses have been performed. However, the clusters developed
                exhibit a large variation with resulting shadow clusters, making
                it impossible to truly identify the individual clusters. Using
                clearly defined dwelling types, this paper will present methods
                to improve clustering by harvesting inherent structure from the
                smart meter data. This paper clusters domestic electricity
                consumption using smart meter data from the Danish city of
                Esbjerg. Methods from time series analysis and wavelets are
                applied to enable the K-Means clustering method to account for
                autocorrelation in data and thereby improve the clustering
                performance. The results show the importance of data knowledge
                and we identify sub-clusters of consumption within the dwelling
                types and enable K-Means to produce satisfactory clustering by
                accounting for a temporal component. Furthermore our study shows
                that careful preprocessing of the data to account for intrinsic
                structure enables better clustering performance by the K-Means
                method.",
   journal   = "Energies",
   publisher = "Multidisciplinary Digital Publishing Institute",
   volume    =  11,
   number    =  4,
   pages     = "859",
   month     =  apr,
   year      =  2018,
   language  = "en"
 }
 
 @article{chicco2010renyi,
  title={Renyi entropy-based classification of daily electrical load patterns},
  author={Chicco, G and Akilimali, J Sumaili},
  journal={IET generation, transmission \& distribution},
  volume={4},
  number={6},
  pages={736--745},
  year={2010},
  publisher={IET}
}
 

@article{ozawa2016determining,
  title={Determining the relationship between a household’s lifestyle and its electricity consumption in Japan by analyzing measured electric load profiles},
  author={Ozawa, Akito and Furusato, Ryota and Yoshida, Yoshikuni},
  journal={Energy and Buildings},
  volume={119},
  pages={200--210},
  year={2016},
  publisher={Elsevier}
} 

@ARTICLE{Motlagh2019-yj,
   title    = "Clustering of residential electricity customers using load time
               series",
   author   = "Motlagh, Omid and Berry, Adam and O'Neil, Lachlan",
   abstract = "Clustering of electricity customers supports effective market
               segmentation and management. The literature suggests the
               clustering of residential customers by their load
               characteristics. The key challenge is the application of
               appropriate processes to reduce the extreme dimensionality of
               load time series to facilitate unique clusters. Time feature
               extraction is a potential remedy, however, it is limited by the
               type of noisy, patchy, and unequal time-series common in
               residential datasets. In this paper we propose a strategy to
               alleviate these limitations by converting any types of load time
               series into map models that can be readily clustered. This also
               results in higher cluster distinction and robustness against
               noise compared to a baseline feature-based approach. A large
               dataset of residential electricity customers is used to confirm
               the outcomes as measured by a number of analytical and industrial
               metrics. The experiment with 12 clusters results in around 61\%
               distinction, improved coincidence factor by around 6.75\%
               relative to a random grouping, and robustness of around 59\%
               against the applied noise.",
   journal  = "Appl. Energy",
   volume   =  237,
   pages    = "11--24",
   month    =  mar,
   year     =  2019,
   keywords = "Electricity consumption; Time series clustering; Unequal time
               series; Load profile"
 }
@ARTICLE{Tureczek2017-pb,
   title     = "Structured Literature Review of Electricity Consumption
                Classification Using Smart Meter Data",
   author    = "Tureczek, Alexander Martin and Nielsen, Per Sieverts",
   abstract  = "Smart meters for measuring electricity consumption are fast
                becoming prevalent in households. The meters measure consumption
                on a very fine scale, usually on a 15 min basis, and the data
                give unprecedented granularity of consumption patterns at
                household level. A multitude of papers have emerged utilizing
                smart meter data for deepening our knowledge of consumption
                patterns. This paper applies a modification of Okoli's method
                for conducting structured literature reviews to generate an
                overview of research in electricity customer classification
                using smart meter data. The process assessed 2099 papers before
                identifying 34 significant papers, and highlights three key
                points: prominent methods, datasets and application. Three
                important findings are outlined. First, only a few papers
                contemplate future applications of the classification, rendering
                papers relevant only in a classification setting. Second; the
                encountered classification methods do not consider correlation
                or time series analysis when classifying. The identified papers
                fail to thoroughly analyze the statistical properties of the
                data, investigations that could potentially improve
                classification performance. Third, the description of the data
                utilized is of varying quality, with only 50\% acknowledging
                missing values impact on the final sample size. A data
                description score for assessing the quality in data description
                has been developed and applied to all papers reviewed.",
   journal   = "Energies",
   publisher = "Multidisciplinary Digital Publishing Institute",
   volume    =  10,
   number    =  5,
   pages     = "584",
   month     =  apr,
   year      =  2017,
   language  = "en"
 }
 
@ARTICLE{Ndiaye2011-pf,
   title    = "Principal component analysis of the electricity consumption in
               residential dwellings",
   author   = "Ndiaye, Demba and Gabriel, Kamiel",
   abstract = "Data gathered from energy audits, phone surveys and smart meter
               readings are used to derive regression models of the electricity
               consumption of housing units in Oshawa (Ontario, Canada). The
               database used comprises 59 predictors, for 62 observations. To
               address the problem of multi-collinearities among the predictors
               and at the same time reduce the number of needed predictors, a
               methodology is developed based on the latent root regression
               technique of Hawkins [5]. Contrary to other variable selection
               techniques such as the stepwise method, the technique used in
               this paper allows an easy identification of alternative subsets.
               Using this technique, a reduction of 85\% in the number of
               predictors is obtained, leaving only nine of them in the final
               subset. These nine variables are the number of occupants, the
               house status (owned or rented), the number of weeks of vacation
               per year, the type of fuel used in the pool heater, the type of
               fuel used in the heating system, the type of fuel used in the
               domestic hot water heater, the existence or not of an air
               conditioning system, the type of air conditioning system, and the
               number of air changes per hour at 50Pa. A regression with these
               nine predictors leads to an R2 of 0.79, with an adjusted R2 of
               0.75 and all regression coefficients statistically significant at
               the 95\% confidence level.",
   journal  = "Energy Build.",
   volume   =  43,
   number   =  2,
   pages    = "446--453",
   month    =  feb,
   year     =  2011,
   keywords = "Principal component analysis; Latent root regression; Subset
               selection; Electricity consumption; Residential; Socio-economic
               factors"
 }
 
 
#### 



@article{olvera2010review,
  title={A review of instance selection methods},
  author={Olvera-L{\'o}pez, J Arturo and Carrasco-Ochoa, J Ariel and Mart{\'\i}nez-Trinidad, J Francisco and Kittler, Josef},
  journal={Artificial Intelligence Review},
  volume={34},
  number={2},
  pages={133--143},
  year={2010},
  publisher={Springer}
}

 @ARTICLE{Fan2021-bq,
   title    = "Unsupervised Visual Representation Learning via {Dual-Level}
               Progressive Similar Instance Selection",
   author   = "Fan, Hehe and Liu, Ping and Xu, Mingliang and Yang, Yi",
   abstract = "The superiority of deeply learned representations relies on
               large-scale labeled datasets. However, annotating data are
               usually expensive or even infeasible in some scenarios. To
               address this problem, we propose an unsupervised method to
               leverage instance discrimination and similarity for deep visual
               representation learning. The method is based on an observation
               that convolutional neural networks (CNNs) can learn a meaningful
               visual representation with instancewise classification, in which
               each instance is treated as an individual class. By this
               instancewise discriminative learning, instances can reasonably
               distribute in the representation space, which reveals their
               similarities. In order to further improve visual representations,
               we propose a dual-level progressive similar instance selection
               (DPSIS) method to build a bridge from instance to class by
               selecting similar instances (neighbors) for each instance
               (anchor) and treating the anchor and its neighbors as the same
               class. To be specific, DPSIS adaptively selects two levels of
               neighbors, that is: 1) an ``absolutely similar level'' and 2) a
               ``relatively similar level.'' Instances in the absolutely similar
               level are used as hard labels, while instances in the relatively
               similar level are used as soft labels. Moreover, during training,
               DPSIS is able to progressively select more neighbors without
               human supervision. At the beginning of training, because CNNs are
               weak, most instances are distributed relatively randomly in the
               representation space and only a few easy-to-recognize instances
               are selected as neighbors. As CNN models become stronger, the
               semantic meaning of each instance grows clearer. Those instances
               originally distributed in a relatively random manner gradually
               move to meaningful positions. This consequently facilitates CNN
               training since the number of reliable samples increases.
               Experiments on seven benchmarks, including three small-scale and
               two large-scale coarse-grained image classification datasets, and
               two fine-grained categorization datasets, demonstrate the
               effectiveness of our DPSIS. Our codes have been released at
               https://github.com/hehefan/DPSIS.",
   journal  = "IEEE Trans Cybern",
   volume   = "PP",
   month    =  mar,
   year     =  2021,
   language = "en"
 }
 
@article{wang2020tsibble,
  author = {Earo Wang and Dianne Cook and Rob J Hyndman},
  title = {A new tidy data structure to support exploration and modeling of temporal data},
  journal = {Journal of Computational \& Graphical Statistics},
  volume = {29},
  number = {3},
  pages = {466-478},
  year  = {2020},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2019.1695624}
}

 @BOOK{Cook2007-qe,
   title     = "Interactive and Dynamic Graphics for Data Analysis: With {R} and
                Ggobi",
   author    = "Cook, Dianne and Swayne, Deborah F",
   publisher = "Springer, New York, NY",
   year      =  2007
 }
 
  @ARTICLE{Xu2015-ja,
   title    = "A Comprehensive Survey of Clustering Algorithms",
   author   = "Xu, Dongkuan and Tian, Yingjie",
   abstract = "Data analysis is used as a common method in modern science
               research, which is across communication science, computer science
               and biology science. Clustering, as the basic composition of data
               analysis, plays a significant role. On one hand, many tools for
               cluster analysis have been created, along with the information
               increase and subject intersection. On the other hand, each
               clustering algorithm has its own strengths and weaknesses, due to
               the complexity of information. In this review paper, we begin at
               the definition of clustering, take the basic elements involved in
               the clustering process, such as the distance or similarity
               measurement and evaluation indicators, into consideration, and
               analyze the clustering algorithms from two perspectives, the
               traditional ones and the modern ones. All the discussed
               clustering algorithms will be compared in detail and
               comprehensively shown in Appendix Table 22.",
   journal  = "Annals of Data Science",
   volume   =  2,
   number   =  2,
   pages    = "165--193",
   month    =  jun,
   year     =  2015
 }
 

@inproceedings{dasu2005grouping,
  title={Grouping multivariate time series: A case study},
  author={Dasu, Tamraparni and Swayne, Deborah F and Poole, David},
  booktitle={Proceedings of the IEEE Workshop on Temporal Data Mining: Algorithms, Theory and Applications, in conjunction with the Conference on Data Mining, Houston},
  pages={25--32},
  year={2005},
  organization={Citeseer}
}

@book{borg2005modern,
  title={Modern multidimensional scaling: Theory and applications},
  author={Borg, Ingwer and Groenen, Patrick JF},
  year={2005},
  publisher={Springer Science \& Business Media}
}
 

@ARTICLE{Gupta2021-hakear,
   title     = "Detecting distributional differences between temporal granularities for     exploratory time series analysis ",
   author    = "Gupta, Sayani and Hyndman, Rob J and Cook, Dianne",
   journal   = "unpublished",
   year      =  2021
 }
 
 @article{wegman1990hyperdimensional,
  title={Hyperdimensional data analysis using parallel coordinates},
  author={Wegman, Edward J},
  journal={Journal of the American Statistical Association},
  volume={85},
  number={411},
  pages={664--675},
  year={1990},
  publisher={Taylor \& Francis}
}

@article{wickham2011tourr,
  title={tourr: An R package for exploring multivariate data with projections},
  author={Wickham, Hadley and Cook, Dianne and Hofmann, Heike and Buja, Andreas and others},
  journal={Journal of Statistical Software},
  volume={40},
  number={2},
  pages={1--18},
  year={2011},
  publisher={Foundation for Open Access Statistics}
}

@article{liao2005clustering,
  title={Clustering of time series data—a survey},
  author={Liao, T Warren},
  journal={Pattern recognition},
  volume={38},
  number={11},
  pages={1857--1874},
  year={2005},
  publisher={Elsevier}
}


 @ARTICLE{Melnykov2013-sp,
   title     = "Challenges in model-based clustering",
   author    = "Melnykov, Volodymyr",
   abstract  = "Abstract Model-based clustering is an increasingly popular area
                of cluster analysis that relies on probabilistic description of
                data by means of finite mixture models. Mixture distributions
                prove to be a powerful technique for modeling heterogeneity in
                data. In model-based clustering, each data group is seen as a
                sample from one or several mixture components. Despite
                attractive interpretation, model-based clustering poses many
                challenges. This paper discusses some of the most important
                problems a researcher might encounter while applying the
                model-based cluster analysis. WIREs Comput Stat 2013, 5:135?148.
                doi: 10.1002/wics.1248 This article is categorized under:
                Statistical Learning and Exploratory Methods of the Data
                Sciences > Clustering and Classification Statistical and
                Graphical Methods of Data Analysis > Density Estimation",
   journal   = "Wiley Interdiscip. Rev. Comput. Stat.",
   publisher = "Wiley",
   volume    =  5,
   number    =  2,
   pages     = "135--148",
   month     =  mar,
   year      =  2013,
   copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
   language  = "en"
 }
 
@ARTICLE{Aghabozorgi2015-ct,
   title    = "Time-series clustering -- A decade review",
   author   = "Aghabozorgi, Saeed and Seyed Shirkhorshidi, Ali and Ying Wah, Teh",
   abstract = "Clustering is a solution for classifying enormous data when there
               is not any early knowledge about classes. With emerging new
               concepts like cloud computing and big data and their vast
               applications in recent years, research works have been increased
               on unsupervised solutions like clustering algorithms to extract
               knowledge from this avalanche of data. Clustering time-series
               data has been used in diverse scientific areas to discover
               patterns which empower data analysts to extract valuable
               information from complex and massive datasets. In case of huge
               datasets, using supervised classification solutions is almost
               impossible, while clustering can solve this problem using
               un-supervised approaches. In this research work, the focus is on
               time-series data, which is one of the popular data types in
               clustering problems and is broadly used from gene expression data
               in biology to stock market analysis in finance. This review will
               expose four main components of time-series clustering and is
               aimed to represent an updated investigation on the trend of
               improvements in efficiency, quality and complexity of clustering
               time-series approaches during the last decade and enlighten new
               paths for future works.",
   journal  = "Inf. Syst.",
   volume   =  53,
   pages    = "16--38",
   month    =  oct,
   year     =  2015,
   keywords = "Clustering; Time-series; Distance measure; Evaluation measure;
               Representations"
 }
 
@article{liao2007clustering,
  title={A clustering procedure for exploratory mining of vector time series},
  author={Liao, T Warren},
  journal={Pattern Recognition},
  volume={40},
  number={9},
  pages={2550--2562},
  year={2007},
  publisher={Elsevier}
}
 
@inproceedings{ratanamahatana2005multimedia,
  title={Multimedia retrieval using time series representation and relevance feedback},
  author={Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
  booktitle={International Conference on Asian Digital Libraries},
  pages={400--405},
  year={2005},
  organization={Springer}
}

@inproceedings{corradini2001dynamic,
  title={Dynamic time warping for off-line recognition of a small gesture vocabulary},
  author={Corradini, Andrea},
  booktitle={Proceedings IEEE ICCV workshop on recognition, analysis, and tracking of faces and gestures in real-time systems},
  pages={82--89},
  year={2001},
  organization={IEEE}
}


@ARTICLE{Ushakova2020-rl,
   title    = "Big data to the rescue? Challenges in analysing granular
               household electricity consumption in the United Kingdom",
   author   = "Ushakova, Anastasia and Jankin Mikhaylov, Slava",
   abstract = "Rapid growth in smart meter installations has given rise to vast
               collections of data at a high time-resolution and down to an
               individual level. However, to enable efficient policy
               interventions, we need to be able to appropriately segment the
               population of users. The aim of this paper is to consider
               challenges and opportunities associated with large
               highly-granular temporal datasets that describe residential
               electricity consumption. In particular, the focus is on
               experiments relating to aggregation of smart meter time-series
               data in the context of clustering and prediction tasks that are
               often used for customer targeting and to gain insight on
               energy-use about sub populations. To cluster energy use profiles,
               we propose a novel framework based on a set of Gaussian based
               models which we use to encode individuals' energy consumption
               over time. The dataset consists of half hourly electricity
               consumption records from smart meters of households in the UK
               (2014--2015). The contribution of this paper comes from its
               investigation of how consumers or groups may be clustered
               according to model parameters in scenarios where additional data
               on consumers is not available to the researcher, or where
               anonymity preservation of the smart meter user is prioritised. A
               secondary aim is to invite greater awareness when data reduction
               is required to reduce the size of a large dataset for
               computational purposes. This may have implications for policy
               interventions acting at the individual or small group level, for
               instance, when designing incentives to encourage energy efficient
               behaviour or when identifying fuel poor customers.",
   journal  = "Energy Research \& Social Science",
   volume   =  64,
   pages    = "101428",
   month    =  jun,
   year     =  2020,
   keywords = "Big data; Time series models; Consumer behaviour; Smart meters;
               Electricity consumption; Clustering"
 }
 
@INPROCEEDINGS{Hennig2014-ah,
   title     = "How Many Bee Species? A Case Study in Determining the Number of
                Clusters",
   booktitle = "Data Analysis, Machine Learning and Knowledge Discovery",
   author    = "Hennig, Christian",
   abstract  = "It is argued that the determination of the best number of
                clusters k is crucially dependent on the aim of clustering.
                Existing supposedly ``objective'' methods of estimating k ignore
                this. k can be determined by listing a number of requirements
                for a good clustering in the given application and finding a k
                that fulfils them all. The approach is illustrated by
                application to the problem of finding the number of species in a
                data set of Australasian tetragonula bees. Requirements here
                include two new statistics formalising the largest
                within-cluster gap and cluster separation. Due to the typical
                nature of expert knowledge, it is difficult to make requirements
                precise, and a number of subjective decisions is involved.",
   publisher = "Springer International Publishing",
   pages     = "41--49",
   year      =  2014
 }
 
 @Manual{R-GGally,
    title = {GGally: Extension to 'ggplot2'},
    author = {Barret Schloerke and Di Cook and Joseph Larmarange and Francois Briatte and Moritz Marbach and Edwin Thoen and Amos Elberg and Jason Crowley},
    year = {2021},
    note = {R package version 2.1.1},
    url = {https://CRAN.R-project.org/package=GGally},
  }
  
@Manual{R-tsne,
    title = {{Rtsne}: T-Distributed Stochastic Neighbor Embedding using Barnes-Hut
Implementation},
    author = {Jesse H. Krijthe},
    year = {2015},
    note = {R package version 0.15},
    url = {https://github.com/jkrijthe/Rtsne},
  }
@ARTICLE{Krzysztofowicz1997-bv,
   title     = "Transformation and normalization of variates with specified
                distributions",
   author    = "Krzysztofowicz, Roman",
   abstract  = "Given two continuous random variables X and Y, with specified
                strictly increasing cumulative distribution functions F and G,
                respectively, the one-to-one transform t which maps one variate
                into another, say Y=t(X), has an analytic form, t(X)=G−1(F(X))
                or t(X)=G−1(1−F(X)), depending upon whether t is increasing or
                decreasing. This fact of probability theory is reviewed and
                compared with another method for finding t that was recently
                proposed. Applications to system identification, normalization
                of a variate, and normalization of a sample are briefly
                discussed.",
   journal   = "J. Hydrol.",
   publisher = "Elsevier BV",
   volume    =  197,
   number    = "1-4",
   pages     = "286--292",
   month     =  oct,
   year      =  1997,
   language  = "en"
 } 
 
 @ARTICLE{Gupta2021-gravitas,
   title     = "Visualizing probability distributions across bivariate cyclic
                temporal granularities",
   author    = "Gupta, Sayani and Hyndman, Rob J and Cook, Dianne and Unwin,
                Antony",
   abstract  = "AbstractDeconstructing a time index into time granularities can
                assist in exploration and automated analysis of large temporal
                data sets. This paper describes classes of time deconstructions
                using linear and cyclic time granularities. Linear granularities
                respect the linear progression of time such as hours, days,
                weeks and months. Cyclic granularities can be circular such as
                hour-of-the-day, quasi-circular such as day-of-the-month, and
                aperiodic such as public holidays. The hierarchical structure of
                granularities creates a nested ordering: hour-of-the-day and
                second-of-the-minute are single-order-up. Hour-of-the-week is
                multiple-order-up, because it passes over day-of-the-week.
                Methods are provided for creating all possible granularities for
                a time index. A recommendation algorithm provides an indication
                whether a pair of granularities can be meaningfully examined
                together (a ?harmony?), or when they cannot (a ?clash?). Time
                granularities can be used to create data visualizations to
                explore for periodicities, associations and anomalies. The
                granularities form categorical variables (ordered or unordered)
                which induce groupings of the observations. Assuming a numeric
                response variable, the resulting graphics are then displays of
                distributions compared across combinations of categorical
                variables.The methods implemented in the open source R package
                gravitas are consistent with a tidy workflow, with probability
                distributions examined using the range of graphics available in
                ggplot2.",
   journal   = "Journal of Computational \& Graphical Statistics",
   year      =  2021,
   note = {to appear}
 }
 
